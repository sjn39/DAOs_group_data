{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/eip-4844-shard-blob-transactions/8430",
            "title": "EIP-4844: Shard Blob Transactions ",
            "index": 8430,
            "category": [
                "EIPs",
                "Core EIPs"
            ],
            "tags": [
                "transactions",
                "sharding"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/protolambda",
                    "index": "1",
                    "likes": "7",
                    "time": "26/02/2022-01:35:06",
                    "content": "Discussion thread for EIP-4844  Background See previous sharding discussion in:  Sharding format blob carrying transactions Sharding design with tight beacon and shard block integration  Background information for self-adjusting independent gasprice for blobs:  multidimensional EIP 1559 with an exponential pricing rule   Rollup integration background:  Commitment proof of equivalence protocol   Implementation The implementation is a work in progress while the specification is being reviewed and improved. Execution layer implementation: Datablob transactions by protolambda \u00b7 Pull Request #1 \u00b7 protolambda/go-ethereum \u00b7 GitHub Consensus layer implementation: Comparing kiln...blobs \u00b7 prysmaticlabs/prysm \u00b7 GitHub ",
                    "links": [
                        "https://ethereum-magicians.org/t/sharding-format-blob-carrying-transactions/8289",
                        "https://ethereum-magicians.org/t/sharding-design-with-tight-beacon-and-shard-block-integration-danksharding/8291",
                        "https://ethresear.ch/t/multidimensional-eip-1559/11651",
                        "https://ethresear.ch/t/make-eip-1559-more-like-an-amm-curve/9082",
                        "https://ethresear.ch/t/easy-proof-of-equivalence-between-multiple-polynomial-commitment-schemes-to-the-same-data/8188",
                        "https://github.com/protolambda/go-ethereum/pull/1",
                        "https://github.com/prysmaticlabs/prysm/compare/kiln...blobs",
                        "https://ethereum-magicians.org/t/eip-4844-shard-blob-transactions-cancun-upgrade/10884"
                    ],
                    "GPT-summary": "The post is a discussion thread for EIP-4844, which proposes a sharding format for blob carrying transactions. The author explains the proposal and provides background information on previous sharding discussions, self-adjusting independent gas price for blobs, and rollup integration. Third parties are giving constructive criticism, asking questions, and auditing and reviewing the proposal.",
                    "GPT-proposal-categories": [
                        "Smart contract updates",
                        "Interoperability and Scalability",
                        "Not a proposal"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None"
                    ],
                    "Sentiment": 4.424603174603174
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/djrtwo",
                    "index": "2",
                    "likes": "3",
                    "time": "28/02/2022-16:23:43",
                    "content": "Great work @protolambda and others! This EIP suggests \u201ca target of ~1 MB per block and a limit of ~2 MB\u201d. I believe StarkWare\u2019s First \u201cBig Red Dot\u201d  and Second \u201cBig Red Dot\u201d mainnet experiments and analysis from July 2019 and Jan 2020, respectively. This analysis showed that when creating a series of \u201clarge\u201d mainnet blocks (45KB-1.46MB) throughout a single day, that it had no appreciable affect on uncle rates. This is an important and valuable analysis, but I think that there are subsequent simulations and analysis warranted before moving into the realm of regular 1MB blocks on mainnet. A few things to consider  Does the shift from PoW to PoS have any impact on the prior analysis?  In PoW, there are likely in the small number of dozens of consensus forming nodes (miners) rather than on the order of thousands in the current beacon chain network. Orphaning in PoW is thus the question of whether these large blocks make it to the other (small number of) miners quickly rather than to all user nodes or to all stakers in PoS 12s slot times in PoS might actually help prevent orphaning compared to PoW (a block is not likely to be orphaned if it makes in sub 12s times whereas the threshold is likely lower in PoW). But there is another important time, 4s \u2013 the attestation time into the slot. If large blocks regularly make it to the network after 4s, we\u2019d see \u201cincorrect head votes\u201d for many attestations even though the block still makes it into the chain. This would be an indicator of non-optimality in the consensus and some loss of revenue for validators. But would potentially result in orphaning in future block-slot PBS fork choice rules which have a stricter reaction to these missed head attestations. Does the shift from devp2p block gossip to libp2p block gossip have any impact on the expectations of load, propagation time, etc   Does the uncle rate actually capture the quality of service required here?  Even though blocks were not orphaned in the Big Red Dot experiments, are user nodes receiving and processing these blocks in a timely manner? or do the small set of mining nodes have some asymmetric bandwidth available and/or privileged position in the network (highly connected, multiple sentry nodes, etc) What does the minimum network speed/bandwidth become in a 1MB block regime? Does this push out some class of user or locality? e.g. at 10mbps, 1MB block transfers take ~1s per hop, whereas a 100mbps, they take 0.1s What becomes the minimum monthly data-cap required for such regular blocks (taking into account data-tx  mempool requirements and the gossip amplification factor)?    I don\u2019t intend to nay-say the 1MB suggestion, but I want to highlight there are some pen-and-paper calculations and likely some network simulations in order beyond the prior Big Red Dot analyses to tune this number. ",
                    "links": [
                        "https://ethereum-magicians.org/t/eip-2028-transaction-data-gas-cost-reduction/3280/35"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.3323751451800225
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/yperbasis",
                    "index": "3",
                    "likes": "1",
                    "time": "04/03/2022-11:40:54",
                    "content": "Can we please use y_parity: boolean rather than v: uint8 in ECDSASignature (similar to EIP-1559 transactions)? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/yperbasis",
                    "index": "4",
                    "likes": "1",
                    "time": "04/03/2022-13:50:28",
                    "content": "get_intrinsic_gas should also charge Gtxcreate  for create transactions as well as ACCESS_LIST_ADDRESS_COST/ACCESS_LIST_STORAGE_KEY_COST for access list.  See Eq (60) in the Yellow Paper. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/EdFelten",
                    "index": "5",
                    "likes": "1",
                    "time": "06/03/2022-14:52:10",
                    "content": "Good proposal!  Thanks for your work on it. One suggestion on the gas pricing.  Currently update_blob_gas sets new_total = max(current_total + blob_txs_in_block, targeted_total). The rationale for this (from the code comment) is to \u201cto avoid accumulating a long period of very low fees\u201d.  This goal make sense.  But it seems that goal would be better met by using something like new_total = max(current_total + blob_txs_in_block, targeted_total - MAX_BLOB_TARGET_DEFICIT) for some value of MAX_BLOB_TARGET_DEFICIT like 64.  That would allow a burst of 64 blobs above the target, without increasing the price, provided there had been a previously quiet period bringing total usage 64 blobs below the target. Without this change, the pricing would deviate from the goal of having pricing be agnostic to the distribution of block usage, in that using 8 and 8 blobs in consecutive blocks would not increase the price, whereas using 0 and 16 in consecutive block would increase the price. The optimal value of MAX_BLOB_TARGET_DEFICIT would try to balance the goal of being usage history agnostic (which would argue for a larger value of MAX_BLOB_TARGET_DEFICIT) with wanting to avoid a long burst of over-target usage without a price increase (which would argue for a smaller value of MAX_BLOB_TARGET_DEFICIT).  Essentially the value of this parameter would say how long a \u201cperiod of very low fees [despite high usage]\u201d to allow after a prolonged period of low usage. My tentative proposal would be a value of 64 blobs, or 8 * TARGET_BLOB_TXS_PER_BLOCK. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.466176470588236
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/dankrad",
                    "index": "6",
                    "likes": "1",
                    "time": "08/03/2022-22:36:53",
                    "content": "    EdFelten:  new_total = max(current_total + blob_txs_in_block, targeted_total - MAX_BLOB_TARGET_DEFICIT) for some value of MAX_BLOB_TARGET_DEFICIT like 64. That would allow a burst of 64 blobs above the target, without increasing the price, provided there had been a previously quiet period bringing total usage 64 blobs below the target.   In principle I agree with this, in practice I think it will make very little difference. The reason is that the gas price at the balance point is very low: if current_total=actual_total, then the cost of one 128 kb data blob would be 1 gas, which at today\u2019s gas prices and Ether price comes to 0.01 cent (for comparison, current prices would be 2M gas, so two million times that). Even at the highest prices we have seen it would still be less than a cent for the whole blob. This pricing is so cheap that I can\u2019t see it being an equilibrium point at any time after there is any significant use of data blocks, so I don\u2019t think it\u2019s necessary to complicate it with another constant. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.235243055555555
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/EdFelten",
                    "index": "7",
                    "likes": "0",
                    "time": "10/03/2022-12:53:56",
                    "content": "Ah, good point.  Although that could equally well be an argument for a larger value of MAX_BLOB_TARGET_DEFICIT, one large enough that the price difference would be significant.  That would increase the burst size that could be accommodated without price increase, in a scenario where the overall average usage is within the target. The limiting factor is how bursty an average-target load could be before it became burdensome for nodes. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.712053571428571
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/protolambda",
                    "index": "8",
                    "likes": "0",
                    "time": "11/03/2022-21:37:14",
                    "content": "@yperbasis thank you for reporting those two issues! I updated the EIP here: EIP-4844: v -> y_parity like EIP-1559, and fix intrinsic gas by protolambda \u00b7 Pull Request #4904 \u00b7 ethereum/EIPs \u00b7 GitHub I\u2019ll check the go-ethereum fork now to make sure the changes are reflected. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 7.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/daniellehrner",
                    "index": "9",
                    "likes": "0",
                    "time": "14/03/2022-11:47:49",
                    "content": "Regarding the Gas price calculation it says:  Note that unlike existing transaction types, the gas cost is dependent on the pre-state of the block.  Do I understand this correctly that an exact gas price estimation is not possible with this new type of transaction? If that should indeed be the case, how should calls to eth_estimateGas with this transaction type be handled? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.6439393939393945
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/protolambda",
                    "index": "10",
                    "likes": "0",
                    "time": "14/03/2022-15:23:06",
                    "content": "Only the \u201cblobs\u201d part of the transaction is dependent on the pre-state. And the cost changes there are bounded by the EIP-1559-like fee adjustments. So eth_estimateGas will work like normal for the most part, but then we need to consider additional gas for the blob data. Maybe we can return a separate estimate for that? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.083333333333334
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/daniellehrner",
                    "index": "11",
                    "likes": "0",
                    "time": "15/03/2022-10:35:08",
                    "content": "    protolambda:  So eth_estimateGas will work like normal for the most part, but then we need to consider additional gas for the blob data. Maybe we can return a separate estimate for that?   Yes, I think we should definitely reflect it in eth_estimateGas. In my experience the endpoint is often used to retrieve which value to set as the gas limit of a transaction. So it definitely should return a result which takes the additional cost of the blob into consideration. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.8125
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/wschwab",
                    "index": "12",
                    "likes": "0",
                    "time": "15/03/2022-19:22:37",
                    "content": "Should this be getting pushed into Review, or is it still too early? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/nibnalin",
                    "index": "13",
                    "likes": "1",
                    "time": "21/03/2022-15:05:39",
                    "content": "Hi folks, big fan of this EIP and the direction this takes Ethereum in! I had one question/suggestion: The proposed KZG commitment is quite useful for rollups that execute inside the EVM, but one other use case for guaranteed data availability of large off chain, short lived blobs is in enabling a new class of SNARK based applications. In particular, blobs (in theory) enable the usage of off-chain blobs as hidden inputs to a SNARK circuit, and the circuit can attest that the hash of the blob matches the on-chain commitment and has valid properties/transformations. I can think of many applications of this, as an example, here\u2019s an idea made possible by this:  You store params for a trusted setup ceremony in a blob, and a guaranteed hash of these on-chain. Every time a new participant wants to be part of this ceremony, they use the off-chain blob to generate the new params and put them in a new blob, along with a snark proof that their contribution transitions correctly from a blob that hashed to the previous on chain hash to the new hash.  Of course, there are many other classes of such applications that could benefit from such a data availability model that would be ideal for running inside a SNARK (thanks to their succinctness properties). However, this brings me to my suggestion: Add an option to store the on-chain commitment in a SNARK friendly method. KZG commitments require a pairing check which is notoriously hard to implement in a SNARK with the current most popular schemes (Groth/PLONK) since constraints blow up with each \u201cbigint\u201d operation. So, adding a SNARK friendly commitment would enable for more practical SNARK based applications. I would suggest the option to add a merkle tree using a SNARK friendly accumulator function (such as Poseidon/MIMC). Of course, at the end, it is a question of whether or not the added complexity to the interface and the difficulty of implementation is worth it for enabling such applications (that are relatively unproven), so I\u2019d be curious to hear thoughts and considerations of that. Thanks! P.S. This is my first time commenting here  Let me know if there\u2019s anything i should add/detail. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.83956358956359
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/vbuterin",
                    "index": "14",
                    "likes": "3",
                    "time": "21/03/2022-15:44:15",
                    "content": "    nibnalin:  However, this brings me to my suggestion: Add an option to store the on-chain commitment in a SNARK friendly method. KZG commitments require a pairing check which is notoriously hard to implement in a SNARK   KZG commitments are actually very SNARK friendly. The trick is that you don\u2019t take the \u201cnaive approach\u201d of actually trying to verify the KZG inside the SNARK directly. Instead, you just directly use the KZG point as a public input (this allows you to directly access everything inside the KZG in systems like PLONK). If you want to make a SNARK over something other than the BLS-12-381 curve or over a different trusted setup, then there is a proof of equivalence protocol that allows you to make another commitment $D$ that is compatible with your SNARK protocol, and prove that $D$ and the KZG commit to the same data. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.347222222222222
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/web3Mike",
                    "index": "15",
                    "likes": "0",
                    "time": "21/04/2022-02:50:36",
                    "content": "This is a great proposal. I have a question that others may not be particularly concerned about: how does EIP4844 (and Full Sharding after that) guarantee that nodes retain blob data for a specific amount of time (say a month)? I see in the EIP that blob data is deleted after 30 days, but there is no specific scheme to guarantee this ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 6.316666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/optimalbrew",
                    "index": "16",
                    "likes": "0",
                    "time": "25/04/2022-18:55:25",
                    "content": " deleted after 30 days, but there is no specific scheme to guarantee this  Perhaps you mean the guarantee that it is stored for at least 30 days. Node operators can choose when they actually purge / delete. Vitalik\u2019s FAQs mentions cases where some will retain data longer. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 4.234375
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/qizhou",
                    "index": "17",
                    "likes": "0",
                    "time": "16/06/2022-03:44:57",
                    "content": "Very nice proposal!  Just a few questions for clarification:  Beacon chain validation On the consensus-layer the blobs are now referenced, but not fully encoded, in the beacon block body.  How the blobs in beacon blocks are referenced?  Would beacon blocks also include the Tx or other data structure to reference? Following the previous question - since the blocks of CL and EL are produced asynchronously, what is the expected sequence of including a tx-without-blob in EL and referencing blob in CL?  Further, how could we ensure that both EL and CL do the correct work (e.g., a tx-without-blob is included in EL, but no such blob is referenced in CL?) ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.239583333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "18",
                    "likes": "0",
                    "time": "08/09/2022-20:47:14",
                    "content": " We add an opcode DATAHASH (with byte value HASH_OPCODE_BYTE ) which takes as input one stack argument index , and returns tx.message.blob_versioned_hashes[index] if index < len(tx.message.blob_versioned_hashes) , and otherwise zero. The opcode has a gas cost of HASH_OPCODE_GAS .  Was a name other than DATAHASH considered? I think it is too similar to CALLDATA as well as potentially accessing the \u201cdata\u201d portion of an account code could have such opcodes. I\u2019d propose to use something akin to BLOBHASH or TXBLOBHASH to start utilising a TX prefix. In connection to EIP-1803: Rename opcodes for clarity we discussed prefixing opcodes according to their role (BLOCK, TX, \u2026) ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 4.84375
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "19",
                    "likes": "0",
                    "time": "08/09/2022-20:50:57",
                    "content": "def point_evaluation_precompile(input: Bytes) -> Bytes:     # Verify P(z) = a     # versioned hash: first 32 bytes     versioned_hash = input[:32]     # Evaluation point: next 32 bytes     x = int.from_bytes(input[32:64], 'little')     assert x < BLS_MODULUS     # Expected output: next 32 bytes     y = int.from_bytes(input[64:96], 'little')     assert y < BLS_MODULUS     # The remaining data will always be the proof, including in future versions     # input kzg point: next 48 bytes     data_kzg = input[96:144]     assert kzg_to_versioned_hash(data_kzg) == versioned_hash     # Quotient kzg: next 48 bytes     quotient_kzg = input[144:192]     assert verify_kzg_proof(data_kzg, x, y, quotient_kzg)     return Bytes([])  The precompile uses little endian byte order for certain inputs. Currently the execution layer exclusively uses big endian notation, while the consensus layer (beacon chain) uses little endian. While this proposals makes this data opaque to the EVM (to be just passed through to this precompile), it feels like trading consistency of the execution layer in favour of consistency within the consensus layer. I do not have any proposed solution here, just interested in opinions and views around the reasoning for this. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.092032967032967
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "20",
                    "likes": "0",
                    "time": "08/09/2022-20:58:16",
                    "content": "Furthermore, I think the description of the precompile is not entirely clear. I assume the assert statements if hit, will result in an OOG outcome (i.e. consume all passed gas in the call). While the successful run will not result in an OOG case, but will return 0 bytes (i.e. returndatasize will equal 0). Furthermore it is unclear whether it pads inputs with zeroes, or would signal failure if the input is not exactly 192 bytes. Is that correct? The closest precompile to this is ECPAIRING (EIP-197: Precompiled contracts for optimal ate pairing check on the elliptic curve alt_bn128) which returns U256(0) or U256(1) depending on the outcome. I think it is nice to avoid the need for checking return values if this can be delegated to only checking the success value of CALL, but nevertheless it is something breaking consistency with other precompiles. In this case likely it makes sense however. Suggestion is to just clarify the description in the EIP. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.516666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "21",
                    "likes": "0",
                    "time": "08/09/2022-21:12:06",
                    "content": "class ECDSASignature(Container):     y_parity: boolean     r: uint256     s: uint256  Nice to see that the new transaction format replaces the RLP transaction encoding with SSZ and moves chain_id to be a field allowing for using boolean for `y_parity. There\u2019s potential for some optimisation here at the expensive of some clarity: EIP-2098: Compact Signature Representation Was this discussed yet? Edit: the parity field seems to have a 1 byte overhead so this is neglible. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.295454545454545
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "22",
                    "likes": "0",
                    "time": "09/09/2022-11:01:21",
                    "content": "intrinsic_gas = 20000  # G_transaction, previously 21000  If the intent is to change G_transaction from 21000 to 20000 that should be a separate EIP that includes rationale for the change just like every other gas cost change. If the intent is to have this specific transaction type have a different intrinsic cost from other transactions then this EIP should include rationale on why these transactions have a lower base operational overhead than other types of transactions. If the intent is to try to incentivize certain behaviors, we should not be hijacking our operational cost pricing mechanism to try to achieve that. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 4.295386904761905
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gakonst",
                    "index": "23",
                    "likes": "1",
                    "time": "10/09/2022-01:37:44",
                    "content": "Went ahead and reverted that change: fix(4844): leave G_transaction at 21000 by gakonst \u00b7 Pull Request #5636 \u00b7 ethereum/EIPs \u00b7 GitHub ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/valerini",
                    "index": "24",
                    "likes": "0",
                    "time": "14/09/2022-06:39:39",
                    "content": "Great proposal! Quick question, @protolambda , why does the signature cover the blob-data, instead of only covering the commitment(s) to the blob-data? It seems that for Danksharding, validators will not be receiving all of the blobs (only the commitments), yet they\u2019ll still have to verify the signatures. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 6.666666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "25",
                    "likes": "0",
                    "time": "21/10/2022-10:32:20",
                    "content": "I made this for myself, just so I can anticipate what this will do to the nodes running on my machines. Sorry for any mistakes. I\u2019ll correct it if there are any. Please let me know. Thought I\u2019d share:  image1594\u00d7822 59 KB  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 2.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/acolytec3",
                    "index": "26",
                    "likes": "0",
                    "time": "09/11/2022-20:20:51",
                    "content": "Can we add something like below from 2718 to the beginning of the specification section to define the  || operator? It\u2019s used in several places in the EIP and can be interpreted in a variety of ways, depending on which programming language(s) one happens to be familiar with.  Definitions   || is the byte/byte-array concatenation operator.   ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.9375
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Pandapip1",
                    "index": "27",
                    "likes": "0",
                    "time": "04/01/2023-21:24:58",
                    "content": "Moving the discussion from Clarify & rationalize blob transaction hash computations in EIP-4844 \u00b7 Issue #6245 \u00b7 ethereum/EIPs \u00b7 GitHub here. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/etan-status",
                    "index": "28",
                    "likes": "0",
                    "time": "08/02/2023-12:45:24",
                    "content": "  Enable 0-blob transactions to use SSZ format: On devp2p eth/68, 4844 suggests that the EIP-2718 transaction type is advertised to indicate Gossip and mempool behaviour. The EIP-2718 transaction type should solely be used to (1) denote the serialization format of a tx on the wire (also, as part of GetBlockBodys), (2) for the purpose of signing, and (3) for deriving the transaction hash. Giving it mempool specific meaning leads to problems and unnecessary discussions, e.g., for the 0-blob transaction case. It would be preferrable to advertise the number (or presence) of blobs instead of the transaction type on eth/68. Type 5 based 0-blob transactions could then be prossed the same as any type 0x01, 0x02, or 0xc0-0xfe transactions; Transactions with blobs (of type 0x05, or any future types that also have blobs) would use the req/resp based solution using the network wrapper.   SSZ encoding: EIP-6475: SSZ Optional SSZ Optional[Address] is preferrable over Union[None, Address], as it makes the SSZ merkle tree shape static, meaning that proof requests including the address can\u2019t randomly fail based on union selector. Note that for fixed length items such as Address, Optional[Address] is equivalent to List[Address, 1] for the purpose of SSZ serialization and merkleization.   Constant tuning: MAX_VERSIONED_HASHES_LIST_SIZE is matching the maximum number of blobs per transaction, and is currently set to 16 million. This exceeds any rational design space; keep in mind that SSZ serialization caps out at 4 GB, so such transactions could not even be serialized. If devp2p eth/68 is updated to indicate the number of blobs, note that this number also is part of the advertisement and making it fit into a uint8 or uint16 may be desirable.   ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.041666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/roberto-bayardo",
                    "index": "29",
                    "likes": "1",
                    "time": "10/02/2023-23:41:16",
                    "content": "There\u2019s been ongoing discussion around mempool DoS concerns, so perhaps we can start adding more recommendations to the spec around mempool handling of blob txs.  Right now the spec suggests increasing data gas price by at least 10% for replacement, and the mempool already requires increasing regular gas price by 10% for replacement.  Additional constraints that prevent specific DoS scenarios without being too burdensome on clients include:   Blob-holding txs should only be replaced by blob txs consuming at least as much datagas (e.g. # of blobs can never decrease).  This prevents mempools from being spammed with multiple-blob txs to have them later deleted by (much cheaper) 0 or 1 blob txs.   There can only be one blob-containing tx per account. This prevents someone from spamming the mempool with multiple blob-holding txs each with sequential nonces in a way where none of them beyond the first would successfully execute & incur fees.   The suggestion from Etan in a comment above around announcing # of blobs in eth/68 instead of tx type would also help mempools better deal with blob-related DoS risk. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.651785714285714
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/etan-status",
                    "index": "30",
                    "likes": "1",
                    "time": "11/02/2023-11:26:57",
                    "content": "Made a concrete proposal for eth/68 changes here:  EIP-5793: eth/68 Add transaction type to tx announcement - #2 by etan-status  For SSZ Optional, opened a PR:  Update EIP-4844: Use SSZ `Optional` for `Address` by etan-status \u00b7 Pull Request #6495 \u00b7 ethereum/EIPs \u00b7 GitHub  ",
                    "links": [
                        "https://github.com/ethereum/EIPs/pull/6495/files"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/roberto-bayardo",
                    "index": "31",
                    "likes": "0",
                    "time": "21/02/2023-16:15:27",
                    "content": "Re: data blob transaction replacement. From today\u2019s 4844 client devs call, Dankrad noted that requiring increasing data gas (whether 10 or 100%) for blob tx replacement may not be a suitable disincentive for DoS since the gas is priced using 1559-type rules. Ansgar suggested we might then also require that replacement txs contain the exact blob as before (so it would not have to be reverified).  Depending on if it\u2019s important, we could also allow replacement txs to add additional blobs, though this may be too much of an edge case to worry about. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 6.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ryanschneider",
                    "index": "32",
                    "likes": "2",
                    "time": "27/02/2023-21:38:40",
                    "content": "Hey I just noticed that EIP-4844 doesn\u2019t actually specify the ReceiptPayload  format anywhere and probably should.  For example EIP-1559 includes the line:  The EIP-2718 ReceiptPayload for this transaction is rlp([status, cumulative_transaction_gas_used, logs_bloom, logs])  But, before we add the ReceiptPayload I\u2019m curious what others think of this idea I expressed on the ACD discord, where we remove the logs_bloom field from type 0x5 tx receipts?  As discussed over there, most tx receipts only include a single event if any, so dedicating a full 256 bytes to the logs_bloom in the tx receipt feels a bit heavy (if the number of events in a tx receipt is small, it\u2019s probably more effecient to just go ahead and scan the events vs. using the bloom since you have to scan the events anyways to double check against false positives in the bloom filter). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.149350649350649
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/roberto-bayardo",
                    "index": "33",
                    "likes": "0",
                    "time": "28/02/2023-20:11:46",
                    "content": "Re: Receipts, it was also noted in discord during the workshop that we should return dataGasUsed as part of the non-consensus receipt values. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/roberto-bayardo",
                    "index": "34",
                    "likes": "0",
                    "time": "07/03/2023-16:22:51",
                    "content": "On today\u2019s 4844 client devs call, Marius from geth suggested we consider making the blob tx type more specialized, e.g. by removing access list and ability to create contracts.  Wanted to open that up for discussion here.   Does anyone have any concrete use cases for access lists and/or contracts in blob txs? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 6.083333333333334
                }
            ]
        }
    ],
    "group_index": "1623"
}