{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/my-technical-take-on-progpows-weakest-link/2983",
            "title": "My technical take on ProgPow's weakest link ",
            "index": 2983,
            "category": [
                "EIPs"
            ],
            "tags": [
                "progpow"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "1",
                    "likes": "4",
                    "time": "25/03/2019-15:30:29",
                    "content": "First let me say that I\u2019ve worked extensively with the algorithm, and as far as I\u2019m concerned it is sound and meets its objective of evening the mining playing field. As proposed ProgPow creates a new pseudo random sequence of OpenCL or CUDA code every 10 blocks or so. In order to run on the GPUs this code is just-in-time recompiled at run-time with each new ProgPow period. In an ideal world that would be fine, but compilers (specially optimizing compilers) are incredibly complex beasts and are subject to undiscovered and subtle bugs. Case in point, while working with ProgPow I stumbled  on such a bug in all current AMD drivers (both Linux and Windows). Progress in resolving the issue at AMD is slow as the bug report was opened and acknowledged some 2 months ago (initially announced by ifdefelse here: https://medium.com/@ifdefelse/progpow-progress-da5bb31a651b) . In that instance the high level code was generated correctly by the miner but for some random sequences and ProgPow periods the generated GPU code was incorrectly compiled. Workarounds where found and implemented, but it got me to thinking\u2026 what if something like this had happened on the main net? All of the AMD miners (a majority of which use the driver in question) would start generating invalid solutions for all blocks within any given and unpredictable ProgPow period. Fairly catastrophic events! Reliance on the dynamic use of 3rd party compilers is the weakest link in the ProgPow chain. OpenCL and CUDA compiler versions can change with any driver release, and ProgPow has many attributes of a compiler fuzz tester. It becomes a prime candidate for exposing these unknown compiler optimization issues. Given the pseudo random nature of the dynamically generated code, the nearly infinite variations, and the fact that it is only compiled at run time, it is nearly impossible to do any advance testing of these code sequences. I wouldn\u2019t ignore the likelihood of such bugs cropping up unpredictably in the future. ",
                    "links": [
                        "https://ethereum-magicians.org/t/governance-concerns-after-listening-to-all-progpow-discussions-on-core-dev-calls/2992/3",
                        "https://ethereum-magicians.org/t/progpow-a-compilation-of-reference-material/3040/4"
                    ],
                    "GPT-summary": "The author of the post explains the ProgPow algorithm and how it creates a new pseudo random sequence of OpenCL or CUDA code every 10 blocks or so. The author also highlights a bug in all current AMD drivers that was discovered while working with ProgPow. The post suggests that reliance on the dynamic use of 3rd party compilers is the weakest link in the ProgPow chain and that it becomes a prime candidate for exposing unknown compiler optimization issues. The post provides constructive criticism of the proposal and audits and reviews the algorithm.",
                    "GPT-proposal-categories": [
                        "Privacy, Security and risk management",
                        "Token economics",
                        "Smart contract updates",
                        "Interoperability and Scaleability",
                        "None"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.110861592111592
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "2",
                    "likes": "1",
                    "time": "25/03/2019-18:08:35",
                    "content": "I\u2019m placing a link to this discussion on AllCoreDevs because I think it is a potential showstopper. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "3",
                    "likes": "1",
                    "time": "25/03/2019-18:32:26",
                    "content": "I think very good mitigations exist and are already practiced. First, full scale impact on a newly introduced bug in a compiler update is unlikely.  There are many strategies for that such as canaries and incremental rollout to address such regular critical bugs.  Microsoft just had one with Windows where entire drives were being corrupted, but it was discovered during incremental rollout. Any DevOps worker that gets an OS, driver, or toolchain update and immediately rolls it out to 100% of the fleet will not be in the industry long. Second, the impact is limited.  It only affects the hashrate of the particular card vendor on the particular toolchain version.  This would not impact the transactions encoded in the block bodies nor any of the other data, it would only impact the proof of work generation.  If a nightmare scenario hit and half the cards went offline the difficulty would correct itself in a number of hours.  Figure in the fact the program changes every 10 blocks it may self correct sooner.  it is nearly impossible to do any advance testing of these code sequences  False, the upgrades can be validated well in advance.  The programs are deterministic based only on block number and not on the content of the chain.  Correct hashing can be tested and program performance can be forecasted well in advance of the actual block mining.  Just run the miner with the new targeted block number.  This may become a recommended practice to \u201cfuture proof\u201d a certain amount of blocks to validate that there will be no compiler problems. Certainly worth discussing in core devs, but I don\u2019t foresee it becoming a showstopper, but a risk a miner would need to plan around.  The mining community may have a different opinion on this however. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.275962830273175
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "4",
                    "likes": "1",
                    "time": "25/03/2019-19:21:09",
                    "content": "What @jean-m-cyr has found is true and and it\u2019s been reported to AMD engineers. The so called \u201cbogus period\u201d flaw affects, actually, only AMD OpenCL compiler. Nvidia OpenCL compiler is not affected. With a trial and error procedure was also found a workaround which (interestingly enough) also slightly improved performance. #pragma unroll 2 on progpowLoop. This said is pretty easy to test, at current releases of drivers (any), periods from here to 3 or 4 years ahead. The problem itself is mitigated however. Most of the OpenCL kernels for ethash too are compiled on the fly when the instance of a miner starts: there are some exceptions though when OpenCL kernels are delivered bound to the miner in binary (pre-compiled) format. In history of software driven miners is not the first time a driver upgrade causes erratic results or serious losses in performance. But when this happens it\u2019s pretty obvious (for miner) to roll back to previous versions while the issue is being reported to respective engineers. AMD for example had to release the so called \u201cBlockChain drivers\u201d to solve several issues caused by increased DAG size. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.637254901960784
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "5",
                    "likes": "0",
                    "time": "25/03/2019-19:48:15",
                    "content": "You are correct that OpenCL code is mostly compiled on the fly. The difference is that Ethash code is static and can be thoroughly tested prior to release. Not the case with with pseudo randomly generated code. ",
                    "links": [],
                    "GPT-discussion-categories": [],
                    "Sentiment": 6.3
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/salanki",
                    "index": "6",
                    "likes": "1",
                    "time": "25/03/2019-20:18:07",
                    "content": "That things can break when you update a driver is a known fact. The number of people who mine and just let Windows Update auto-update their drivers are very small. Most of the network is run in Hive OS, ethOS and vanilla Linux. The two former do rigorous testing before they include a new driver. With that said, I would recommend that miner software developers include a regression testing mode that runs through a bunch of ProgPoW periods to allow easier testing of new driver testing. Since the programs are pseudo-random and not truly random, one can conclusively test all generated programs ahead of time. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 4.9564393939393945
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "7",
                    "likes": "1",
                    "time": "25/03/2019-20:22:22",
                    "content": "    jcyr:  Not the case with with pseudo randomly generated code.   The code is deterministic in its output, purely as a function of the block number.  This can be validated and tested in a brute-force fashion in O(n) time. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.2857142857142865
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "8",
                    "likes": "0",
                    "time": "25/03/2019-20:23:34",
                    "content": "It\u2019s good that you acknowledge the issue and are thinking about ways to mitigate. In assessing the risk however we must also accept that verifying static code, as opposed to dynamically generated pseudo-random code known to stress optimizing compilers, are two entirely different things. ",
                    "links": [],
                    "GPT-discussion-categories": [],
                    "Sentiment": 7.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "9",
                    "likes": "1",
                    "time": "25/03/2019-20:57:14",
                    "content": "So far it has been suggested that all ProgPow periods 2-3 years into the future could be generated and tested prior to release, combined with strict driver and toolchain version verification to guard against potentially incompatible or unverified tools. Less general, more error prone, but would address the issue. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "None",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.345238095238095
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "10",
                    "likes": "1",
                    "time": "26/03/2019-00:22:27",
                    "content": "With the progpow period at 10 blocks, and avg. block time at 12 minutes. Testing 2 years worth of ProgPow sequences represents nearly 9,000 test cases! Then you\u2019d need to test for all architectures, but we already have to do that now. I\u2019d much rather constrain the number of possible randomized sequences to a number that is still prohibitive for ASICs but that can be statically compiled. Not a new idea, I know. Other aspects of ProgPow such as large state, heavy use of fast on-chip memory already make the algorithm expensive for ASIC. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.066410861865407
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "11",
                    "likes": "1",
                    "time": "26/03/2019-01:14:55",
                    "content": "The question is how fast is the compiler.  10ms is 90 seconds.1 second is 2 and a half hours.  Not unreasonable for a final check test.  And what you really care about is the next few months. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.023809523809524
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "12",
                    "likes": "0",
                    "time": "26/03/2019-10:04:36",
                    "content": "@shemnon The speed of the compiler is irrelevant. As compilation is done async while previous compiled kernel is searching its weight is negligible. Just for sake of precision an avg compile time goes from 250ms to 1.2 s depending on CPU and IO speed. At miner instance startup the compilation happens while DAG is being generated thus, once again, no delays. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 3.333333333333334
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "13",
                    "likes": "0",
                    "time": "26/03/2019-10:47:00",
                    "content": "@Anlan I believe @shemnon was referring to the time it would take to run the requisite 2-3 years worth of ProgPow period testcases. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "14",
                    "likes": "0",
                    "time": "26/03/2019-10:57:21",
                    "content": "Not directly related to this specific issue, but another friction point is the Windows DLL hell miner devs would need to contend with. The run-time compiler support DLLs for Nvidia are only included in the CUDA toolkit, a 2+GB download. My understanding of the Nvidia license is that those DLLs can\u2019t be delivered separately from the entire toolkit. The current method of packaging the DLLs with the miner seems to violate the terms of the Nvidia license. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "15",
                    "likes": "0",
                    "time": "26/03/2019-10:57:50",
                    "content": "Oh I see \u2026 but then again the speed of compiler is irrelevant. What really matters is how much time a search kernel takes to find the right nonce which is a matter of the difficulty we target to and the champion machine(s) we use (1 Gpu ? 2 ? 6 ?). Assuming a fixed header hash and a very low diff the whole test can be carried out maybe quicker than 2 hours. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.346938775510203
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "16",
                    "likes": "0",
                    "time": "26/03/2019-11:01:01",
                    "content": "In regard of DLLs my initial intention was( and is ) to completely remove any nvidia library in the miner package. Instead it will detect if necessary libraries are installed and eventually it\u2019s up to the end user to install them from vendor. \u201cYou have to install CUDA toolkit from \u2026\u201d ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.166666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "17",
                    "likes": "0",
                    "time": "26/03/2019-11:02:35",
                    "content": "Sure, you can place the onus on the user to sort things out. I guess my gut feeling is that we are building a cumbersome and fragile construct. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "18",
                    "likes": "1",
                    "time": "26/03/2019-11:06:48",
                    "content": "I may be well wrong but I don\u2019t see it as a huge problem. While I agree most \u201coccasional\u201d miners are \u201ccopy and paste -> next -> next save\u201d almost not knowing what they\u2019re doing, on a development perspective is way easier to mantain a miner which is not bound to particular CUDA releases or Driver versions. Actually mainstream ethminer has a release for CUDA 8, one for CUDA 9.x and one for CUDA 10 due to the nvidia fat loader. It all ends up in providing the most exhaustive knowledge base. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.070833333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/xazax310",
                    "index": "19",
                    "likes": "0",
                    "time": "26/03/2019-13:09:31",
                    "content": "As a miner, I\u2019d like to chime in here and give me a short idea of what a typical miner would look for in a miner program. For the most part, as Andrea has said, many wouldn\u2019t even bother doing extensive testing. A) Does it work? B) Is my hashrate as advertised by the miner? C) Are my submitted pool shares/payout correct for the hashrate. Check off all answers they start using it. It seems to me test the compiler for years on out in a minersoftware seems wasteful. At most I\u2019d say 6+ month lead time due to the changing landscape of crypto and further bug swatting/fixes. Again it\u2019s not ProgPoW Algorithm that\u2019s the failure here but something with AMD openCL compiler. @jcyr Isn\u2019t this something we could test/find out in a Testnet scenario? If there would be some major flaw that could cause any failures because of AMD miners. This would have become fairly evident. @Anlan I would assume since it\u2019s an AMD openCL issue, the ROCm wouldn\u2019t be affected by this same issue? This is something Linux users, a majority of miners which use, could totally avoid. SMOS, PimpOS, HiveOS, ETHos are popular Linux mining systems many use and I\u2019m sure would use ROCm. I believe HiveOS already does for Vega\u2019s. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.876302083333334
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "20",
                    "likes": "1",
                    "time": "26/03/2019-13:23:23",
                    "content": "@xazax310 We ran with this on a testnet for some time before it became evident. And yes, it is not the algorithm that bugs me, but rather it\u2019s increased dependence on 3rd party tools. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.625
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/greerso",
                    "index": "21",
                    "likes": "1",
                    "time": "26/03/2019-17:25:54",
                    "content": "nvrtc is distributable according to https://docs.nvidia.com/cuda/eula/#attachment-a ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "22",
                    "likes": "0",
                    "time": "26/03/2019-17:41:56",
                    "content": "@greerso Good news\u2026 But the license terms also include:  The distributable portions of the SDK shall only be accessed by your application.  and  Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK.  IANAL but it might be ok to bundle the nvrtc DLLs and SOs in some cases. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party or author is advertising proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 7.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "23",
                    "likes": "3",
                    "time": "26/03/2019-22:14:26",
                    "content": "Reading the code at https://github.com/ifdefelse/ProgPOW/blob/master/libprogpow/ProgPow.cpp it seems that instead of generating and compiling code a similar effect could be achieved by having code similar to what is generated by getKern() calling randomly into a (very large) set of pre-compiled math() and merge() functions. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 4.723214285714286
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "24",
                    "likes": "0",
                    "time": "27/03/2019-20:02:50",
                    "content": "    shemnon:  Second, the impact is limited. It only affects the hashrate of the particular card vendor on the particular toolchain version. This would not impact the transactions encoded in the block bodies nor any of the other data, it would only impact the proof of work generation. If a nightmare scenario hit and half the cards went offline the difficulty would correct itself in a number of hours. Figure in the fact the program changes every 10 blocks it may self correct sooner.   The worst-case scenario is not cards going offline, it\u2019s a fork, with all AMD cards on one side and NVIDIA cards on the other. The fork would persist even after the problematic epoch has passed, as each side would see the other side\u2019s blocks as invalid.     gcolvin:  Reading the code at https://github.com/ifdefelse/ProgPOW/blob/master/libprogpow/ProgPow.cpp  it seems that instead of generating and compiling code a similar effect could be achieved by having code similar to what is generated by getKern() calling randomly into a (very large) set of pre-compiled math() and merge() functions.   I believe the point of ProgPoW is that such a set of functions would be too large to practically implement and push to the GPU; if it weren\u2019t, you could build an ASIC that does this more efficiently. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.0626750700280105
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "25",
                    "likes": "2",
                    "time": "27/03/2019-20:29:33",
                    "content": "    Arachnid:  The worst-case scenario is not cards going offline, it\u2019s a fork, with all AMD cards on one side and NVIDIA cards on the other.   I don\u2019t think so. None of the clients do POW validation on GPUs, it\u2019s all software for them.  So instead of a fork you will get AMD GPUs generating bad blocks that no clients will propagate. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 2.937500000000001
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "26",
                    "likes": "0",
                    "time": "27/03/2019-20:31:20",
                    "content": "    Arachnid:  I believe the point of ProgPoW is that such a set of functions would be too large to practically implement and push to the GPU; if it weren\u2019t, you could build an ASIC that does this more efficiently.   ASICs can do it more efficiently.  The question is how much more efficiently.  The EIP preamble calls this out and provides an estimate at the maximum gains an ASIC could gain.  The aim of ProgPow was never to make ASICs impossible but to reduce the efficiency that they can gain relative to existing GPU architectures. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.748299319727891
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "27",
                    "likes": "1",
                    "time": "27/03/2019-20:34:16",
                    "content": "I\u2019m aware of that, but you\u2019re missing my point. For the conditions in the preamble to be true, the set of possible pipelines has to be too large to precompile and include in a binary as Greg is suggesting. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.614285714285714
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "28",
                    "likes": "1",
                    "time": "27/03/2019-20:34:56",
                    "content": "    shemnon:  I don\u2019t think so. None of the clients do POW validation on GPUs, it\u2019s all software for them. So instead of a fork you will get AMD GPUs generating bad blocks that no clients will propagate.   Good point, I stand corrected. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "29",
                    "likes": "0",
                    "time": "27/03/2019-20:58:14",
                    "content": "    Arachnid:  the set of possible pipelines has to be too large to precompile and include in a binary as Greg is suggesting.   Actually I would expect a good ASIC to have a software driver that would communicate the lane configurations.  Loading and configuring the calculations is not the long part of the process, once it\u2019s set up it runs for 2.5 minutes. The \u201cunpredictability\u201d of the calculations isn\u2019t the aim of progpow.  It\u2019s the variety of calculations over the life of the chain. If unpredictability across the chain is desired all it would take is to add in the block hash from sum number of blocks before the start of the period to change the kiss99 seed. But I\u2019m not sure how much \u201cefficiency\u201d that would cost because it is similarly calculable in a driver that can send out the config to an entire fleet of ASICs. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.174603174603175
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "30",
                    "likes": "0",
                    "time": "27/03/2019-21:02:01",
                    "content": "Again, not my point, but I seem to be unable to communicate my point clearly enough, so I give up. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 3.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "31",
                    "likes": "0",
                    "time": "28/03/2019-04:34:45",
                    "content": "    Arachnid:  I believe the point of ProgPoW is that such a set of functions would be too large to practically implement and push to the GPU; if it weren\u2019t, you could build an ASIC that does this more efficiently.   I think you are right, though I wasn\u2019t clear that a new executable would still be created each time.  No new code would be compiled, just different combinations of the same precompiled code linked together.  It would take some redesign of the kernel algorithm.  And the period of the \u201crandom program generator\u201d would be a lot less, but I don\u2019t down how much that would matter to ASIC resistance. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.435816498316498
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "32",
                    "likes": "1",
                    "time": "28/03/2019-05:45:56",
                    "content": "A reasonable analogy explaining an ASIC pipeline can be found here: http://chipverification.blogspot.com/2008/06/what-is-pipelining.html Each stage in an ASIC pipeline requires it\u2019s own copy of the state. Even with a simple SHA-256 hash, with a relatively small state, a fully unrolled pipeline will have a couple of hundred stages. The number of stages times state size dictates the number of on-chip storage bits required to accommodate the pipeline. ProgPow has a much larger state and a more complex algorithm which precludes a fully unrolled pipeline (too much expensive on-chip storage bits) forcing the ASIC designer to chose a more sequential, less unrolled, slower, and less power efficient design. I don\u2019t think you\u2019d need a very large number of pre-generated random sequences. I\u2019m guessing that 32 such sequences would be enough to encumber an ASIC and would still be GPU loadable. Certainly not as complex as the completely pseudo random merge but difficult enough! You\u2019d need to mock up the design and run simulations to get a better idea of the lower bound for the number of sequences. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.986961451247166
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "33",
                    "likes": "0",
                    "time": "28/03/2019-09:30:13",
                    "content": "@gcolvin. Technically speaking a compilation of a new search kernel from pseudo-random source on every period is not mandatory.  In the end the CPU validation implemented as a backup-verification into the miner itself or directly into node\u2019s code does not recompile anything. When thinking about parallel processing though (thus GPU mining) the removal, at maximum extent possible, from the kernel of all if, switch, select \u2026 braces has a huge impact on processing speed. A very small compile time (spent async when a previous search kernel is on duty) is well worth the increment in processing speed on GPU side. Delivering a miner which implement a statically compiled pattern would be meaningless: nobody would use it and all GPU miners would use the versions which implement async compile as they\u2019d give a higher hashrate. Due to the above is worth mentioning that on-the-fly compilation matches with GPU arch and can be optimized on behalf of various values detected from the GPU (eg. best work multiplier, compute capabilities etc). ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.373440285204991
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "34",
                    "likes": "2",
                    "time": "28/03/2019-10:05:11",
                    "content": "@xazax310 the issue of the bogus period found by @jcyr occurred in a very limited environment (if compared to the variety of GPU types and driver versions out there). IIRC was found on Ellesmere AMD family cards with driver 18.x with OpenCL 1.2 implementation embedded into ethminer. Neither Jean nor me had all the collection of all possible cards available for testing. I personally had no availability to test wether or not earlier versions of drivers affected the very same GPU architecture in the same way. Due to this I can\u2019t honestly affirm that this was exclusively a \u201cgeneric\u201d OpenCL issue on all AMD cards. I can report what we witnessed which is this : on AMD-GPU-PRO driver the issue was there. On AMD ROCm driver the issue did not appear. But the code generated by ethminer was the very same in both cases. So we had to assume it was something related to driver compiler. We\u2019re still waiting from AMD a more detailed report of what happened though even if we\u2019re informed they\u2019re working on it. To be very specific the \u201cerror\u201d was caused by a particular sequence of math operations. Say we had 3 operations named A B and C. When the sequence was A B C everything ok; when it was B A C everything stil ok; when sequence was B C A \u2026 something got messed up and the kernel produced results which for its inner logic were meant correct but failed the CPU verification. On the ROCm matter \u2026 ROCm is both a driver (btw with high(er) hardware requirements on the motherboard) and a platform. To be very honest the finding ROCm did not suffer from the issue was a lucky shot (I recall it was late night - for me - while testing and gittering with Jean and Dobromir). Worth to reiterate that ROCm driver was compiling the very same OpenCL 1.2 code. Having a full ROCm implementation is another matter. While, for instance, OpenCL forces code to a C99 syntax, with ROCm you have a C++ like syntax and you can use inline asm (like Nvidia/CUDA). This opens the scenario to new optimization possibilities. I conclude: I personally think the proposal of implementation of, say, 32 pre generated sequences would\u2019nt be worth the effort. GPU miners would always prefer the most performant solution they may find available and, atm, the async precompile is the best. It\u2019s up to nodes to verify the nonce is valid. I honestly don\u2019t feel to share the worries of \u201ccatastrophic events\u201d. As said we (developers) can easily, if supported, carry out extensive tests on a reasonably wide time span for future epochs/periods using the actual state of the art (current driver releases, known architectures etc) while keeping up-to date miner software over time. Plus, to my knowledge, GPU miners who un-advisedly install new software/drivers without a proper functioning/regression test (thus exposing their business plan - if any - to the risk of being voided) are very few. Even on Windows the mostly advised hint given in all forums, channels, whatever is to disable Windows automatic updates. The possibility to get to a situation so critical where all gpu types, with any driver version, could all of sudden produce only invalid results (thus causing the halt of the chain) is, imho, == NULL. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.828689273689274
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "35",
                    "likes": "0",
                    "time": "28/03/2019-12:26:33",
                    "content": "To be more factual the error occurred will all versions of AMDGPU and AMDGPU-PRO for Windows or Linux, the drivers used by the majority of AMD users. It is really unknowable how and when this type of error will occur. I have no clue what you mean when you say that dynamically compiled code would be faster? Based on the often cited Bitcoin experience, many miners have this irrational fear of ASICs due to a belief in some kind of magical superpower. ASICs achieve their performance and power efficiency in two ways. By using optimized dedicated circuitry for logic and math operations, and by maximizing the use of all of on-chip circuitry via pipelining. The 1st gen. Ethash ASICs were not at all impressive, demonstrating that the type of algorithm matters. For a simple hash algorithm like SHA256d, with a small state and a shorter hash operator sequence, ASICs have a huge advantage. With an algorithm like Ethash where the mix portion of the algorithm imposes external memory access and stalls the entire pipe with each access, many of an ASIC\u2019s advantages disappear. Adding to that a large state and multiple computational paths further shrinks their advantage. Correct me if I\u2019m wrong, but the underlying strategy with ProgPow is to force a dedicated ASIC to operate more like a GPU ASIC, thus evening things out. ProgPow as it stands hits that objective, but you\u2019ll have a hard time convincing me that added run-time complexity and increased reliance on external components doesn\u2019t lower reliability. Just wanted to make that clear. I understand that changing course at this stage, with EIP rewrites, etc. that it would entail is also a major consideration. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.81952380952381
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "36",
                    "likes": "0",
                    "time": "28/03/2019-13:09:41",
                    "content": " I have no clue what you mean when you say that dynamically compiled code would be faster?  It seems to me pretty obvious in ProgPoW scope. The purpose of code generator is to provide the GPU a kernel freed from all conditional braces which drive the \u201cperiod\u201d adjustment. I think we can all agree that If(cond) {     doThis(); } else {     doThat(); }  will have an execution time higher than doThis();  when the \u201cprogram\u201d have to be executed gazillions of times.  Based on the often cited Bitcoin experience, [cut]  The matter here is not technical about ASIC efficiency rather than related to the ecosystem they imply. By extreme simplification if I had to see ASIC machines sold on store shelves along with GPUs I\u2019d have no problem with them.  Correct me if I\u2019m wrong, but the underlying strategy with ProgPow is to force a dedicated ASIC to operate more like a GPU ASIC  You\u2019re not wrong. That\u2019s what ProgPoW proponents have stated from day one.  but you\u2019ll have a hard time convincing me that added run-time complexity and increased reliance on external components doesn\u2019t lower reliability.  I\u2019m pretty sure I can\u2019t convince you about anything. Nevertheless the whole digital world relies on third party components. It\u2019s everywhere. A flaw in OpenSSL could make damages even worse. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.281798245614036
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "37",
                    "likes": "2",
                    "time": "28/03/2019-15:21:35",
                    "content": "You don\u2019t really need to convince me\u2026 It\u2019s just an idiom. It is a generally accepted engineering property that increased complexity will lower reliability. Granted, it is one factor of many in evaluating ProgPow. I meant the thread\u2019s title quite literally, and it is hard to quantify (assign weights) to these factors. If this issue is indeed insignificant and is the \u2018weakest\u2019 part of the implementation, then I\u2019d say ProgPow is in pretty good shape. I doubt very much that an audit of the actual algorithm will yield any important surprises. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.149074074074075
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "38",
                    "likes": "0",
                    "time": "28/03/2019-15:34:16",
                    "content": "    Anlan:  It seems to me pretty obvious in ProgPoW scope. The purpose of code generator is to provide the GPU a kernel freed from all conditional braces which drive the \u201cperiod\u201d adjustment. I think we can all agree that If(cond) {     doThis(); } else {     doThat(); }  will have an execution time higher than doThis();    It\u2019s not like the GPU miner could choose to do one or the other. As long as all miners have to run the same algorithm, speed doesn\u2019t matter.     Anlan:  The matter here is not technical about ASIC efficiency rather than related to the ecosystem they imply. By extreme simplification if I had to see ASIC machines sold on store shelves along with GPUs I\u2019d have no problem with them.   I only pointed out a technical issue.     Anlan:  A flaw in OpenSSL could make damages even worse.   OpenSSL is statically linked and runs on the host, not on the GPU. Need a better example. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.115384615384615
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "39",
                    "likes": "1",
                    "time": "28/03/2019-15:52:12",
                    "content": "The tangible (actionable) takeaways I get from this thread are:  Miner devs will need to test some number of ProgPow periods into the future. Implement stricter control over driver and tooling versions.  ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "40",
                    "likes": "0",
                    "time": "28/03/2019-16:09:02",
                    "content": "    jcyr:  It\u2019s not like the GPU miner could choose to do one or the other. As long as all miners have to run the same algorithm, speed doesn\u2019t matter.   Yes they can choose. If I know that for a certain period the condition will always be such to drive doThis() I prefer to compile a kernel without the \u201cIf\u201d, let it run for what is necessary, and when condition change replace with another kernel. Even at period height of 5 this has advantages. If, on the other hand, you want to apply conditional logic into the kernel, then you have enforce logic over something more volatile like, for example, workpackage header.     jcyr:  OpenSSL is statically linked and runs on the host, not on the GPU. Need a better example.   It\u2019s only a more \u201cgeneric\u201d example of how everthing in the stack can have dependencies from third party components. But I am sure you got the point. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.736263736263735
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "41",
                    "likes": "0",
                    "time": "28/03/2019-16:15:49",
                    "content": "    Anlan:  Yes they can choose. If I know that for a certain period the condition will always be such to drive doThis() I prefer to compile a kernel without the \u201cIf\u201d, let it run for what is necessary, and when condition change replace with another kernel. Even at period height of 5 this has advantages.   In theory sure. In practice I doubt you could measure a difference. You\u2019re talking about a few extra instructions to do an indexed load from a branch table, then a jump to the entry\u2019s content. Also do we know that Nvidia\u2019s just-in-time nvrtc compiler does as good a job optimizing as the command-line compiler nvcc? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.86734693877551
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "42",
                    "likes": "1",
                    "time": "28/03/2019-16:39:11",
                    "content": "    Anlan:  Technically speaking a compilation of a new search kernel from pseudo-random source on every period is not mandatory. \u2026   \u2026 but I think I get from your argument that it improves performance on GPUs enough that if it is possible miners will do it, so there is no point make it unnecessary.  Do I understand you? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.736363636363636
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "43",
                    "likes": "1",
                    "time": "28/03/2019-16:57:12",
                    "content": "@gcolvin that is exactly what I meant ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "44",
                    "likes": "0",
                    "time": "28/03/2019-17:25:47",
                    "content": "    jcyr:  Also do we know that Nvidia\u2019s just-in-time nvrtc compiler does as good a job optimizing as the command-line compiler nvcc?   According to CUDA documentation the \u201cextra\u201d -O cli argument supplied by nvcc has effect only on \u201chost\u201d code (https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior-optimize). NVCC is meant to do a lot more than simply compiling a device kernel. It links in fatbinary host and device code (so you can invoke kernels using <<<>>>), allows profilation of code, allows usage of host and device context mixed etc. But for ptx generation both nvcc and nvrtc produce the very same output (tested). ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "45",
                    "likes": "0",
                    "time": "28/03/2019-17:50:34",
                    "content": "    jcyr:  In theory sure. In practice I doubt you could measure a difference. You\u2019re talking about a few extra instructions to do an indexed load from a branch table, then a jump to the entry\u2019s content.   Not true. Remove conditionals, remove index increments, remove everything not necessary, use inline asm (when possible) during the immutability period and you easily gain hashes and hashes per second from kernels. But you should already know that having contributed to ethminer for so long and having witnessed all the small adjustments that have brought ethminer to be on par with claymore (at least on CUDA). ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.981060606060606
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "46",
                    "likes": "0",
                    "time": "28/03/2019-20:14:37",
                    "content": "    Anlan:  Not true. Remove conditionals, remove index increments, remove everything not necessary, use inline asm (when possible) during the immutability period and you easily gain hashes and hashes per second from kernels.   That\u2019s all very speculative What you\u2019re saying is that: invoke_super_optiminzed_inlined_seqience_for_period(n);  will be significantly faster than: switch (period) {   case n:      invoke_super_optiminzed_inlined_seqience_for_period(n);      break;   case n + 1:      invoke_super_optiminzed_inlined_seqience_for_period(n + 1);      break;   case n + 2:      invoke_super_optiminzed_inlined_seqience_for_period(n + 2);      break;   case n + 3:      invoke_super_optiminzed_inlined_seqience_for_period(n + 3);      break;   ... }  I really don\u2019t think it would be perceptibly slower for any given n. For a sequential integral switch statement selector a compiler will generate an indexed load from a static table of branch addresses, followed with a jump to that address, and a branch out of the switch at the end.  Not much overhead considering the amount of stuff inside each sequence, and the same wiz-bang optimizations can be applied in each case. Hence my earlier suggestion to limit the choice to 32 possible sequences. Such things have been tried in other POW algorithms, but I don\u2019t have any data about how effective it was, so that\u2019s also very speculative. Of course it would be impractical to do this with a large set of choices. I\u2019m just trying to give a sense of the actual performance degradation you speak of. Anyway, I think this has been sufficiently flogged and I will leave it at that. It is likely too late to journey into even more unknown territory anyway! ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.521221532091097
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "47",
                    "likes": "2",
                    "time": "28/03/2019-20:54:53",
                    "content": "Agreed, reluctantly.  Doing self-modifying code by printing out text and running it back through a compiler at runtime is usually an anti-pattern, but one I should have complained about a long time ago. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.25
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/lightuponlight",
                    "index": "48",
                    "likes": "0",
                    "time": "29/03/2019-14:13:44",
                    "content": "Yes agree, especially when you have little control over the compiler. ProgPoW seems to add a huge amount of complexity to the mining algorithm with many new added dependencies. This looks like a recipe for lowered reliability and perhaps even potential network attacks. Whether it will end up showing unreliability in actual operation is certainly unclear, but the large increase in complexity, layered components and dynamic behavior makes it a lot more likely than with the existing hash algo. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.7405979437229435
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "49",
                    "likes": "1",
                    "time": "29/03/2019-15:04:51",
                    "content": "I think it\u2019s worth considering whether we can reduce the complexity without reducing the security, but somebody (and @ifdefelse might not want to volunteer) has to do the work, and I\u2019m not seeing this as a showstopper. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "50",
                    "likes": "1",
                    "time": "29/03/2019-15:08:39",
                    "content": "    lightuponlight:  ProgPoW seems to add a huge amount of complexity to the mining algorithm with many new added dependencies. This looks like a recipe for lowered reliability and perhaps even potential network attacks.   How hard would it be to back off to EthHash temporarily if a problem is found? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.620580808080808
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jcyr",
                    "index": "51",
                    "likes": "0",
                    "time": "29/03/2019-20:32:30",
                    "content": "There already exists a miner that can do both Ethash and Progpow. However, there are no provisions in the protocol for signaling the choice of algorithm. Can\u2019t speak for the nodes. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "52",
                    "likes": "0",
                    "time": "30/03/2019-00:04:09",
                    "content": "I don\u2019t know that it would need to be in the protocol. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ifdefelse",
                    "index": "53",
                    "likes": "2",
                    "time": "30/03/2019-09:43:12",
                    "content": "Appreciate the technical discussion here. We have thought through many other options. In the end, the increased complexity in data manipulation and code generation was the best way to ensure hardware architecture affinity. Furthermore, I\u2019d like to echo an earlier comment that is very important when considering bugs that might break things. We\u2019re always dependent on some party\u2019s driver software or firmware - whether it is a cryptocurrency-ASIC or a GPU-ASIC. With bad software/firmware updates, hardware can be taken offline. Consider this: is it better to trust a party with a vested interest in getting the driver software/firmware right at enormous mission-critical scales and which is audited/tested every instant by a independent global computing ecosystem?  Or, is it better to trust the alternative parties whose interests are profiting from mining hardware, who are historically well-known for backdoors and a lack of transparency, and naturally have a much smaller ecosystem of users? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.759033613445378
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "54",
                    "likes": "2",
                    "time": "30/03/2019-19:46:50",
                    "content": "Thus my reluctant agreement, @ifdefelse.  The code generation introduces some weakness and complexity, but am not sure that adequate security can be had without some sort of code generation, and starting into redesign now seems a bigger risk.  And I\u2019m hearing that any problems that arise can be easily mitigated. But some discussion of whether we can simplify things safely in a future upgrade may be worthwhile after the current storm has past. And, thankfully, I don\u2019t think the GPU over ASIC arguments are relevant here   We are looking for weaknesses with an eye to correcting or mitigating them.  If this is indeed the weakest link ProgPoW is looking pretty good. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.881944444444445
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ifdefelse",
                    "index": "55",
                    "likes": "1",
                    "time": "01/04/2019-03:40:23",
                    "content": "Appreciate the technical discussion here. We have thought through many other options. In the end, the increased complexity in data manipulation and code generation was the best way to ensure hardware architecture affinity. Furthermore, I\u2019d like to echo an earlier comment that is very important when considering bugs that might break things. We\u2019re always dependent on some party\u2019s driver software or firmware. With bad software/firmware updates, hardware can be taken offline. Consider this: Is it better to trust a party with a vested interest in getting the driver software/firmware right at enormous mission-critical scales and which is audited/tested every instant by an independent global computing ecosystem?  Or, is it better to trust the alternative parties whose interests are profiting from mining hardware, and which have a much smaller ecosystem of users? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.826904761904762
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Anlan",
                    "index": "56",
                    "likes": "1",
                    "time": "01/04/2019-07:46:40",
                    "content": "    gcolvin:  I don\u2019t know that it would need to be in the protocol.   Protocol can be easily tweaked to signal mining algo. See https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1571.md ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 7.166666666666667
                }
            ]
        }
    ],
    "group_index": "766"
}