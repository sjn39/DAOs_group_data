{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/eip-2926-chunk-based-code-merkleization/4555",
            "title": "EIP-2926: Chunk-Based Code Merkleization ",
            "index": 4555,
            "category": [
                "EIPs"
            ],
            "tags": [
                "evm",
                "eth1x",
                "core-eips"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "1",
                    "likes": "1",
                    "time": "31/08/2020-17:35:28",
                    "content": "Discussion topic for  Ethereum Improvement Proposals   EIP-2926: Chunk-Based Code Merkleization Details on Ethereum Improvement Proposal 2926 (EIP-2926): Chunk-Based Code Merkleization       Code merkleization, along with binarification of the trie and gas cost bump of state accessing opcodes, are considered as the main levers for decreasing block witness sizes in stateless or partial-stateless Eth1x roadmaps. Here we specify a fixed-sized chunk approach to code merkleization, outline how the transition of existing contracts to this model would look like, and pose some questions to be considered.  ",
                    "links": [
                        "https://eips.ethereum.org/EIPS/eip-2926"
                    ],
                    "GPT-summary": "The post discusses Ethereum Improvement Proposal 2926, which proposes a fixed-sized chunk approach to code merkleization to decrease block witness sizes in stateless or partial-stateless Eth1x roadmaps. The author explains the proposal, outlines the transition of existing contracts to this model, and poses questions to be considered. A 3rd party gives constructive criticism and asks questions about the proposal.",
                    "GPT-proposal-categories": [
                        "Smart contract updates",
                        "Interoperability and Scalability",
                        "None",
                        "None",
                        "None"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.833333333333334
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "2",
                    "likes": "0",
                    "time": "02/09/2020-18:51:43",
                    "content": "I think it would be worth explicitely forbidding 0xffffffffas a key for a chunk.  This fits into the practically we won\u2019t ever have this problem, but in theory, long enough code would overwrite the metadata entry. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.3125
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "3",
                    "likes": "0",
                    "time": "02/09/2020-18:52:41",
                    "content": "Cool to see the EIP, just gave it a read-through.  At the risk of bike shedding    I\u2019d like to float the idea of removing the RLP from the spec.  It looks like the only places that RLP is used are:  RLP([METADATA_VERSION, codeHash, codeLength]) RLP([firstInstructionOffset, C.code])  LEB128 looks suitable for METADATA_VERSION, codeLength, and firstInstructionOffset.  codeHash can just be fixed length bytes.  C.code can also just be the raw bytes, allowing us to just serialize these using concatenation:  LEB128(METADATA_VERSION) || codeHash || LEB128(codeLength) LEB128(firstInstructionOffset) || C.code  If there is negative sentiment towards LEB128, codeLength could be a fixed size (?4 bytes?) big endian, and METADATA_VERSION & firstInstructionOffset could just be encoded as single bytes. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.2765567765567765
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "5",
                    "likes": "0",
                    "time": "08/09/2020-18:23:02",
                    "content": "I think SSZ is a good candidate here for simplifying the spec. REF: SSZ specification There are a number of ways the data structure could be modeled so I\u2019ll just start off with a suggestion: codeRoot = ssz.hash_tree_root(merklizedCode) merklizedCode = Container[metaData, code] metaData = Container[version, codeHash, codeLength] codeHash = keccak(raw_bytecode)  # is this correct? version = uint8 codeHash = bytes[32]  # uint8[32] codeLength = uint32 code = List[Container[uint8, bytes[32]]]  This eliminates any need for the spec to specify how each of these individual things are serialized, as well as leaning on the existing SSZ merklization rules. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 6.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/sinamahmoodi",
                    "index": "6",
                    "likes": "1",
                    "time": "09/10/2020-09:30:27",
                    "content": "I was experimenting with @pipermerriam 's suggested schema and was mainly curious how the backing tree would look like. code-merkleization-ssz-white-bg980\u00d7971 91.4 KB This was my first interaction with SSZ, so sharing my takeaways here:  Leaves are (padded to) 32 bytes The List type needs a limit parameter. This means if we assume a contract could have at most 1024 chunks (768 rounded to the next power of 2), regardless of the actual number of chunks the corresponding subtree will have 1024 leaves. This can affect proof size if empty subtrees are not compressed I used a List of bytes with a limit of 32 for the chunk code (to accommodate the last chunk which can have a length < 32), this basically doubles the number of leaves. We should probably use fixed 32 byte vectors and encode the last chunk\u2019s length somewhere (maybe metadata) Also by default SSZ uses sha256. But this should be easily replaceable with keccak256  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.541666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "7",
                    "likes": "0",
                    "time": "09/10/2020-14:10:07",
                    "content": "@sina would you be up for posting the SSZ schema you used to remove an ambiguity? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/sinamahmoodi",
                    "index": "8",
                    "likes": "0",
                    "time": "09/10/2020-14:25:41",
                    "content": "Yes sure: https://gist.github.com/s1na/2528af8c643f24309958515d8610a428 ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 7.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/hmijail",
                    "index": "9",
                    "likes": "0",
                    "time": "23/10/2020-15:18:47",
                    "content": "Given that code necessarily has to start from address 0 and fill up addresses from there, and that PC=0 is the necessary entry point, every proof will have to send the first (leftmost) chunk. Further chunks will be progressively rarer; the rightmost side of the trie will usually be rather empty, since few contracts fill the available code space. So I\u2019d imagine that putting metadata in the last chunk (addresses 0xfff\u2026) causes the merklization to send the rightmost branch with full cost of the proof for that chunk, since the hashes for that proof won\u2019t be usually amortized by the neighboring chunks. Therefore, wouldn\u2019t it be better to do something like putting the metadata in an extra chunk at the leftmost side of the trie, before the code block at address 0? This way, the metadata and first block of code (both always needed) can be sent together and amortize their proofs. Of course this means that every code block would have to be \u201cshifted left\u201d, but that is an exceedingly simple fix. Also, this prevents the collision risk that @pipermerriam mentioned. Regarding the use of Merkle Patricia Tries: why not use instead a simpler Merkle Tree? Given that chunk sizes are known and are filled from address 0, the (key, value) capability of the MPT is probably not useful, since the key can be easily calculated from the tree\u2019s (or proof\u2019s) structure. I imagine this would simplify things / save space? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.365079365079365
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/vbuterin",
                    "index": "10",
                    "likes": "0",
                    "time": "24/10/2020-00:52:47",
                    "content": " Regarding the use of Merkle Patricia Tries: why not use instead a simpler Merkle Tree?  SSZ already uses simple Merkle trees, not MPTs.  I was experimenting with @pipermerriam 's suggested schema and was mainly curious how the backing tree would look like.  Why have length for each chunk? Isn\u2019t length already covered by the code length (and again by the chunk count)? I saw the comment \u201cLast chunk could be shorter than 32 bytes\u201d in the github, but I don\u2019t think there\u2019s any actual need to be strict about this; code ending is identical to code being followed by zeroes (for all purposes except CODESIZE, which uses length anyway). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/sinamahmoodi",
                    "index": "11",
                    "likes": "0",
                    "time": "28/10/2020-10:12:37",
                    "content": "    hmijail:  Therefore, wouldn\u2019t it be better to do something like putting the metadata in an extra chunk at the leftmost side of the trie, before the code block at address 0?   That\u2019s indeed possible, I can update the spec.  why not use instead a simpler Merkle Tree  The options right now are either SSZ as Vitalik mentioned or the binary trie. I\u2019m slightly leaning towards the binary trie just for consistensy\u2019s sake (specially in the proofs) and to unnecessarily introducing a new tree structure. But I\u2019m open for either options.  Why have length for each chunk? Isn\u2019t length already covered by the code length (and again by the chunk count)?  Yes I missed that. Having codeLength it\u2019s easy to infer the length of the last chunk. Thanks! ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.581619769119769
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/hmijail",
                    "index": "12",
                    "likes": "0",
                    "time": "04/11/2020-03:24:51",
                    "content": "I have just posted some results of applying fixed-size chunking to >500K Mainnet blocks. The optimal size seems to be 1 byte: 3% size overhead vs 30% for 32-byte chunks. This happens because:  The median length for basic blocks is 16 bytes, with a statistical distribution very skewed towards smaller sizes. Smaller contiguous chunks need fewer proof hashes than bigger isolated chunks.  The tools I used have been published and the results should be repeatable in a matter of hours by anyone interested. So I hope I am not missing anything! ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.410714285714286
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/sinamahmoodi",
                    "index": "13",
                    "likes": "0",
                    "time": "25/11/2020-17:17:05",
                    "content": "I implemented the merkleization logic in a fork of geth and measured the overhead. The goal is to determine how much contract creation gas costs need to be bumped. The following chart shows the merkleization overhead in terms of gas in relation to the contract size, for all contracts on the Goerli testnet. Gas was computed by measuring the runtime and comparing it against the runtime of the ECRECOVER precompile on the same machine. overhead-goerli-mpt1920\u00d71007 55 KB Here we used the hexary MPT for the code tree as specified currently in the EIP. As expected the runtime rises linearly with code size. Currently contract creation costs 200 gas per byte of code. By increasing that number to 203 we can cover this overhead. Or because the overhead is not that significant not bump the gas cost at all. But we don\u2019t plan to use the hexary MPT for code. There are 3 options: SSZ, a simple merkle tree, the binary trie. Here I also experimented with the fastssz implementation and measured the overhead in a similar way as above. overhead-goerli-comparison1920\u00d71007 33.1 KB Fastssz uses an accelerated sha256 implementation by default for hashing the tree. The column ssz_sha shows fastssz\u2019s performance overhead when using the default hasher. ssz_keccak replaces the default hash function with geth\u2019s keccak256 implementation. Interestingly both perform better than the MPT, even though binary trees incur more hashing. I think this might be because SSZ\u2019s tree structure is more-or-less known at compile-time as opposed to the MPT. The same measurement is pending for the binary trie and I\u2019ll update the post once I have numbers on that. ",
                    "links": [
                        "https://ethereum-magicians.org/uploads/default/original/2X/a/ac681e8757d6f6b3afcfa182d636673ae4829cba.png",
                        "https://eips.ethereum.org/EIPS/eip-3102",
                        "https://github.com/ferranbt/fastssz",
                        "https://ethereum-magicians.org/uploads/default/original/2X/4/47af0547fa4bbb9ba6ca46251005390f8a7ef384.png",
                        "https://github.com/minio/sha256-simd"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.705357142857142
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "14",
                    "likes": "0",
                    "time": "31/05/2021-22:57:11",
                    "content": "There were some updates regarding this EIP in the EF-Supported Teams 2020 Pt 1. and 2021 Pt 2. posts. Notable is the complete merkleization implementation in geth ussing SSZ, which is explained in depth in here. ",
                    "links": [
                        "https://blog.ethereum.org/2021/04/26/ef-supported-teams-research-and-development-update-2021-pt-1/#code-merkleization",
                        "https://github.com/s1na/go-ethereum/tree/code-merkleization-ssz-stats",
                        "https://hugo-dc.github.io/cm-docs/"
                    ],
                    "GPT-discussion-categories": [],
                    "Sentiment": 6.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/axic",
                    "index": "15",
                    "likes": "0",
                    "time": "01/06/2021-00:28:05",
                    "content": "There is a related proposal by @vbuterin to first introduce charging chunk costs. I took the liberty to share it here, given there\u2019s an implementation in geth which links to it, and also an implementation in evmone. @hugo-dc has published an impact analysis of the proposed costs, detailing the effect of the 350 gas cost per chunk on mainnet. ",
                    "links": [
                        "https://github.com/ethereum/go-ethereum/pull/22886",
                        "https://github.com/ethereum/evmone/pull/326",
                        "https://notes.ethereum.org/@ipsilon/code-chunk-cost-analysis"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.625
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/hugo-dc",
                    "index": "16",
                    "likes": "0",
                    "time": "11/08/2021-00:12:29",
                    "content": "We\u2019ve just published a document describing the code merkleization implementation in geth and an analysis made on ~1M mainnet blocks: Code Merkleization Practical Implementation & Analysis - HackMD ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/luzius",
                    "index": "17",
                    "likes": "0",
                    "time": "23/09/2021-15:49:59",
                    "content": "Would this be an opportunity to start allowing smart contracts to refer to existing bytecode? That way, they could simply re-use already existing code and one could potentially a lot of storage as identical smart contracts could be deduplicated. See also:  github.com/ethereum/EIPs     \t      EIP 911 - Zero redundancy/overhead bytecode deployments (Discussion/Draft)            opened 11:16PM - 03 Mar 18 UTC               ivica7                 ## Preamble ``` EIP: 911 Title: Zero-redundancy and overhead-free bytecode de\u2026ployments Author: Ivica Aracic <aracic@gmail.com> Category: EVM Created: 2018-03-04 Updated: 2018-03-04 ``` ## Short Description  The user should not have to pay gas for deploying the code, which already exists in the DB, because the node has no extra work in this case except of setting the correct codeHash in the account, which is covered by the CREATE fee.  ## Motivation  At the time of the mainnet block number 5200035, there are 5,212,554 active contract instances referring to 64,561 unique bytecodes. 10,552 (16.34%) of these unique bytecodes have been deployed more than once, resulting in a redundancy of  2,934,072,376 bytes. This is 93.05% of the total byte size of all deployed contracts.   If we only consider the price of 200 gas / byte, which is calculated for storing the bytecode to the database, these redundancies have in sum a cost of 586,814,475,200 gas. This is ~7.45% of all gas ever spent in Ethereum. Moreover, this number looks even worse, if we consider the costs for tx input data size and historical instantiations of contracts, which do not exist anymore at the analysed block.  *Stats are generated with https://github.com/ivica7/eth-chain-stats-collector* ```               EOAs: 24332129 Contract Instances: 5212554    Unique Bytecodes    with duplicates: 10552 / 64561 -> 16.344232586236274 %       Bytes Wasted: 2934072376 / 3153359448 -> 93.04592211525136 %         Gas Wasted: 586,814,475,200 ```  Currently two patterns exist for partially solving this problem: (1) delegatecalls, and (2) extcodecopy/size.   delegatecalls allow us to deploy the code once, but again they introduce a code redundancy on its own  for the forwarder contracts. Further, they have an additional run-time overhead of ~1000 gas / call.  Extcodecopy/size will reduce transaction input size, but the deployment cost of 200 gas / byte still applies even if it is copied from an existing instance.  Both patterns are helpful, however, they fail in providing an overhead-free solution for the code redundancy problem. When we consider the fact that Ethereum is targeting x100+ in transaction output (e.g. through PoS and Sharding), we have to be careful with this problem, because it will grow proportionally.  For this reason, I am opening this EIP proposal to discuss how to tackle this problem. Below, I provide one possible solution by defining CREATEFROMADDR opcode.   ## Possible Solution 1 **Specification**  Add new opcode CREATEFROMADDR(v, p, s) * v - value in wei sent to the new contract address * p - init bytecode memory address * s - size of the init bytecode  CEATEFROMADDR behaves like CREATE, but it interprets the return data from the init code as a contract address and sets the codeHash of this address as the codeHash of the new contract. Since there is no insert of the code to node's database required, the additional gas costs for the code Insert can be omitted. The sender pays only a flat fee of 1400 gas instead of len(code)*200.  *TODO*: for Constantinople also specify CRATEFROMADDR2(v, n, p, s) analogous to CREATE2.  **Implementation (related to geth)**  * add a flag to evm:Create to signal which modus is used for the return data. * Old create-opcodes will set the flag to false * New create opcodes will set it to true * If the flag is true, evm:Create will lookup the code hash of the address returned by init code and it will set it as codeHash of the new contract. size*200 gas fee will be replaced by a flat fee of 1400 gas.  ## Possible Solution 2  Change CREATE2 specification to support the cloning of the codeHashExtend as described above. This opcode is not productive yet and we would not have to introduce new opcodes.  ???       ",
                    "links": [
                        "https://github.com/ethereum/EIPs/issues/911",
                        "https://github.com/ivica7"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.155630222490688
                }
            ]
        }
    ],
    "group_index": "1126"
}