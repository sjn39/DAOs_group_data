{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/graphql-interface-to-ethereum-node-data/2710",
            "title": "GraphQL interface to Ethereum node data ",
            "index": 2710,
            "category": [
                "EIPs"
            ],
            "tags": [
                "NONE"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "1",
                    "likes": "7",
                    "time": "21/02/2019-19:25:39",
                    "content": "This topic is for discussion of EIP-1767, GraphQL interface to Ethereum node data. The goal of this EIP is to provide a new API for accessing Ethereum node data that\u2019s more efficient and flexible than the current JSON-RPC interface, with the eventual goal of replacing it. An implementation of the standard so far will be in the next release of geth. I\u2019m keen to collaborate with developers on other clients to get this adopted as widely as possible, so your feedback is greatly appreciated - as is reaching  out to your favourite client developers! ",
                    "links": [],
                    "GPT-summary": "The author of the post is introducing a new proposal, EIP-1767, which aims to provide a more efficient and flexible API for accessing Ethereum node data. The author is asking for feedback and collaboration from developers on other clients to get this proposal adopted as widely as possible.",
                    "GPT-proposal-categories": [
                        "Interoperability and Scalability",
                        "Smart contract updates",
                        "None",
                        "None",
                        "None"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party or author is advertising proposal",
                        "None"
                    ],
                    "Sentiment": 5.478535353535353
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Ethernian",
                    "index": "2",
                    "likes": "0",
                    "time": "22/02/2019-03:29:13",
                    "content": "Great proposal! One question: could you compare the EIP-1767 with the TheGraph\u2019s approach? ",
                    "links": [
                        "https://thegraph.com"
                    ],
                    "GPT-discussion-categories": null,
                    "Sentiment": 10.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "3",
                    "likes": "1",
                    "time": "22/02/2019-03:35:32",
                    "content": "    Ethernian:  One question: could you compare the EIP-1767 with the TheGraph\u2019s approach?   The Graph provides a way to create custom ETL schemas over Ethereum events, making it possible to efficiently index and query contract-specific data. In contrast, this EIP specifies a GraphQL schema for presenting the node data that\u2019s already stored and indexed by nodes and provided via the JSON-RPC interface, with the ultimate goal of replacing JSON-RPC. This is particularly useful for tools that consume blockchain data en-masse, like The Graph. ",
                    "links": [
                        "https://thegraph.com"
                    ],
                    "GPT-discussion-categories": [
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Ethernian",
                    "index": "4",
                    "likes": "0",
                    "time": "22/02/2019-04:32:10",
                    "content": "RFE: I would like to have an ability to get a public key for address (without intermediary explicit tx lookup and ecrecover call). ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "5",
                    "likes": "0",
                    "time": "22/02/2019-04:33:01",
                    "content": "There\u2019s not any way to do this efficiently, unfortunately. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 2.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Ethernian",
                    "index": "6",
                    "likes": "0",
                    "time": "22/02/2019-04:42:35",
                    "content": "    Arachnid:  There\u2019s not any way to do this efficiently, unfortunately.   hmm\u2026 is it really no way for a node to maintain internally an efficient index from ethereum address to [blockNr, txNr] of some of Tx signed by this address (and recover a public key from this Tx upon request)? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 4.4
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "7",
                    "likes": "2",
                    "time": "22/02/2019-05:57:56",
                    "content": "Sure, that\u2019s possible, but:  It  would require new index datastructures on disk. It wouldn\u2019t work for transactions sent before the node was fast-synced without changes to the fast-sync protocol. It wouldn\u2019t work for light nodes without changes to the LES protocol.  And for all of the reasons above, it\u2019s totally out of scope for this EIP.  ",
                    "links": [],
                    "GPT-discussion-categories": [],
                    "Sentiment": 5.863636363636364
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ryanschneider",
                    "index": "8",
                    "likes": "1",
                    "time": "22/02/2019-23:25:47",
                    "content": "I think call and account should also be options on a block just like how logs is done.  That way I could do: block(hash: \"0xblockHash\") {      account( \"0xaddress\") { balance } }  To do the equivalent of eth_getBalance(\"0xaddress\", \"0xblockHash\"), which actually isn\u2019t actually supported by JSONRPC (the 2nd param is always a block number, not a hash). This would access the snapshot of state from that block, so would be useful for people indexing data from archive nodes, and by allowing lookup by hash it solves the issue of possible reorgs happening under your feet between two queries. The top-level call and account should stay, they would just continue to reference the canonical head\u2019s state root. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.300000000000001
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "9",
                    "likes": "0",
                    "time": "22/02/2019-23:48:24",
                    "content": "    ryanschneider:  I think call and account should also be options on a block just like how logs is done.   Good call. Perhaps it would make more sense to remove them from the top level query in that case? One way of doing things is generally clearer than multiple, and I don\u2019t think block { account(\"0xaddress\") { balance } } for a query against the latest block is unclear. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 6.875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "10",
                    "likes": "0",
                    "time": "23/02/2019-22:15:15",
                    "content": "I don\u2019t know enough about GraphQL to know if this will be a problem (nor have I had the time to do my research) but\u2026 It would be ideal if we could loosen the spec from requiring the JSON-RPC endpoint be exposed over HTTP.  The Trinity client only exposes JSON-RPC over an IPC socket.  Our plans are to provide websockets and HTTP using https://github.com/ethereum/dopple .  That tool will proxy an IPC socket based JSON-RPC server over either HTTP or websockets, allowing the Trinity codebase to not worry about HTTP servers of any kind.  Ideally we\u2019d expose the graphQL API through the same mechanism with the option of proxying it over HTTP if desired. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 7.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "11",
                    "likes": "0",
                    "time": "23/02/2019-23:10:35",
                    "content": "    pipermerriam:  It would be ideal if we could loosen the spec from requiring the JSON-RPC endpoint be exposed over HTTP. The Trinity client only exposes JSON-RPC over an IPC socket. Our plans are to provide websockets and HTTP using https://github.com/ethereum/dopple  . That tool will proxy an IPC socket based JSON-RPC server over either HTTP or websockets, allowing the Trinity codebase to not worry about HTTP servers of any kind. Ideally we\u2019d expose the graphQL API through the same mechanism with the option of proxying it over HTTP if desired.   I\u2019m certainly happy to loosen the spec to allow for that - but we\u2019d need to specify an encapsulation and encoding schema. As far as I can tell, dopple just takes the POST body and sends it raw over the socket, then returns the response. This is fine, if you don\u2019t need any metadata or headers, but I\u2019m fairly sure GraphQL does; for instance, it supports query parameter substitution, which requires sending a query and a separate map/dict of parameters. What\u2019s the motivation behind using such a bare-bones transport? I can understand wanting to avoid some of the complexities of HTTP, but couldn\u2019t you expose a very basic HTTP service over IPC instead, which would preserve useful features like headers and content encoding, and use a reverse proxy to serve it to clients? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.387820512820513
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pipermerriam",
                    "index": "12",
                    "likes": "0",
                    "time": "24/02/2019-03:42:08",
                    "content": "Hrm, I need to go read up a bit on GraphQL.  I wasn\u2019t aware that it leveraged HTTP apis like query params. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "13",
                    "likes": "0",
                    "time": "24/02/2019-03:50:03",
                    "content": "Looks like I was mistaken; an HTTP binding is provided which encapsulates both the document and any query parameters in one blob: https://graphql.org/learn/serving-over-http/ ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/kshinn",
                    "index": "14",
                    "likes": "1",
                    "time": "25/02/2019-05:07:46",
                    "content": "I\u2019ve been through the respective differences between the EthQL and this standard a few times and it looks like a lot of work has been done to align them. On the EthQL side, there are some things that can be done to align some of the more minor differences. There are also some comments I wanted to rehash from previous conversations. There were a few comments made in previous conversations that seemed like the biggest differences that I wanted to align on.  Big integers should probably have their own scalar type, not just String Likewise, hex-encoded byte strings other than addresses and hashes should have their own types.  Agreed. The additional specificity is needed for standards. There are a number of \u201cconveniences\u201d that EthQL layers on top of the raw output to give to dApp developers. Among them, you mentioned additional transaction filters, expressing values in selectable base units, and decoding things like Solidity storage layout. For something baked into a node, the additional complexity / overhead for that is not needed and as we work with EthQL it does feel like there is a Standard API and an Extended API that provides the type of application side processing one would do. Suggestions: For arguments that reference a block, the EIP limits to blockNumbers. What about adding back the string based references of \u201clatest\u201d, \u201cearliest\u201d, and \u201cpending\u201d? Also, are there small conveniences we could bake into the standard? For transaction status, the 1, 0, null values feels like an enum. EthQL has them defined as SUCCESSFUL, FAILED, PENDING. There are some methods that went unimplemented in this EIP eth_newFilter and the like. Are you thinking that Subscriptions would be included in a future EIP? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.045847268673356
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "15",
                    "likes": "0",
                    "time": "25/02/2019-06:24:33",
                    "content": "    kshinn:  For arguments that reference a block, the EIP limits to blockNumbers. What about adding back the string based references of \u201clatest\u201d, \u201cearliest\u201d, and \u201cpending\u201d?   \"earliest\" is redundant, since you can specify it as 0 instead, and the EIP specifies that omitting any block number or hash operates on the latest block. For \"pending\", if we make the change I suggested earlier of putting all operations such as call and balance in a Block object, perhaps the top level query can have a pendingBlock field?     kshinn:  Also, are there small conveniences we could bake into the standard? For transaction status, the 1 , 0 , null values feels like an enum. EthQL has them defined as SUCCESSFUL , FAILED , PENDING .   Good point - let\u2019s make this an enum.     kshinn:  There are some methods that went unimplemented in this EIP eth_newFilter and the like. Are you thinking that Subscriptions would be included in a future EIP?   Designing a filtering API that makes good use of GraphQL\u2019s subscription functionality and doesn\u2019t just replicate the JSON-RPC filter mechanism seemed like a project all of its own; I didn\u2019t want to cram in something that just kind of worked into this spec. I think it makes the most sense as a separate EIP. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.21875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ryanschneider",
                    "index": "16",
                    "likes": "2",
                    "time": "25/02/2019-16:56:00",
                    "content": " Good call. Perhaps it would make more sense to remove them from the top level query in that case? One way of doing things is generally clearer than multiple, and I don\u2019t think  block { account(\"0xaddress\") { balance } }  for a query against the latest block is unclear.  I agree, the only reason I proposed keeping account and call at the high-level was compatibility with earlier versions of the spec, but IMO the more we can do to educate web3 developers on the interaction between blocks and state the better, and tying these actions to a specific block does that. As an added plus, this forces multiple calls to be explicit on what block they operate on, previously account(\"0xa...\"){} account(\"0xb....\"){} could have a small chance of being resolved on separate blocks if a new block came in mid-resolver, and so the spec would need to define that behavior.  Now, that behavior is up to the query writer (if they write block(){ account(a){} account(b){} } vs. block{ account(a){} } block{ account(b){} }.  Designing a filtering API that makes good use of GraphQL\u2019s subscription functionality and doesn\u2019t just replicate the JSON-RPC filter mechanism seemed like a project all of its own  I agree 100%.  That said, we should perhaps have some informal discussions on where we see the subscription support in EthQL going before this EIP is finalized?  Just so to minimize the chance of baking something in at the query level that will conflict with later subscription ideas? Over the winter holiday I spent a little time toying with an addition to the EthQL schema to add support for checking the confirmation of a sent transaction via a subscription, I got a very messy proof-of-concept version working locally using parity_subscribe.  The code isn\u2019t really in a state to be shared yet, but here\u2019s a gist where I outlined how I imagine it working:   gist.github.com   https://gist.github.com/ryanschneider/e7819ff02256138b780969ddf114a0e7 flow.graphql # offline, sign a transaction to generate it's raw bytes 0xRAWTX # connect to our new transaction websocket endpoint # send the graphql mutation: mutatation {     sendRawTransaction(data: \"0xRAWTX\") {         hash     } } # get back the hash or an error { This file has been truncated. show original        Anyways, just mentioning this to show my interest in subscriptions in later iterations of EthQL, for now I agree 100% we should be focused on the query aspects. ",
                    "links": [
                        "https://gist.github.com/ryanschneider/e7819ff02256138b780969ddf114a0e7",
                        "https://gist.github.com/ryanschneider/e7819ff02256138b780969ddf114a0e7"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.75049845987346
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ryanschneider",
                    "index": "17",
                    "likes": "0",
                    "time": "26/02/2019-01:40:25",
                    "content": "    Arachnid:  For \"pending\" , if we make the change I suggested earlier of putting all operations such as call and balance in a Block object, perhaps the top level query can have a pendingBlock field?   I like this idea.  It also reinforces that the pending block is special, which it is (there\u2019s no guarantee that your nodes pending block will match in any way what actually gets mined by the miners). Also if we get to the point where the txpool RPCs have any exposure over graphQL I could see pendingBlock living there (e.g. txpool { pendingBlock } ) to further hammer home the point that it\u2019s a local best-effort at guessing what might be mined next, but I think that could wait.  I\u2019d also personally be fine with not support pending blocks at all in the baseline graphQL spec since they are such a special beast. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.741341991341992
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/kshinn",
                    "index": "18",
                    "likes": "0",
                    "time": "28/02/2019-06:34:29",
                    "content": "    Arachnid:  For \"pending\" , if we make the change I suggested earlier of putting all operations such as call and balance in a Block object, perhaps the top level query can have a pendingBlock field?   I was originally suggestion that we have some reference to \u201cpending\u201d because of the JSON-RPC spec and how it treats the \u201cearliest\u201d, \u201clatest\u201d, and \u201cpending\u201d ideas. Based on some of the discussion in @ryanschneider\u2019s responses I wonder if it could be confusing to the end user? That said, I don\u2019t have any problem with the proposed field on the Block object.     ryanschneider:  That said, we should perhaps have some informal discussions on where we see the subscription support in EthQL going before this EIP is finalized? Just so to minimize the chance of baking something in at the query level that will conflict with later subscription ideas?   One of the things I like about the EIP is that (as far as I understand) it keeps things simple and close to the storage implementation in the node.  Most of the use cases for subscription I\u2019ve had in my mind (and that I\u2019ve heard in conversations) are around event subscriptions which look like a natural extension to the filtering functionality here. Are there particular cases you are worried about in subscriptions that could conflict? I think that the subscription conversation could go pretty deep and pull this discussion off topic. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.6506410256410255
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ryanschneider",
                    "index": "19",
                    "likes": "0",
                    "time": "28/02/2019-17:31:05",
                    "content": "    kshinn:  Are there particular cases you are worried about in subscriptions that could conflict? I think that the subscription conversation could go pretty deep and pull this discussion off topic.   Very valid point.  I think the obvious ones are subscriptions on new blocks and logs (to match the functionality of the eth_subscribe RPC).  While eth_subscribe also supports subscribing to pending transactions, IMO that\u2019s a bit too much of a firehose on public networks, I feel like the graphQL equivalent should have some sort of required filter (address(es), etc.) to limit the scope of data being sent. The less obvious ones are:  transaction confirmation: currently there\u2019s no way to see that your transaction was mined and has stayed mined for N blocks short of polling the JSONRPC layer.  parity_subscribe (https://wiki.parity.io/JSONRPC-parity_pubsub-module.html#parity_subscribe) I haven\u2019t looked at all into how this is implemented but I assume they are basically re-running the RPC every time a new block is imported into the chain (and additionally possibly on every new change to the txpool).  I think this can be emulated pretty well if there\u2019s a block subscription and call and address are available from the block like we\u2019ve discussed.  Anyways, I agree this is all out of scope for the EIP, I just wanted to think through it some to make sure we\u2019re not baking in anything that will make subscriptions harder down the road, but so far I haven\u2019t seen anything that would do that. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.488936988936989
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/kshinn",
                    "index": "20",
                    "likes": "0",
                    "time": "01/03/2019-04:46:14",
                    "content": "What do we think about adding an ID primitive field? https://facebook.github.io/graphql/draft/#sec-ID The hash reference (or some encoded version of it) could be used for this purpose. It presents a bit more complexity and arguably redundant representation of data, but from what I understand the graphql clients use this primitive type to decide how to cache and resolve data. I can see a lot value in this for highly connected graph structures where parts of the query tree can be cached/reused without having to refetch it upstream. However, outside of account relationships I don\u2019t know if the data is highly connected enough to really take advantage of the concept. I think there are 2 possible ways of going about this:  Change the type of *.hash and account.address from Bytes32 to ID. This drawback is it obscures the underlying datatype and starts moving it away from the Ethereum spec. Keep the fields the way they are and add an additional id field with the Type ID. This gives some flexibility to define IDs that don\u2019t have hashes (for example logs IDs could be defined as keccak256(block.hash + log.index). Drawback is that it adds a new field to the data type that is not really part of the underlying node storage.  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.428181818181819
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "21",
                    "likes": "0",
                    "time": "01/03/2019-05:19:22",
                    "content": "Good thought. I hadn\u2019t recognised the significance of using ID to other generic graphql tools. I think your option 2 is better - make IDs opaque to callers, with no guarantees except they\u2019re unique. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 6.45
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "22",
                    "likes": "0",
                    "time": "07/03/2019-22:29:08",
                    "content": "I\u2019ve updated the EIP to add a Pending type, and to move account, call and estimateGas to Pending and Block. This also makes it possible now to query transactions in the pending pool, which the schema didn\u2019t previously offer. Pending has a subset of fields from Block that make sense based on the available data. I\u2019ve also written a PR updating the geth implementation to reflect these changes. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "Author of proposal is asking for feedback",
                        "3rd party or author wants to collaborate on proposal"
                    ],
                    "Sentiment": 5.388888888888888
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/zyfrank",
                    "index": "23",
                    "likes": "0",
                    "time": "03/05/2019-00:42:13",
                    "content": "It is a little strange for query \u2018\u2019\u2018block\\miner\u2019\u2019\u2019  to have a  \u2018\u2019\u2019(block :Long)\u2019\u2019\u2019 augment. Not sure if people have requirement of  query one block for another block\u2019s miner. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 4.1875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "24",
                    "likes": "0",
                    "time": "03/05/2019-19:15:59",
                    "content": "Question about output formats.  From the current spec  # BigInt is a large integer. Input is accepted as either a JSON number or as a string. # Strings may be either decimal or 0x-prefixed hexadecimal. Output values are all # 0x-prefixed hexadecimal. scalar BigInt # Long is a 64 bit unsigned integer. scalar Long  When outputting the json for a transaction some of the fields are Long, such as gasUsed.  Should those be outputted as standard json numbers or as hex strings as though it was a BigInt.  The lack of formatting instructions leads me to believe the latter. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.040178571428572
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "25",
                    "likes": "0",
                    "time": "03/05/2019-23:55:34",
                    "content": "    zyfrank:  It is a little strange for query \u2018\u2019\u2018block\\miner\u2019\u2019\u2019 to have a \u2018\u2019\u2019(block :Long)\u2019\u2019\u2019 augment.   Any argument that specifies an account has a block number - so you can specify what block you want to fetch that account at.     shemnon:  When outputting the json for a transaction some of the fields are Long, such as gasUsed . Should those be outputted as standard json numbers or as hex strings as though it was a BigInt. The lack of formatting instructions leads me to believe the latter.   Good point, we should specify this. The intention is that longs are read and formatted as numbers; GraphQL only specifies a 31 bit integer type, and it\u2019s useful to be able to use a longer (52 bit, safely in Javascript) numeric type. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 5.778409090909092
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "26",
                    "likes": "1",
                    "time": "04/05/2019-01:28:41",
                    "content": "Also, it may be worth referencing the GraphQL website when saying \u201cimplement a graphql endpoint\u201d because there are about 3 ways to do it.  GET, POST applicaiton/json, and POST applicaiton/graphql.  No need to spell it out in the spec, just reference their page: https://graphql.org/learn/serving-over-http/ ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "27",
                    "likes": "0",
                    "time": "24/05/2019-07:17:24",
                    "content": "Recommend changing the recommended default port to 80, since that is the standard port for HTTP traffic.  Most clients who implement this will likely provide a mechanism to override the port selection, but I do not think this standard should recommend against an incredibly well established standard as the default. Alternatively, remove the port and endpoint path form this standard entirely, as it isn\u2019t something that needs to be standardized on.  Discovery of a GraphQL endpoint is necessary, and during that discovery process acquiring the port, path, ip/domain can all be done at the same time.  Since you already have to discover the ip/domain, discovery of the port and path along with that is totally reasonable (and likely going to happen anyway). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.666666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "28",
                    "likes": "0",
                    "time": "24/05/2019-07:25:41",
                    "content": "Recommend changing from an HTTP recommendation to an HTTPS recommendation.  Alternatively, perhaps recommend HTTPS if bound to anything other than 127.0.0.1 and HTTP when bound to 127.0.0.1 (I recognize that SSL certificates that browsers accept are complicated when self-signed). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 3.4375
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "29",
                    "likes": "0",
                    "time": "24/05/2019-07:41:28",
                    "content": "Bundling my comments since apparently Discourse doesn\u2019t like multiple comments in a row.  Why 0x-prefixed hex strings for Bytes32, Address, Bytes, and BigInt?  Base32, Base58 and Base64 all compress data better and are well standardized so relatively easy to extract data from in any language.  The 0x prefix not only is wasted bytes on the wire, but it also makes extracting the data more complicated in most cases as you first have to strip the 0x characters off of the string (exception for JavaScript which is notorious for \u201cguessing\u201d what you mean when processing data, and it will treat a string that starts with a 0x as a hex string, even if it isn\u2019t).  GraphQL doesn\u2019t specify a wire serialization mechanism, though JSON is certainly the most common.  This specification should state what the wire serialization will be.  https://graphql.github.io/graphql-spec/draft/#sec-Serialization-Format  If JSON serialization is used, then the Long type may be problematic, pragmatically, as all JavaScript deserializers I\u2019m aware of automatically deserialize any JSON number into a JavaScript number, which cannot hold a 64-bit unsigned integer.  Ideally, these would be deserialized into a bigint, but at the moment bigint isn\u2019t supported (per spec) in JSON (de)serializing.  Consider either putting Longs into encoded strings like BigInt for usability reasons.  Alternatively, consider specifying Long as 52 bits, since I believe every language that is used in the Ethereum ecosystem currently supports integers up to 52-bits wide.  All arrays should be not-null (!).  Currently, a number of array properties are set as nullable (IIUC).  In almost all cases, if there is no data an empty array should be returned.  The only time a nullable array should be used is if you need to differentiate between the empty set and a sentinel set.  Specify what null means for any nullable property.  Why is Block.parent nullable?  Is this for the genesis block?  The comment should specify what null means.  This is an exapmle of the problem, but in general anywhere a property has a sentinel value (such as null), the comment should indicate what null means.  Recommend using consistent indentation in the GraphQL schema.  At the moment, lines are not consistently indented.  The new interface doesn\u2019t appear to support operating against the pending block.  I\u2019m not against this (and in fact, I\u2019m generally for it), but the Backward Compatibility section should mention that.  The behavior when both parameters for Query.block(number, hash) are provided should be specified in the comments.  The same goes for any function that has mutually exclusive parameters.  Recommend versioning the protocol.  This can be done via the recommended default path if that is retained, e.g., /graphql/v1.  Recommend a discovery endpoint or query a user can make that will give details about the current protocol version.  This could be used when doing service discovery to find out if graphql is available from the server, what path/port it lives at, and what version(s) of the protocol it supports.  While it is possible for the app to just probe a number of different endpoints, it is simpler if there is a single GET request that can be made to a well known path on a server to see if it supports GraphQL and if so, what versions and where.  Add ability to query for Chain ID.  This is necessary for replay protection as well as making it easier to properly alert the user when they are communicating with an endpoint that is not delivering data from the correct chain.  Also, since signing is extracted out (I\u2019m a fan), we need a way to ensure that the signer is speaking to the same chain as the GraphQL provider.  Consider adding support for fetching the output data of an on-chain transaction.  Since light clients may not have access to this, especially for ancient transactions, it could be a nullable field.  Alternatively, can someone champion an EIP to get return data into the receipts please?    Even if it was constrained to 32-bytes, this would be a huge boon on dapp development.  It should be noted that at the moment dapps that need return data write an event log with their transaction that contains the result, this results in more wasted resources than if return data was simply included in the receipt directly.  It appears that the Log filter has been ported forward basically as is.  I feel like we can probably do better with the log filter query language than the current 2 dimensional array.  I am not familiar enough with GraphQL\u2019s query to be able to assert that we can do better, but it feels like we could do better (like supporting logical AND and NOT) ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.588174474335188
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "30",
                    "likes": "0",
                    "time": "26/05/2019-05:56:54",
                    "content": "    MicahZoltu:  Why 0x-prefixed hex strings for Bytes32, Address, Bytes, and BigInt? Base32, Base58 and Base64 all compress data better and are well standardized so relatively easy to extract data from in any language. The 0x prefix not only is wasted bytes on the wire, but it also makes extracting the data more complicated in most cases as you first have to strip the 0x characters off of the string (exception for JavaScript which is notorious for \u201cguessing\u201d what you mean when processing data, and it will treat a string that starts with a 0x as a hex string, even if it isn\u2019t).   This conforms to the JSON-RPC conventions.  If we are worried about wire compression and HTTP GZip isn\u2019t sufficient then we sholdn\u2019t be using JSON result but RLP.     MicahZoltu:  If JSON serialization is used, then the Long type may be problematic, pragmatically, as all JavaScript deserializers I\u2019m aware of automatically deserialize any JSON number into a JavaScript number , which cannot hold a 64-bit unsigned integer. Ideally, these would be deserialized into a bigint , but at the moment bigint isn\u2019t supported (per spec) in JSON (de)serializing. Consider either putting Longs into encoded strings like BigInt for usability reasons. Alternatively, consider specifying Long as 52 bits, since I believe every language that is used in the Ethereum ecosystem currently supports integers up to 52-bits wide.   The fields using Long won\u2019t reasonably exceed 52 bits(gas, indexes, status, counts, protocol versions, block numbers), bringing that restriction in will in practice never matter and leak the JSON encoding into the spec.     MicahZoltu:  The new interface doesn\u2019t appear to support operating against the pending block. I\u2019m not against this (and in fact, I\u2019m generally for it), but the Backward Compatibility section should mention that.   There is a root level Pending object that you can check account details, estimate gas, etc.  That looks to be how such queries are done.  What constitutes the \u201cpending\u201d query would be nice to have specified.     MicahZoltu:  Recommend a discovery endpoint or query a user can make that will give details about the current protocol version. This could be used when doing service discovery to find out if graphql is available from the server, what path/port it lives at, and what version(s) of the protocol it supports. While it is possible for the app to just probe a number of different endpoints, it is simpler if there is a single GET request that can be made to a well known path on a server to see if it supports GraphQL and if so, what versions and where.   This would be useful not just for GraphQL but all standard endpoints: JSON-RPC, WebSockets, Disovery, etc.     MicahZoltu:  Add ability to query for Chain ID. This is necessary for replay protection as well as making it easier to properly alert the user when they are communicating with an endpoint that is not delivering data from the correct chain. Also, since signing is extracted out (I\u2019m a fan), we need a way to ensure that the signer is speaking to the same chain as the GraphQL provider.   This is a good idea, but will have to wait and see which ChainID EIP wins.  If it is current only it could be under Query, if not it would need to be on Block since the historical ChainID changes.     MicahZoltu:  Consider adding support for fetching the output data of an on-chain transaction. Since light clients may not have access to this, especially for ancient transactions, it could be a nullable field. Alternatively, can someone champion an EIP to get return data into the receipts please?  Even if it was constrained to 32-bytes, this would be a huge boon on dapp development. It should be noted that at the moment dapps that need return data write an event log with their transaction that contains the result, this results in more wasted resources than if return data was simply included in the receipt directly.   It should be noted that storing receipts is one of the biggest sections of unused data in a blockchain sync, so adding more data to the receipt will meet with some pushback.     MicahZoltu:  It appears that the Log filter has been ported forward basically as is. I feel like we can probably do better with the log filter query language than the current 2 dimensional array. I am not familiar enough with GraphQL\u2019s query to be able to assert that we can do better, but it feels like we could do better (like supporting logical AND and NOT)   There are no good graphql facilities for this unless a query structure is encoded into the schema.  the 2 dimensional array has proven to work with a minimum of overhead. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.772987814654481
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "31",
                    "likes": "0",
                    "time": "26/05/2019-06:56:47",
                    "content": "    shemnon:  This conforms to the JSON-RPC conventions. If we are worried about wire compression and HTTP GZip isn\u2019t sufficient then we sholdn\u2019t be using JSON result but RLP.   This new endpoint gives us an opportunity to fix mistakes of the past.  I am of the opinion that 0x prefixed hex encoding was a mistake in the Ethereum JSON-RPC API, and I would like to see us not port that mistake forward.     shemnon:  The fields using Long won\u2019t reasonably exceed 52 bits(gas, indexes, status, counts, protocol versions, block numbers), bringing that restriction in will in practice never matter and leak the JSON encoding into the spec.   IIUC, you are arguing that the specification actually is 52-bits, not 64-bits, but in a way that is undocumented.  If a future version of this specification includes a new variable that is > 52-bits, it would be valid but it would cause a bunch of problems.  If the intent is that a Long is never greater than 52-bits then the specification should indicate that, rather than just having it be tribal knowledge.     shemnon:  There is a root level Pending object that you can check account details, estimate gas, etc. That looks to be how such queries are done. What constitutes the \u201cpending\u201d query would be nice to have specified.   This is not a full replacement with the Ethereum JSON-RPC API.  For example, you can do an eth_call against a pending block via the Ethereum JSON-RPC, but I don\u2019t believe you can do that here.  Again, I think this is good, but should be mentioned in the Backward Compatibility section.     shemnon:  the 2 dimensional array has proven to work   As a dapp developer I can assert that this is not the case.  Building dapps against the log filter system is a PITA, not only is it difficult to understand, after you understand it it is difficult to read and comprehend.  It also is not as expressive as is often desired and dapps I have built have had to work around its lack of expressiveness.  Overall I think that the current 2d array solution results in a poor developer experience and an inability to execute certain queries. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.145717377860235
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "32",
                    "likes": "0",
                    "time": "26/05/2019-14:07:46",
                    "content": "    MicahZoltu:  This is not a full replacement with the Ethereum JSON-RPC API. For example, you can do an eth_call against a pending block via the Ethereum JSON-RPC, but I don\u2019t believe you can do that here. Again, I think this is good , but should be mentioned in the Backward Compatibility section.   You use the pending object and do a call just like the query object {   pending {     call(data: {from: \"a94f5374fce5edbc8e2a8697c15331677e6ebf0b\", to: \"0x6295ee1b4f6dd65047762f924ecd367c17eabf8f\", data: \"0x12a7b914\"}) {       data       status     }   } }  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.625
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "33",
                    "likes": "0",
                    "time": "26/05/2019-20:33:32",
                    "content": "    shemnon:  If JSON serialization is used, then the Long type may be problematic, pragmatically, as all JavaScript deserializers I\u2019m aware of automatically deserialize any JSON number into a JavaScript number , which cannot hold a 64-bit unsigned integer. Ideally, these would be deserialized into a bigint , but at the moment bigint isn\u2019t supported (per spec) in JSON (de)serializing. Consider either putting Longs into encoded strings like BigInt for usability reasons. Alternatively, consider specifying Long as 52 bits, since I believe every language that is used in the Ethereum ecosystem currently supports integers up to 52-bits wide.   Fair enough.     shemnon:  Why 0x-prefixed hex strings for Bytes32, Address, Bytes, and BigInt? Base32, Base58 and Base64 all compress data better and are well standardized so relatively easy to extract data from in any language. The 0x prefix not only is wasted bytes on the wire, but it also makes extracting the data more complicated in most cases as you first have to strip the 0x characters off of the string (exception for JavaScript which is notorious for \u201cguessing\u201d what you mean when processing data, and it will treat a string that starts with a 0x as a hex string, even if it isn\u2019t).   I generally agree that Ethereum\u2019s use of 0x prefixed hex everywhere is problematic. I\u2019m open to consider converting some of these types to base64. Address should definitely remain 0x-prefixed hex, as it\u2019s the canonical textual representation of an Ethereum address. BigInt currently supports numbers, decimal strings, and 0x-prefixed hexadecimal; that\u2019s consistent and I don\u2019t think it should be changed.     MicahZoltu:  As a dapp developer I can assert that this is not the case. Building dapps against the log filter system is a PITA, not only is it difficult to understand, after you understand it it is difficult to read and comprehend. It also is not as expressive as is often desired and dapps I have built have had to work around its lack of expressiveness. Overall I think that the current 2d array solution results in a poor developer experience and an inability to execute certain queries.   The filter format is restricted by what can be efficiently filtered for in a bloom filter. You\u2019re welcome to propose an alternate filter syntax, though! ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.552362351190476
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/rjl493456442",
                    "index": "34",
                    "likes": "0",
                    "time": "24/07/2019-03:13:23",
                    "content": "Can we consider adding some light client friendly query? For example add an additional query method named header(number: Long, hash: Bytes32): Header. Currently we can retrieve header information via JSON-RPC or GraphQL. But essentially this type of RPCs need to retrieve block first and then return a header instance. In the Geth side, we just merge a PR https://github.com/ethereum/go-ethereum/pull/19669 to implement GetHeaderBy* API so that we can retrieve header directly, which is super friendly to light client. What\u2019s your opinion about it  @Arachnid ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 6.091666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "35",
                    "likes": "0",
                    "time": "24/07/2019-21:15:34",
                    "content": "Presently the geth implementation fetches the header, and only fetches the full block when required, so a special API shouldn\u2019t be required. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.883928571428571
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "36",
                    "likes": "0",
                    "time": "24/07/2019-21:40:33",
                    "content": "I agree with @Arachnid,  this sounds like an implementation optimization that doesn\u2019t need to bleed into the API.  This would also involve introducing a Header type where the only real difference would be the inability to enumerate ommers and transactions.  Ommers (which are just headers) are already modeled as blocks. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal"
                    ],
                    "Sentiment": 5.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "37",
                    "likes": "0",
                    "time": "30/07/2019-11:22:06",
                    "content": "I\u2019m interested in moving this EIP forward. I think a few things are missing  Merge the schema of the proposal and the EthQL implementation. Update the proposal in some cases and update EthQL in the rest. Agree on a Pagination concept. Define error codes, maybe in accordance with JSON RPC Error Codes Improvement Proposal. See the GraphQL spec on errors. Testing and test cases, hopefully aligned with JSON RPC test cases. A champion who is motivated in getting this EIP accepted, has an in-depth knowledge of API design, GraphQL, and hopefully of the implementation details of one or more clients. A commitment from some client teams that they would implement it. It\u2019s already merged in go-ethereum and released in v1.9.0. Trinity has an open issue for it. I don\u2019t know of other clients.  ",
                    "links": [
                        "https://github.com/ConsenSys/ethql/issues/60",
                        "https://github.com/ethereum/wiki/wiki/JSON-RPC-Error-Codes-Improvement-Proposal",
                        "https://graphql.github.io/graphql-spec/June2018/#sec-Errors",
                        "https://github.com/ethereum/go-ethereum/pull/18445",
                        "https://github.com/ethereum/go-ethereum/releases/tag/v1.9.0",
                        "https://github.com/ethereum/trinity/issues/302"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.1875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shemnon",
                    "index": "38",
                    "likes": "0",
                    "time": "30/07/2019-15:52:05",
                    "content": "I think these goals would fit better in a new \u201cGraphQL revision 2\u201d EIP.  As a first revision EIP-1767 is implemented and operational, no need to make perfect the enemy of the good and go back and change what is out there. Pantheon also shipped support for GraphQL, so there are at least 2 clients with fully operational support deployed. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.678977272727273
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "39",
                    "likes": "0",
                    "time": "31/07/2019-14:44:55",
                    "content": "    shemnon:  I think these goals would fit better in a new \u201cGraphQL revision 2\u201d EIP   Let\u2019s go one by one.  Unify EthQL and EIP schemas \u2192 I think there\u2019s minor changes already proposed in this thread, like changing Long to BlockNumber. But it\u2019s not a showstopper, I agree. Pagination \u2192 Not a showstopper. Error codes \u2192 Not a showstopper, but definitely a big win over the current JSON RPC. Test cases \u2192 EthQL and Pantheon already have them. A pointer to Pantheon test cases may suffice. Champion \u2192 I think we need one   Clients \u2192 yep, 2 clients supporting it and 1 WIP sounds nice.  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.293181818181819
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "40",
                    "likes": "2",
                    "time": "31/07/2019-15:00:41",
                    "content": "This is a proposal how to unify the top level queries in the EthQL and the current EIP schema. It includes some changes to the EIP schema.     EthQL Current EIP 1767 Proposed EIP 1767 Update in EthQL Update in EIP Note     _: String  - - - Only in EthQL   account(address: Address!): Account  account(address: Address!): Account - Add it    block(number: BlockNumber, hash: Bytes32, tag: BlockTag): Block block(number: Long, hash: Bytes32): Block block(number: BlockNumber, hash: Hash): Block Change Bytes32 to Hash for hash Change Long to BlockNumber for number, Bytes32 to Hash for hash    blockOffset(number: BlockNumber, hash: Bytes32, tag: BlockTag, offset: Int!): Block  - - - Only in EthQL   blocks(numbers: [BlockNumber], hashes: [Bytes32]): [Block] blocks(from: Long!, to: Long): [Block!]! blocks(numbers: [BlockNumber], hashes: [Hash]): [Block] Change Bytes32 to Hash for hash Support arbitrary numbers and hashes. Drop to and from args (see blocksRange for that functionality)    blocksRange(numberRange: [BlockNumber], hashRange: [Bytes32]): [Block]  blocksRange(numberRange: [BlockNumber], hashRange: [Bytes32]): [Block] - Add it     gasPrice: BigInt! gasPrice: BigInt! Add it -    health: String!  - - - Only in EthQL    logs(filter: FilterCriteria!): [Log!]! logs(filter: FilterCriteria!): [Log!]! Add it -     pending: Pending! pending: Pending! Add it -     protocolVersion: Int! protocolVersion: Int! Add it -     syncing: SyncState syncing: SyncState Add it -    transaction(hash: Bytes32): Transaction transaction(hash: Bytes32!): Transaction transaction(hash: Hash!): Transaction Change Bytes32 to Hash for hash Change Bytes32 to Hash for hash      ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal"
                    ],
                    "Sentiment": 2.677884615384616
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "41",
                    "likes": "0",
                    "time": "31/07/2019-22:49:49",
                    "content": "@adamschmideg Thanks for the summary! A few notes:     adamschmideg:  account(address: Address!): Account   This is already available via the pending or block queries. I think it makes sense to make it explicit that the account state depends on which block you query it in.     adamschmideg:  |Change Long to BlockNumber for number, Bytes32 to Hash for hash|   With the use of the \u2018pending\u2019 query, I don\u2019t think we need a special BlockNumber type.     adamschmideg:  |Support arbitrary numbers and hashes. Drop to and from args (see blocksRange for that functionality)|   There\u2019s no need for a special query for this - callers can include multiple block sections in their query if they want multiple arbitrary blocks. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 5.432142857142858
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pgebheim",
                    "index": "42",
                    "likes": "1",
                    "time": "01/08/2019-05:14:40",
                    "content": "Thanks everyone for spearheading making GraphQL for Ethereum a real thing! Given there is now an expressive syntax for querying a node with the implementation of this proposal, it now opens the door to providing much friendlier APIs do dapp developers. One which standardize and simplify their jobs, and can provide performance and scalability performance to those apps. I\u2019d like to suggest a feature (or perhaps class of features) that would be a natural extension to the current schema and are perhaps supportable in a GraphQL Extension proposal \u2013 if not added directly to EIP-1767. I\u2019m looking forward to thoughts, feedback, and ideas on what the best way to take continue moving this proposal forward. Thanks in advance!  Proposal Extend Query Schema to support grouping by topics and returning FIRST N or LAST N logs from the group. Example Schema The follow example GraphQL schema addition would support all necessary operations for this grouped-selection proposal. Other schema formulations may do the the same, so another may work as well or better than this example.  type GroupedLogs {      groupTopics: [Bytes32!]!      count: Long!      logs(first: Int = NULL, last: Int = NULL): [Log!] }  type Query {      logs(filter: FilterCriteria!): [Log!]!            # The same FilterCriteria as `logs` are used, but specifying topic indices       # in the groupTopics parameter would allow grouping the resulting logs by       # the topics specified.      #      # For the context of this proposal, logs are always sorted in natural       # (BlockNumber, LogIndex) order.      groupedLogs(filter: FilterCriteria!, groupTopics: [Int!]!): [GroupedLogs!]! }  Rationale Logs are emitted by smart contracts for a number of reasons. For complex applications, they are often the only way of notifying a client of certain results, and being able to quickly select the last N available logs would man clients don\u2019t need to scan nodes for ALL logs just to retrieve the most recent event for the index they care about. An Example For example, say you have a game that generates NFTs, lets call them Wizards, where the Wizard has a contract function evolve. Holders of the Wizard can call evolve at any block, and by virtue of doing that transaction, there is some probability that the Wizard evolves and has its \u201cpower\u201d incremented by some amount. Assuming you were a UI that wanted to display all created Wizards, sorted by their current power, there are a few options at play of how to implement this solution.   Keep track of this large sorted list of Wizards on chain as the Wizard is evolved.  This doesn\u2019t scale for a number of reasons, most notably the unbounded data-structure / algorithmic runtime don\u2019t work with a Gas-capped based system.    Track all Wizards created with an off-chain aggregation (e.g. by specifying a log filter for a Created event), and using that list issue a large number of queries for the current chain state of each of the Wizards, and then organizing and sorting those items.  This requires issuing a large amount of queries to eth nodes to fetch all of the current state of the chain, potentially across thousands of eth_calls. Even when batching this is particularly expensive especially for light nodes where you\u2019re penalized heavily for asking for too much data. Also, if its not also coupled with an Event system there is no efficient way to query Power-over-time for a set of Wizards. When implemented in this way, current solutions effectively use centralized application servers to consolidate the on-chain data and provide a nice query interface.    Each time an Wizard is evolved, an Evolved event can be emitted, which would contain the current Wizard address as well as the power-level after the evolution. Using this method a client can issue a Query.logs request to fetch ALL logs across all Wizards, and then the client can store all those logs and do various operations on them, including sorting and grabbing the last Evolved event \u2013 using this to drive the UI.  This approach is architecturally nice because it allows things users expect, like being able to see power-over-time easily for Wizards, but comes with the drawback that a client must effectively fetch all available data to figure out what the current power-level of each Wizard. The fact that fetching all that data is a costly endeavor      This proposal Architecturally we could use the above approach, coupled with the ability of an eth node to return the last N Evolved logs for each Wizard. With this proposal, a dapp builder doesn\u2019t need to do any expensive blockchain scanning, nor coordinate large-scale eth_call spam to a node. This also totally eschews the need for business login in a centralized app server, and means that any node implementing this protocol can efficient power the user-interface for this theorized game. It also means that any third party node providers can easily cache the results of this particular query and scale it out even further, without having any specific business logic implemented for a dapp.  Real-Life Usage The current Augur event architecture works similarly to the 2nd the last example above. Each Augur market is represented by a contract deployment, and each match trade of shares on the market is logged by a parent contract (Augur) so that the trades can be analyzed by clients to support common trading use cases like Last Trade Price, or to display a user\u2019s current Profit and Loss across all the markets in which they participate. Currently, fetching the most recent state of the application involves scanning and caching all log messages for the set of markets, and then using that to drive basic UI functionality, like sorting based on last trade price. This is incredibly expensive per client, putting load on eth nodes to synchronize all log state to each client instead of returning the exact relevant data (the current state of each market over a range of blocks). Potential Objects Adding more advanced Filtering may add load on already loaded nodes While this may seem true on the outset, and will be true to some extent, I believe that this load pales in compare to the load genreated by the alternatives. Take a common case, a client is listening to the Events coming off an eth node, the client goes offline for some number of hours and then needs to catch up to the state of the chain.  In the case where they are able to query the chain directly for the data, they may need to do an unbounded number of requests to refresh the state of all objects that may have changed. In this case, a client would fall into the second case above and be required to issue potentially thousands of eth_calls to the node in order to get understand the current state. In the case where the client relies upon log notifications, they may need to scan up to 12 hours of logs to fetch the most recent events they care about. In this case there are two pieces: the index scan, and returning the data. The index scan for the data will need to happen both in the naive log fetching implementation and this proposal, but the amount of data returned stands to be significantly reduced if only the LAST N logs filter per group were used.  But Logs are supposed to be for Events, not long term storage! Even if this is true there are cases where relatively short term storage of the logs for reliable delivery to nodes which may go away for some small number of hours or days could see a benefit from the ability to efficiently query for logs. If, on the other hand, it is decided the Log storage should be for a moderate, long, or forever time scale for full nodes then being able to efficiently query large amounts of event data becomes even more useful. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.3979466786714685
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "43",
                    "likes": "0",
                    "time": "01/08/2019-13:05:05",
                    "content": "    Arachnid:  With the use of the \u2018pending\u2019 query, I don\u2019t think we need a special BlockNumber type.   I don\u2019t see how pending and the BlockNumber type are related. I mean to replace all occurrences of number: Long, both in block() and in blocks(). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal"
                    ],
                    "Sentiment": 4.993303571428571
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "44",
                    "likes": "1",
                    "time": "01/08/2019-14:05:59",
                    "content": "Parity assigned GraphQL-support to their next milestone. Yey. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "45",
                    "likes": "0",
                    "time": "01/08/2019-23:03:53",
                    "content": "    adamschmideg:  I don\u2019t see how pending and the BlockNumber type are related. I mean to replace all occurrences of number: Long , both in block() and in blocks() .   As I understand it, the reason for a special BlockNumber type is because in addition to numbers, we have two special values: pending and latest. In the current schema, the pending query handles the former, and the latest block can be fetched with a block query specifying neither number nor hash.     pgebheim:  Extend Query Schema to support grouping by topics and returning FIRST N or LAST N logs from the group.   Interesting idea, but I\u2019m not sure how this can be implemented efficiently on a node. As I understand it, returning the set of GroupedTopics would require scanning all the logs covered by the FilterCriteria, the same as a regular logs query. Why not simply do a regular query with the same criteria and do the grouping on the client side? The load on the node will be the same, and the reduction in data transmitted doesn\u2019t seem substantial. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.514384920634921
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/kshinn",
                    "index": "46",
                    "likes": "0",
                    "time": "03/08/2019-04:56:26",
                    "content": "@pgebheim I see a lot these ideas developing as extension EIPs that provide an API layer on top of the nodes, but not necessarily in them. There is definitely a base layer of GraphQL that should be supported by the nodes for all of the reasons listed in the original justification as well as the extra benefits you get with query stitching and composing extensions on top of the base API. In my opinion, EIP-1767 should cover the base functionality of what nodes currently expose for data in and out. There are additional layers that can be added on top of the nodes that can provide extra functionality. This can include an extension API for event / log indexing, transaction filtering, unrolling of the rlp encoded data etc. As to the differences between what is in EIP-1767 and what is currently in the EthQL project, I view this as an implementation toward this direction. For this particular standard (EIP-1767), I think it should limit the functionality and concentrate on providing a solid foundation from which we can extend in different directions including concepts like paging, log querying, and higher level convenience functions that are currently embedded in application code. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 4.969907407407407
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "47",
                    "likes": "0",
                    "time": "06/08/2019-09:37:04",
                    "content": "I don\u2019t see it specified how many items are returned from a collection. For example, Github has these requirements  Clients must supply a  first  or  last  argument on any connection. Values of  first  and  last  must be within 1-100.  This is one way to be explicit about it and it still allows for different pagination strategies. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.833333333333334
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pgebheim",
                    "index": "48",
                    "likes": "0",
                    "time": "09/08/2019-17:14:34",
                    "content": "This feels like a reasonable approach. Any thoughts on how we should approach various proposals for layers on top of EIP-1767? Is there a need for separate EIPs for each extension or group of extensions? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 6.166666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pgebheim",
                    "index": "49",
                    "likes": "0",
                    "time": "12/08/2019-15:04:05",
                    "content": "    Arachnid:  Interesting idea, but I\u2019m not sure how this can be implemented efficiently on a node. As I understand it, returning the set of GroupedTopics would require scanning all the logs covered by the FilterCriteria , the same as a regular logs query. Why not simply do a regular query with the same criteria and do the grouping on the client side? The load on the node will be the same, and the reduction in data transmitted doesn\u2019t seem substantial.   In a naive implementation an eth node could scan all blocks to find this data. In an optimized case, this would be a composite key lookup \u2013 followed by a HEAD or a TAIL operation on the on the data based off a natural index. DBMS can handle these sorts of queries over VAST data sets optimally. It\u2019s really a problem with known solutions, we just need to decide whether we want to do this in Ethereum nodes. I personally find these sorts of premature optimization conversations to be a limiting factor in terms of bring good interfaces for application developers. I can construct any number of examples where the data size of log fetching becomes substantial \u2013 but even worse than that is pushing that development cost to every single application development team, and the cost of degraded UX from web applications that now need to fetch potentially thousands of documents spanning thousands of blocks in order to render data from a small set. The performance penalty here becomes particularly poor when the client is speaking to a light client, where actually fetching logs from every block in a range requires also fetching block headers for each block returned even if only a small subset of them are actually used in the application. I have concrete examples from Augur if you want me to go into them. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.025714285714286
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "50",
                    "likes": "0",
                    "time": "12/08/2019-21:12:03",
                    "content": "    pgebheim:  I personally find these sorts of premature optimization conversations to be a limiting factor in terms of bring good interfaces for application developers.   We have half a dozen node implementations for Ethereum, and if we want graphql to be a standard, they\u2019ll all be expected to implement whatever is defined here. Thus, we should strive for a minimum viable interface first. For a feature to add value, it should do something that\u2019s significantly more efficient to do on the node than on the client. In cases like this, it seems like you could implement it just as efficiently by using the existing log filter support, then doing a grouping operation in the client. This functionality can be provided via a client-side library, meaning it only has to be implemented once - instead of half a dozen times. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.564814814814815
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/pgebheim",
                    "index": "51",
                    "likes": "0",
                    "time": "15/08/2019-23:52:25",
                    "content": "Lets do some math: For Augur\u2019s first order volume goals, we expect around 2600 trades per week. Currently, the logs that are emitted for a trade happening clock in at just around 900 bytes. This means that a normal weekly cadence of coming back to trade will cause each user to download 2.34mb of data just for this one piece in order to update their local databases to then update the order. Those trades are likely to exist across ~30 markets at a time, meaning that if we could just fetch the latest log for each market efficiently, we would transmit 30*900, or around 27kb of data to get that user\u2019s state up to date. In any respect, transmitting an extra 2.2mb of data to client + the associated cost of deserialization, grouping, etc etc is going to make the user experience of a dApp that uses an eth node directly far worse. NOW \u2013 lets take a look at this from the perspective of a user that is getting this data from a light node  Based on the way that light nodes need to fetch data from full nodes, and then verify the blocks before giving it a to a client, a light client needs to on average fetch 8x the amount of data in block headers as a full node. In the example above, a light client would need to fetch ~2600 blocks worth of headers from a full node to scan and return all the logs. This creates the situation where the node must request ~20,000 block headers from the full nodes that are serving that light client in order to return the data. Contrast this with the case where the light client can ask the full node directly for the last logs, where it would need to fetch and validate headers for 30 blocks for a total of 240 blocks to fetch. Put that into the context of light client throttling and you\u2019ll see that clearly expecting all clients to just fetch all the logs is going to put undue stress on the entire network and degrade UX for any client that is attempting to take advantage of eth nodes. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.916666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "52",
                    "likes": "0",
                    "time": "16/08/2019-04:54:45",
                    "content": "    pgebheim:  For Augur\u2019s first order volume goals, we expect around 2600 trades per week. Currently, the logs that are emitted for a trade happening clock in at just around 900 bytes. This means that a normal weekly cadence of coming back to trade will cause each user to download 2.34mb of data just for this one piece in order to update their local databases to then update the order. Those trades are likely to exist across ~30 markets at a time, meaning that if we could just fetch the latest log for each market efficiently, we would transmit 30*900, or around 27kb of data to get that user\u2019s state up to date. In any respect, transmitting an extra 2.2mb of data to client + the associated cost of deserialization, grouping, etc etc is going to make the user experience of a dApp that uses an eth node directly far worse.   In the edge case that you only want the latest log entry for each group, this would save some data from node to client, yes. I\u2019d suggest, though, that this is quite uncommon - usually we need all the logs in order to reconstruct a state. Neither solution is scalable, though, because of the load it puts on the node - a solution like The Graph makes a lot more sense. Also - 900 bytes per log entry?! That\u2019s 28 words. What on earth is being recorded here?     pgebheim:  Based on the way that light nodes need to fetch data from full nodes, and then verify the blocks before giving it a to a client, a light client needs to on average fetch 8x the amount of data in block headers as a full node.   Er? Where are you getting this from?     pgebheim:  Contrast this with the case where the light client can ask the full node directly for the last logs, where it would need to fetch and validate headers for 30 blocks for a total of 240 blocks to fetch.   Adding this interface to the graphql API won\u2019t allow this - you\u2019d need to change the light client protocol instead. I don\u2019t see how this is possible, though, as there\u2019s no way at present to generate a proof-of-nonexistent for a more recent entry than the one the full node returned. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.923387096774193
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "53",
                    "likes": "0",
                    "time": "16/08/2019-11:56:07",
                    "content": "    Arachnid:  This functionality can be provided via a client-side library, meaning it only has to be implemented once - instead of half a dozen times.   You say if a functionality can be added both on the node side, and on the client side, we should strive for the minimum on the node side. I think the math is actually the inverse here. We have 5-6 node implementations, and we\u2019d need at least as many client-side libraries as mainstream programming languages. So if we want to gain a wider adoption of GraphQL, we have two options to support debated features, a) Make it part of the spec and include it in the node implementation b) Create libraries for the mainstream languages and implement it there ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.033333333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "54",
                    "likes": "0",
                    "time": "16/08/2019-12:06:46",
                    "content": "It\u2019s not clear to me who the target audience / potential users of GraphQL would be. The EIP suggests it\u2019s a long-term replacement of JSON-RPC. But it covers only a subset of JSON-RPC. Do you see the two APIs living together in the long run? Will their feature set diverge? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 4.875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "55",
                    "likes": "0",
                    "time": "16/08/2019-12:17:02",
                    "content": "Why do from(): Account! and to(): Account! in a transaction take a block argument? I\u2019d specify the block like this: block(number: 42) {   transactions {     from     to   } }  Now I have to specify a \u2013 potentially different \u2013 block for the accounts. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "56",
                    "likes": "0",
                    "time": "16/08/2019-20:02:17",
                    "content": "    adamschmideg:  You say if a functionality can be added both on the node side, and on the client side, we should strive for the minimum on the node side. I think the math is actually the inverse here. We have 5-6 node implementations, and we\u2019d need at least as many client-side libraries as mainstream programming languages. So if we want to gain a wider adoption of GraphQL, we have two options to support debated features, a) Make it part of the spec and include it in the node implementation b) Create libraries for the mainstream languages and implement it there   For most purposes the standard graphql libraries already available in the user\u2019s language of choice should be sufficient - though in practice, most consumers are in JavaScript. If we start adding everything and the kitchen sink, we will end up with 0 implementations on the server side, or several incompatible implementations, and arguing about how many languages will have to add support on the client side will be academic.     adamschmideg:  It\u2019s not clear to me who the target audience / potential users of GraphQL would be. The EIP suggests it\u2019s a long-term replacement of JSON-RPC. But it covers only a subset of JSON-RPC. Do you see the two APIs living together in the long run? Will their feature set diverge?   There\u2019s a table in the EIP that shows JSON-RPC coverage; the GraphQL API covers all JSON-RPC functionality other than deprecated functionality (mining interface, transaction signing, etc).     adamschmideg:  Why do from(): Account! and to(): Account! in a transaction take a block argument? I\u2019d specify the block like this:   To give you the flexibility to specify the block you want to fetch the account at. We should make it clear that the default value is the block the transaction was mined in, though, or the latest block if the transaction has not been mined. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.707261029411765
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "57",
                    "likes": "0",
                    "time": "28/08/2019-13:14:44",
                    "content": "I set up a Slack channel for all Ethereum+GraphQL  related stuff, including this EIP. Feel free to join. It may enable a quicker feedback loop for clients wanting to implement or implementing a GraphQL interface. I created an initial test suite based on Pantheon\u2019s work. It\u2019s a collection of graphql files and their expected output as json. This is not it\u2019s final location, we\u2019re working on a framework to run the tests across clients.  See the README.md in the parent folder for how to run the tests. According to these tests, the Geth and the Pantheon implementations are somewhat different, 8 of 67 tests fail with Geth. I\u2019ll look into them and compare with the current spec. ",
                    "links": [
                        "https://gitlab.com/offcode/ethereum-graphql/tree/master/test-graphql/eth"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 4.875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "58",
                    "likes": "1",
                    "time": "29/08/2019-22:26:55",
                    "content": "    adamschmideg:  I set up a Slack channel for all Ethereum+GraphQL related stuff, including this EIP.   Please, mercy, I can\u2019t handle another Slack tab open in my browser all the time. Can\u2019t we use Gitter, or Discord, if we must? ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "59",
                    "likes": "0",
                    "time": "30/08/2019-10:02:22",
                    "content": "    Arachnid:  Can\u2019t we use Gitter, or Discord, if we must?   Sorry, I know given a few different people in this space, they all prefer different chat apps. I have 9 installed on my phone  You can set up a Zapier integration from Slack to Discord. Or I\u2019ll cross-post the most important updates here. I\u2019m checking the differences between the Geth and Pantheon implementations. What I\u2019ve found so far is how they differ in error handling. They both consider a case, but return different error messages (for example \u201cInvalid params\u201d vs \u201chex number with leading zero digits\u201d). I see a few options here.  Standardize error messages Ignore the message part and standardize only the categories (the \u201cextensions\u201d part of the returned json). Standardize \u201ccompile-time\u201d error messages only (missing fields, etc).  They treat missing entities differently. If you query the balance of a non-existent account or a property of a non-existent block, Geth will return 0 and null, respectively. Pantheon returns an error in both cases (specific to the case). In this case, I\u2019d return both data, and errors (which is ok by the spec). So a dumber client would be use 0 balance as earlier, a smarter client could tell the account name was too short. ",
                    "links": [
                        "https://graphql.github.io/graphql-spec/June2018/#sec-Errors"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.052631578947368
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "60",
                    "likes": "0",
                    "time": "10/09/2019-10:51:56",
                    "content": "I wrote a draft of how to handle error messages. It\u2019s in the form of a WIP pull request so we can discuss it. I think an important lesson of JSON RPC is that we should standardize error codes and messages. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 7.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Arachnid",
                    "index": "61",
                    "likes": "1",
                    "time": "10/09/2019-21:06:43",
                    "content": "Thanks for your hard work! I\u2019m just about to leave on a week\u2019s vacation, but I\u2019ll take a close look as soon as I get back. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.725694444444445
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/adamschmideg",
                    "index": "62",
                    "likes": "0",
                    "time": "12/09/2019-10:02:35",
                    "content": "I encountered an issue with Long. JavaScript can represent numbers up to 53 bits, that\u2019s why Long is not part of the GraphQL spec. Our schema uses a custom Long type defined as a 64-bit int in these cases:  gas-related (cumulativeGasUsed, estimateGas, gas, gasLimit, gasUsed) block-indexing (highestBlock, etc) counting states (knownStates, pulledStates) nonce transactionCount  I think we should use a custom Int53 in cases where it\u2019s feasible and use BigInt for the rest. Or maybe define Long in a way that it returns an integer if it fits in 53 bits and a hex-encoded string otherwise, like in this example. So it would be an error to input or output \u201c0x1\u201d because it would fit into the 53 bits. ",
                    "links": [
                        "https://stackoverflow.com/a/49911974/380587"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.2
                }
            ]
        }
    ],
    "group_index": "710"
}