{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/analysing-time-taken-to-extract-data-from-an-ethereum-node/911",
            "title": "Analysing time taken to extract data from an ethereum node ",
            "index": 911,
            "category": [
                "Primordial Soup"
            ],
            "tags": [
                "NONE"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/ankitchiplunkar",
                    "index": "1",
                    "likes": "1",
                    "time": "31/07/2018-12:53:59",
                    "content": "Reposting the discussion from ethresearch here. We analyse the time-taken to extract data from an Ethereum node. Block time of the Ethereum mainnet is ~15 seconds, if certain data extraction functions take more than 15 seconds to pull data then a block explorer using these functions will fall behind the ethereum chain. We take one such heavy function, which is used commonly in block explorers to access the internal transactions, and demonstrate that if gas used increases to more than 12 million units, getting internal transactions will take greater than 15 seconds and the block explorers will start lagging behind the main chain. This means that we should constraint gas limit to 12M, for a block explorer to easily follow the mainnet. The following notebook can be used to rerun the analysis. Key findings:  Even at the current 8M gas limit we sometimes overshoot the 15 second time limit. The time taken to extract transaction traces depends on complexity of the contract codes and gas used in the block. If gas used increases to more than 12M then we would consistently start hitting this time limit. The above value of gas limit assumes that transactions in the block are not complex contract calls. It is possible to find specific OPCODE\u2019s which might create complicated contracts making the extraction exceed the time-limit at lower gas limits. One method to bypass this bottleneck is developing Ethereum nodes optimized in data-delivery.  Presentation:  How to rerun the notebook  Have an archive node synced upto 5M blocks. Install python3 Clone the research repo with notebook Install dependencies from requirements.txt: $ pip install -r requirements.txt  Open the notebook using: >>> jupyter notebook   Questions: Tweet to @AnalyseEther ",
                    "links": [
                        "https://github.com/analyseether/research/blob/master/data_extraction_regression/data_gathering.ipynb"
                    ],
                    "GPT-summary": "The post discusses the time taken to extract data from an Ethereum node and how it affects block explorers. The author analyzes a heavy function used to access internal transactions and demonstrates that if gas used increases to more than 12 million units, getting internal transactions will take greater than 15 seconds and the block explorers will start lagging behind the main chain. The post also suggests developing Ethereum nodes optimized in data-delivery to bypass this bottleneck. The author invites questions and feedback on the topic.",
                    "GPT-proposal-categories": null,
                    "GPT-discussion-categories": [],
                    "Sentiment": 5.282857142857143
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ligi",
                    "index": "2",
                    "likes": "0",
                    "time": "31/07/2018-13:30:57",
                    "content": "    ankitchiplunkar:  One method to bypass this bottleneck is developing Ethereum nodes optimized in data-delivery.   maybe replacing RPC with GraphQL might be an option. There was a interesting talk about this at dAppcon: https://twitter.com/dappcon_berlin/status/1017436965803909120 ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 7.5
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ankitchiplunkar",
                    "index": "3",
                    "likes": "0",
                    "time": "31/07/2018-14:39:41",
                    "content": "@ligi Can you provide a video link to this talk/workshop? To my understanding, EthQL uses JSON-RPC to feed data in its GraphQL indexer. https://github.com/ConsenSys/ethql/blob/13ee8107e2b8ba92a5f26ec498bc10f0cce7c0f2/src/config.ts#L16. Keeping the above analysis still valid, since the bottleneck is introduced due to JSON-RPC calls. ",
                    "links": [],
                    "GPT-discussion-categories": [],
                    "Sentiment": 4.6875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ligi",
                    "index": "4",
                    "likes": "1",
                    "time": "31/07/2018-14:54:17",
                    "content": "looks like this video is not yet released - they are still in the process of cutting. You can find the videos here: https://www.youtube.com/channel/UCruCCeCNWBAM7JI-xAMtXpQ/videos Yea - in the end the detour over json-rpc needs to vanish in the end - but it can be a nice PoC as it looks that GrapQL can reduce overhead nicely for this use-case. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "5",
                    "likes": "1",
                    "time": "01/08/2018-22:43:07",
                    "content": "The bottle neck is not the RPC calls per-se. It\u2019s the shape of the data and the fact that the data is an append-only time-ordered log. GraphQL (or any scraper for that matter) must convert that time-ordered log into an indexed database. This means it must extract not only all the blocks and all the transactions but all the traces (internal messages or internal transactions) as well. Since the traces are not physically stored on the hard drive, they must be re-generated when requested (via RPC or IPC) or, as noted, by modifying the internals of the node. Not to speak for AnalyzeEther, but once the data is extracted, I\u2019m sure most modern databases are more than capable of delivering the data. The issue is in the extraction and indexing. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.804166666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ankitchiplunkar",
                    "index": "6",
                    "likes": "0",
                    "time": "03/08/2018-07:18:34",
                    "content": "As @tjayrush said, getting the data out of a node is the issue. After the data is extracted and indexed and modern database, GraphQL, PostgreSQL or MySQL would return the data at speed. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 6.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/dontpanic",
                    "index": "7",
                    "likes": "1",
                    "time": "03/08/2018-21:22:35",
                    "content": "The way it is done with etherhub.io is to export blocks into a mongoDB and process data from there. I\u2019ve had similar experience with hyperledger fabric and couch db, and Microsoft workbench(parity poa) with azure sql. The node is used for write operations, the db is used for trivial reads and analytics. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ankitchiplunkar",
                    "index": "8",
                    "likes": "0",
                    "time": "05/08/2018-19:50:09",
                    "content": "    dontpanic:  export blocks into a mongoDB   @dontpanic the above analysis demonstrates that the process of extracting trace data from a node can take more than 15 seconds for a block. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/dontpanic",
                    "index": "9",
                    "likes": "2",
                    "time": "05/08/2018-20:30:59",
                    "content": "export of raw blocks  is done in a  synchronous manner, not at the time of query. a script is constantly calling the node for the \u2018latest\u2019 block and if it is > the last block store it is processed and appended in the db. traces and lookups  only happen in the mongodb stored data so you dont have the latency of the node. nodes are processing a lot of other info besides the requests, using mongo or couch makes dynamic reads more efficient than leveldb of a node. separate schema store block data & transactions. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.103021978021978
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "10",
                    "likes": "0",
                    "time": "05/08/2018-20:45:58",
                    "content": "    ankitchiplunkar:  the above analysis demonstrates that the process of extracting trace data from a node can take more than 15 seconds for a block   Do you have a sense for the percentage of such blocks there are? Does this occur for 1% of blocks, 10%, 20%? I\u2019m sure it happens for some blocks, but I wonder how many? Also, I see you\u2019re using the w3.parity.traceReplayBlockTransactions.  Have you tried other methods? (We use Parity\u2019s trace_transaction.) I wonder if that affects the performance. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 6.145833333333334
                }
            ]
        }
    ],
    "group_index": "1726"
}