{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/who-what-and-why-decides-the-gas-limit/6753",
            "title": "Who, what and why decides the gas limit? ",
            "index": 6753,
            "category": [
                "Process Improvement"
            ],
            "tags": [
                "governance",
                "gas"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/uri",
                    "index": "1",
                    "likes": "2",
                    "time": "31/07/2021-06:06:58",
                    "content": "During the PEEPandEIP discussion today on the gas limit, @vitalikbuterin suggested that moving to long-form might be valuable for more complete arguments. Below is a short summary of the discussion (H/T @poojaranjan!), and what I see as the open-ended questions. Quick Summary Vitalik mostly covered the technical limitations for increasing the gas limit, as he previously articulated so well in his post. The most interesting points Vitalik brought up, which are worth reiterating:   The gas limit is not binary \u201csafe\u201d or \u201cunsafe\u201d - it\u2019s a spectrum, and it must balance safety and usability.   ETH design should make it easy for \u201cmany users\u201d to run their own node, to protect users from the majority of hash or stake. Sync times should remain within the 12h-1d range.   the gas limit is very much a question of community values, and should be decided by the community. Technical arguments are critical, but we should be careful not to allow them to be used to sneak in personal opinions.   Open-Ended Questions I think most would agree with Vitalik\u2019s technical analysis (even if I think he slightly mischaracterizes the limitation at the network layer - propagation vs bandwidth - and I think @AlexeyAkhunov @vorot93 and the Arigon team might disagree with his storage/memory numbers). However, there were a few points which remained open-ended, which are the reason for this discussion. Specifically:  How the current gas limit is actually being set right now? Who should set it? How should it be set?  First, to the one point I think Vitalik was completely off (partially due to simplification, partially due to not being in close touch with the smaller mining pools). Vitalik outlined how:  Miners adjust the gas limit with every new block they mine, pushing it up or down by up to 0.1% Miners listen to core devs (see 2016 Shanghai DDoS)  1. Miners adjust the gas limit This is not how the gas limit is set in practice. The gas limit is being set by those constructing the blocks - the mining pools. Tomato Tom\u00e2toe, right? well, no. Mining pools are much more concentrated, with the top-3 pools controlling 55% of the hashpower. Pools, especially the smaller ones, are mostly a DevOps operations, helping miners to reduce payout variance and remove their need to run nodes with great connectivity of their own. Pools all generally earn just 1% of the mining revenues, so they hold all the power but not the $. The gas limit is effectively set by the top-3 pools, and the rest of the mining pools just follow their lead. When we asked some of the top-20 pools why are they setting the gas to a certain level, they were surprised to learn that they control this parameter. Let that sink in. Obviously, someone from their tech team knows and adjusts the gas limit parameter, but as an entity most don\u2019t know nor care to learn what the gas limit should be. It is controlled by the top-3, with some push from Pools 4-6, but 40% of the hashpower just follows their lead. Why is this so important that I\u2019m draggin y\u2019all thru the details? a. Because it means the gas limit is currently controlled by 3 actors. In fact, two are so big as to have veto power - without them the gas limit won\u2019t change since 40% are just followers - and we have seen a pool singlehandedly preventing the move to 15M gas for months, and unfortunately their incentives are misaligned with the long term success of ETH b. Pools hold immense power, but capture relatively little income, and can be \u201cpersuaded\u201d relatively cheaply - about $1-2M. Whenever I say this everyone jump and say \u201cbut if the pools push the gas limit to dangerous limits they will just get forked by the devs\u201d which is obviously true. But I\u2019m not talking about raising the gas limit to 80M, I\u2019m talking about moving from 12.5M to 15M gas. If tomorrow 5 pools push it to 17.5M gas, and in 3 months to 20M - would you know if I bribed them? Or Alameda, Wintermute, CMS, 3Arrows, Multicoin, Parafi or any of the other major DeFi trading firms for whom $1-2M is pocket change? 2. Miners listen to core devs They don\u2019t. Sure, they will if there\u2019s a catastrophe, an obvious malicious behavior, or an immediate danger. They are already walking on thin ice with the community and everyone knows it. but do they follow the gas limit core devs tell them? no  We shouldn\u2019t blame them though - different core devs have different opinions, and it\u2019s very hard for them to gage what the community actually wants.  Screen Shot 2021-07-31 at 12.26.06 AM1166\u00d7556 93.3 KB   Screen Shot 2021-07-31 at 12.28.01 AM1176\u00d7588 73.8 KB   Screen Shot 2021-07-31 at 12.26.46 AM1176\u00d7586 93.7 KB  Discussion So who should decide the gas limit? I think everyone (Vitalik and Peter included) believes it shouldn\u2019t be the core devs - that it should be \u201cthe community\u201d.  Screen Shot 2021-07-31 at 12.34.37 AM1180\u00d7256 49.9 KB  But that doesn\u2019t really say much\u2026 If I\u2019m running my own ETH validator, what should I set the gas limit to be? I don\u2019t know what the different core devs think, using the defaults is the same as letting the core devs set it (which nobody wants). and how do we pass the decision from mining pools to the community? I think that instead of hiding the question behind a giant \u201cthis is technical!\u201d sign, the community should be presented with:   Insight to the different opinions among core devs.   explanations on budget implications: ETH can handle X if nodes run on $75 raspi, or Y if nodes run on $400 laptops, or Z if it requires $800 laptop to run a node. There will be disagreements on the assessment, and these will change over time due to improvements to HW and client implementations, but it would be a good starting point.   a way for the community to make its desires clear, and an incentive for pools to follow the community desires.   I know there are those who disagree with me, and I\u2019d be glad to discuss where we diverge in our view of the current situation and the potential solution. ",
                    "links": [
                        "https://etherscan.io/stat/miner?range=7&blocktype=blocks",
                        "https://ethereum-magicians.org/uploads/default/original/2X/5/5c0a89919bc8ed75c004b4ecfb984d47669fe66c.png",
                        "https://ethereum-magicians.org/uploads/default/original/2X/b/b4b3ae2eb5bf26946b92a67911ce630c441723ae.png",
                        "https://ethereum-magicians.org/uploads/default/original/2X/f/f73a9089335d159eacceab84b44434cd25cfcae5.png",
                        "https://ethereum-magicians.org/uploads/default/original/2X/6/6b955ea6ee77a9e9642e686f19a69b3838427eeb.png"
                    ],
                    "GPT-summary": "The post discusses the current state of the gas limit in Ethereum and how it is being set by mining pools. The author provides constructive criticism and asks questions about who should set the gas limit and how it should be set. The post also suggests presenting insights into different opinions among core developers, explanations on budget implications, and a way for the community to make its desires clear.",
                    "GPT-proposal-categories": null,
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None"
                    ],
                    "Sentiment": 5.680224941048111
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/vbuterin",
                    "index": "2",
                    "likes": "4",
                    "time": "31/07/2021-23:46:21",
                    "content": "    uri:  If tomorrow 5 pools push it to 17.5M gas, and in 3 months to 20M - would you know if I bribed them?   Historically, as far as I can tell, none of the gas limit increases were initiated by miners. They all followed some kind of movement with wide community support agitating for a gas limit increase. Sure, some core devs were often not enthusiastic about the increase, but they also did not violently oppose (there\u2019s always a distribution of opinion, the extreme tail of the distribution does not reflect on the mean). Often, the increase happens after core devs\u2019 concerns are satisfied (eg. the recent increase to 15M happened the week after Berlin included EIP 2929 that addressed DoS concerns). So if tomorrow 5 pools push the limit to 17.5M, this definitely would raise eyebrows, and if they then push it further to 20M it would raise even more eyebrows and I fully expect \u201cremove gas limit voting\u201d EIPs to be proposed and discussed on ACD. People are sniffing not for positive evidence of collusion (which is hard to get), but for absence of evidence of a \u201cnormal\u201d explanation.  the community should be presented with:  I\u2019ll also put into writing the suggestion I made in the call: we should just increase the gas limit on one of the testnets (Ropsten or Goerli or a new one but one that people actually use for applications) to 40M+, just so we can have an environment where the gas limit is higher and we can better understand what the consequences of that environment are. People are often better at understanding tradeoffs in the specific than in the abstract. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.698174931129476
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/uri",
                    "index": "3",
                    "likes": "0",
                    "time": "02/08/2021-20:44:40",
                    "content": " the extreme tail of the distribution does not reflect on the mean  But Peter is anything but the extreme, at least in the public eye. He is the loudest voice in the community when it comes to the gas limit, and to many he represents \u201cwhat core devs think\u201d (regardless if that\u2019s true or not). Even your own tweet says you told sparkpool you oppose increasing the gas limit because he opposes it. The fact nobody has any insight into what other core devs think, or what the community wants just doesn\u2019t make sense in the world\u2019s #1 transparent decentralized ecosystem. But more importantly, if the pools would increase the gas limit to 17.5M today, Peter would object just like he did in the past - it won\u2019t \u201cfeel\u201d any different. And if they do it again, maybe not in 3 months but in 4-5 months, I don\u2019t think anyone will be able to tell whether this is \u201corganic\u201d or not. My point is not to say that Peter is wrong in opposing any increase - it is that\u2019s the entire process is opaque:  For pools, trying to understand what the community wants For the community, trying to reach a collective decisions For anyone, trying to gage whether the current gas limit is what the community wants   we should just increase the gas limit on one of the testnets (Ropsten or Goerli or a new one but one that people actually use for applications) to 40M \u2026 People are often better at understanding tradeoffs in the specific than in the abstract.  I think that\u2019s a great idea! how to achieve this though? you need to communicate to everyone mining on it that this is what the community wants. (well, you could just testnet-vote on the gas limit with EGL\u2026 ) ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "3rd party or author wants to collaborate on proposal"
                    ],
                    "Sentiment": 5.682473776223777
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/poojaranjan",
                    "index": "4",
                    "likes": "1",
                    "time": "13/08/2021-11:59:07",
                    "content": "Recording of the conversation on Block gas limit with @vbuterin     ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/kladkogex",
                    "index": "5",
                    "likes": "0",
                    "time": "13/08/2021-12:58:59",
                    "content": "I do not understand the general argument that Peter makes about the general health of the network being affected by block size. Anything can run faster if computers become faster and networks become faster. I personally think that fears of ETH becoming too centralized if higher block limit forces people to use more powerful computers and faster networks are grossly exaggerated. It may force out some people who run on AWS , but it is arguably better for decentralization.   People run ETH2 clients on AWS and store keys in plaintext.  Jeff Bezos can take over ETH2 anytime. If people are forced to run powerful hardware servers it is going to be much more secure. Insecurity of ETH2 keys is a major problem that no one is talking about. A vulnerability in an ETH2 client can take the entire network down in minutes if the vulnerability allows for BLS key extraction. In addition, lots of performance problems come from geth quality not improving much. At SKALE we fight with geth bugs, slowness, instability and chaos every day. It is not able to reliably respond to basic API calls.  May be people need to switch to turbogeth or something else. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.589236111111111
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shamatar",
                    "index": "6",
                    "likes": "1",
                    "time": "23/08/2021-18:24:29",
                    "content": "I think that we encounter a usual twitter problem where there are a lot of opinions and heated argument, but without any material to support the case or help people to make an educated choice on a subject. Let\u2019s try to make some assumptions and analyze the current state of affairs. Some of these assumptions may be opinionated, but conclusions will follow only after facts that are global. So, let\u2019s start with some facts that readers should be aware:  EVM opcodes are mispriced all over the place, but fixes only affect a storage (discussion of Geth vs Erigon is beyond this post or thread). One can run some Geth benchmarks on opcodes and see that  BenchmarkOpAdd256-12    \t15743443\t        75.0 ns/op\t      32 B/op\t       1 allocs/op BenchmarkOpMul-12    \t12864930\t        83.4 ns/op\t      32 B/op\t       1 allocs/op BenchmarkOpDiv256-12    \t 6107662\t       194 ns/op\t      32 B/op\t       1 allocs/op  Let\u2019s ignore for a second an underlying reason why addition is as expensive as multiplication (I didn\u2019t look for it, may be it\u2019s overhead bound benchmark, but what it benchmarks then?), but it\u2019s clear that division is few times slower than multiplication, while opcodes for them are priced the same (5 gas). Those benchmarks in Geth may be not benchmarks for the \u201cworst execution time cases\u201d, but the intuition is that divisions should be more expensive. On the machine that performed the benchmarks once can assume 30-35M gas/second for CPU bound tasks, so it\u2019s around 2.5 gas for multiplication and 6 for division. Similar examples may be found for other opcodes.  A number of empty blocks over ranges is the following:  from 12_000_000 to 12_010_000, around 12.5M gas limit: 179 empty blocks (with 0 transactions) from 12_500_000 to 12_510_000, around 15M gas limit: 140 empty blocks from 13_000_000 to 13_010_000, around 30M gas limit under EIP1559: 138 empty blocks  (yes, it\u2019s a small statistics, but better than nothing) Now comes an assumptions: if blocks on average represent a balanced load (so there is nothing special in those block like state read spam, and opcodes on average more-or-less ok-ish priced. Here we have a contradiction, but let\u2019s assume at least something productive) then ratio of empty blocks to the number of all blocks is roughly proportional (with a factor of 2 most likely) to the ratio of the time that takes for a miner to process a block to the time between blocks. Why we can assume it: a reasonable behavior for a miner would be to start to mine on top of the newly received block (it it passes the PoW check) while the block is parsed, and then fill the block and start mining on a full one. Numbers above indicated that such ratio is roughly constant even though the block limit has increased, so performance of the miner\u2019s hardware has increased roughly proportionally. Another explanation may be also that that EVM is opcodes are so mispriced on average that large increase of the gas limit and number of executed opcodes per block only lead to a proportional increase of the execution time that of that opcodes that was negligible at the first place (e.g. that some execution time contribution was 1% and now is 2% on top of some other largely unchanged contributions). Alternatively that block size has increased proportionally to their part related to storage related opcodes cost. Now one would ask a question whether \u201ccasual\u201d mining exists on scale (reasonable mining decentralization beyond pools), is having a causal mining a target, should one assume that non-mining node hardware should be at some minimal decent performance level, etc. E.g. if average time to assemble a block by miner is roughly 2% of the time between block then one can naively assume that CPU/IO bound node would be able to keep up with a network even if it would have 50 times less performance than a mining node (50 is an upper bound, as a node should sync a history first). I hope readers can make their choice more educated, but I would also appreciate if EGL would perform some more thorough analysis on a subject and assemble it in some place as it would help everyone. In any case I do not think that any hardcap on a gas limit is a solution as it builds a negative precedent for a network that wants to be decentralized, but arbitrary developers decisions can take declared protocol powers from a wider audience - that kind of negates that essence of PoS. Regarding DoS attacks: if one claims that they exist in a current state of Ethereum 1.0 (e.g. storage access is too cheap, etc) such information (vector, etc) should be made public and not just claimed. If one would really want to DoS Ethereum it wouldn\u2019t be hard for them to find such vector if it exists and if such DoS is in any sense rational, so hiding any such vectors only protects from lazy \u201chackers\u201d. I\u2019m not aware about the current state of performance bottlenecks between clients, but again, it\u2019s not a thread for Geth vs Erigon discussion. My personal take it that clients\u2019 competition should be supported as a healthy initiative to keep network in better state and remove inefficient legacy. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party auditing and reviewing proposal",
                        "None of the topics listed match",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.21432579765913
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/norswap",
                    "index": "7",
                    "likes": "0",
                    "time": "25/08/2021-13:57:56",
                    "content": "    kladkogex:  I do not understand the general argument that Peter makes about the general health of the network being affected by block size. Anything can run faster if computers become faster and networks become faster. I personally think that fears of ETH becoming too centralized if higher block limit forces people to use more powerful computers and faster networks are grossly exaggerated. It may force out some people who run on AWS , but it is arguably better for decentralization.   It depends where we you want to be on the continuum of decentralization. A geth node currently requires a 1TB SSD, and will soon be past that. It\u2019s probably possible to diminish that by accepting that one does not need to keep full blocks for the whole history. But still, it\u2019s not hard to imagine a  medium-term future where you need 10TB in SSD. I can\u2019t even buy that from Amazon, and buying 2x4TB drives will put me back ~12000+\u20ac. And in fact, the the decentralization continuum goes the other way too - it would be better if we\u2019re able to validate the network on our existing machine, wouldn\u2019t it? What about a machine that isn\u2019t always on (laptop), what about a smartphone? I think there are a few \u201cengagement cliffs\u201d where you get less and less people that are able to validate the network. Right now you can with some difficulty validate the network on a dev laptop with a 1TB SSD. That might not hold for very long. The fear is that if you increase the throughput of the chain considerably, you might get to the point where the validation setup is the same cost as a second-hand car (a few 1000$) and you need to setup some kind of nerd cave to host it. At that point there are very few \u201chobbyists\u201d validating anymore and it\u2019s all the more easy to dismiss them and move further in the direction of centralization. Even if we ignore storage (by far the biggest concern - that\u2019s why nobody is overly looking at gas cost of multiplication vs division @shamatar, it\u2019s not even remotely on the critical path), increasing the throughput \u201cbecause we can\u201d is a slippery slope, because it\u2019s always possible, but each time you increase it, it\u2019s possible for less and less people. I\u2019m not saying we shouldn\u2019t increase the limit, but I feel like people who want to increase it systematically fail to address the concern of people who don\u2019t want to increase it, which is: how does that impact who validates the network (or just as importantly, who can validate the network)? For sure, it would be good to have some better numbers on state growth + the projected hardware requirements in case of gas limit increase. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.603066770186336
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/shamatar",
                    "index": "8",
                    "likes": "0",
                    "time": "25/08/2021-16:41:07",
                    "content": "norswap may be I should have made it explicit, but div vs mul example there is to demonstrate that block gas limit is not anywhere close to an indication of validation complexity as a lot of gas may be just \u201cwasted\u201d for something that e.g. doesn\u2019t take so much CPU cycles to execute. If one would really care about network health and if the storage is the most difficult problem then SSTORE pricing should have become dynamic consensus parameters like base fee in EIP1559, or may be even some deterministic function over statistics of non-empty storage slots in the state to avoid continuous repricing. The last time I\u2019ve checked for numbers the state (without archive) was quite well below 1TB and you do not need archive for mining (well, unless you mine then you do not strictly validate a network but just observe it\u2019s state), and mining has other capital requirements contributions in a similar price range. It\u2019s not cheap to have a machine that can efficiently follow the network, but it\u2019s PoW and not PoS yet and a reason to have such machine should be separated from network requirements. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "None",
                        "None",
                        "None"
                    ],
                    "Sentiment": 4.670454545454545
                }
            ]
        }
    ],
    "group_index": "1403"
}