{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/data-ring-topic-precompiles-crypto-primitives-for-data-verification/2685",
            "title": "Data Ring Topic: Precompiles / Crypto Primitives for Data Verification ",
            "index": 2685,
            "category": [
                "Working Groups",
                "Data Ring"
            ],
            "tags": [
                "NONE"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "1",
                    "likes": "1",
                    "time": "19/02/2019-22:11:47",
                    "content": "There was discussion at Denver about things the node could provide to help with data verification.  Verification of complex data relationships (e.g. pre-compiled contracts/ Zksnarks) Recursive Zksnarks to prove current block is descendant of its parent block Blinded attestations of transaction histories  Action Items: We need one! ",
                    "links": [
                        "https://ethereum-magicians.org/t/data-ring-problem-statement/2681/2",
                        "https://ethereum-magicians.org/t/eth-denver-ring-notes-discussion-2-14-19/2662/3"
                    ],
                    "GPT-summary": null,
                    "GPT-proposal-categories": null,
                    "GPT-discussion-categories": null,
                    "Sentiment": 4.708333333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "2",
                    "likes": "1",
                    "time": "19/02/2019-22:30:07",
                    "content": "My take: I\u2019m supportive of these ideas, but I think there may be a distinction to be made between data that is intended to be fed back into the system (see this topic) and data that is destined to be used in outside systems. Also, there may be a fruitful distinction between what I\u2019ll call \u2018close-to-the-head\u2019 data and \u2018historical, never-changing\u2019 data. I know that there is technically no such thing as \u2018never-changing\u2019 data given that the chain can fork arbitrarily back into infinite history (infinitesimal), so the first thing we could agree on is \u2018How old does the data need to be to never look back?\u2019 Once we\u2019ve made that sort of decision, then people can build systems that have to be aware of this (and handle it if needed). Also, I\u2019m convinced that once data becomes immutable, it\u2019s possible to derive any further data from it (as long as you carefully exclude any \u2018meta-data\u2019) in such a way that the derived data can be as mathematically secure as the original \u2018consented-to\u2019 data. In other words, the fact that address 0x...whatever mined block 10100 is true now and will be true for the rest of human history. We should take advantage of that. One of the reasons why I want to make this distinction is because the \u201cfeature\u201d of querying broadly across the entire chain\u2013including as deeply as one wishes into its history\u2013imposes the need for \u2018sorting\u2019 and \u2018mixing\u2019 of new and old data. An index of every miner (or every address for that matter) from every block sorted by address changes each time a new address is added to the index. This \u2018adding\u2019 and \u2018sorting\u2019 negates the benefit of publishing an historical index to a content addressable file system such as IPFS. Another way to accomplish the same thing (make a query across every address) possible would be to publish \u2018digests\u2019 or \u2018volumes\u2019 of  addresses per some grouping function (such as after a certain number of records) and publish those digests to IPFS. This would make the data undeniable to anyone who has the ability download IPFS and has access to the hash to the digests. It solves the \u2018rising price of data excludes all but the wealthy\u2019 problem. Another huge benefit of publishing frozen, immutable digests is that as different people query for different addresses, and the hashes of the digests is returned, and if end users then pinned those files locally, the index would naturally be distributed across the world. (in other words, we would be taking advantage of immutable data on a content addressable file system.) Furthermore, heavy users would tend to download and pin a larger number of digests (this is why per number of records is important as opposed to per time period). Smaller users (addresses with few transactions) would download (and share and store) a much smaller portion of the index. Over time, assuming people can trust the hashes for the digests, the total number of bytes flowing through the system would decrease as users no longer need to query for their own historical transactions (since its local and pinned). Plus, this makes an improvement on the issues discussed here. With an index of transactions for any given address users can query directly against their own locally-running node for a full and complete audit trail of all activity on that account (i.e. permissionless auditing). This gives an incentive for more people to run more nodes because the nodes become more useful. Sorry for the brain dump, but I\u2019m glad that crap is now outside my head. I\u2019d love to hear some feedback. ",
                    "links": [
                        "https://ethereum-magicians.org/t/data-ring-topic-provider-infrastructure-running-nodes/2683"
                    ],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.56146928468357
                }
            ]
        }
    ],
    "group_index": "703"
}