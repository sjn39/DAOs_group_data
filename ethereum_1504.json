{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://ethereum-magicians.org/t/eip-4444-bound-historical-data-in-execution-clients/7450",
            "title": "EIP-4444: Bound Historical Data in Execution Clients ",
            "index": 7450,
            "category": [
                "EIPs",
                "Core EIPs"
            ],
            "tags": [
                "state-expiry"
            ],
            "content": [
                {
                    "author_link": "https://ethereum-magicians.org/u/matt",
                    "index": "1",
                    "likes": "1",
                    "time": "10/11/2021-14:17:27",
                    "content": "Discussion thread for EIP-4444. ",
                    "links": [],
                    "GPT-summary": "The post is a discussion thread for EIP-4444, which is a proposal. The author is not explicitly asking for feedback, but the post is open for discussion, and people are asking questions about the proposal.",
                    "GPT-proposal-categories": [
                        "Smart contract updates",
                        "Interoperability and Scalability",
                        "None",
                        "None",
                        "None"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is asking for feedback",
                        "Author of proposal is explaining proposal",
                        "None"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/yoavw",
                    "index": "2",
                    "likes": "0",
                    "time": "10/11/2021-20:32:38",
                    "content": "A link to the EIP itself?  (I don\u2019t see it in the github repo) ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/matt",
                    "index": "3",
                    "likes": "0",
                    "time": "10/11/2021-21:24:05",
                    "content": "Was hoping to get it merged quickly and update with eips.ethereum.org link, but it didn\u2019t happen. I\u2019ve updated it just now with the PR link. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.666666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/mkalinin",
                    "index": "4",
                    "likes": "1",
                    "time": "11/11/2021-10:42:32",
                    "content": "It worth say when the specification of this EIP comes into effect. An important thing that this change depends on the Merge. This is because before the Merge historical block headers are essential for sync and bootstrapping process. Verifying the PoW seal of all blocks in the chain is the only way to prove the chain is valid. Network upgrade to PoS shifts this paradigm and makes historical data of EL chain not a requisite for node bootstrapping process. I don\u2019t think that this spec should be so prescriptive, i.e. saying MUST NOT wrt serving ancient data. The reason is that sync process of some clients may depend on the historic data and they will need a time to be prepared for such a big change. For instance, Erigon executes all blocks since genesis and doesn\u2019t used any state downloading techniques to get in sync with the network; they will probably want to serve ancient blocks even if other clients stop doing it. IMO, the purpose of this EIP in the context of the Merge is to send a clear signal to infra, users, and clients that they should consider that the invariant of storing the history is going to be broken in some future. If we don\u2019t want this for the Merge then it could be prescriptive but it worth considering that it will take a lot of time for network participants to get prepared for such a change. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.191666666666666
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/djrtwo",
                    "index": "5",
                    "likes": "0",
                    "time": "11/11/2021-14:53:38",
                    "content": "    mkalinin:  I don\u2019t think that this spec should be so prescriptive, i.e. saying MUST NOT wrt serving ancient data.   I disagree here. If we don\u2019t make this prescriptive then we will have a gradual degradation in historic block and receipt sync while users/clients still rely on it, until it just becomes broken and a bunch of users are confused and upset. Making this a MUST NOT serve makes this a harder EIP to implement because it will require preparing dapps (especially receipts) and users for this breaking change, but then it will be completed in a clean way. If instead it\u2019s a SHOULD NOT or MAY NOT, we create a path for dapps to slowly become broken on some indefinite timeline (because many will just continue to rely on the functionality for as long as it seems to work).     mkalinin:  For instance, Erigon executes all blocks since genesis and doesn\u2019t used any state downloading techniques to get in sync with the network; they will probably want to serve ancient blocks even if other clients stop doing it.   iiuc, Erigon has had a torrent block downloader in production for quite a while (it is way faster) and is expecting this breaking change at some point.     mkalinin:  IMO, the purpose of this EIP in the context of the Merge is to send a clear signal to infra, users, and clients that they should consider that the invariant of storing the history is going to be broken in some future   I agree with this, but to do so effectively, I think the strategy is to specify the EIP as it will be in it\u2019s final form and begin communicating about it now rather than actually introducing the breaking change simultaneously with the Merge. Imo, this is going to take 12+ months to properly communicate and execute on the dapp/community side, but this EIP can use the Merge and weak subjectivity in it\u2019s rationale to bound it to this shift to PoS even though it wont be fully implemented at the point of the shift. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.741228070175438
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "6",
                    "likes": "3",
                    "time": "12/11/2021-02:26:27",
                    "content": " Preserving the history of Ethereum is fundamental  Yes.  We suggest that a specialized \u201cfull sync\u201d client is built. The client is a shim that pieces together different releases of execution engines and can import historical blocks to validate the entire Ethereum chain from genesis and generate all other historical data.  You don\u2019t say who would build or maintain this client.  And it\u2019s not clear to me how the shimming would work.  Existing clients go to some effort to efficiently manage the differences between releases, given most of the code hasn\u2019t changed.  There are censorship/availability risks if there is a lack of incentives to keep historical data. \u2026 there is a risk that more dapps will rely on centralized services for acquiring historical data.  Yes.  And I don\u2019t think mitigating these risks is \u201cout-of-scope for this proposal\u201d. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.4125
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/matt",
                    "index": "7",
                    "likes": "0",
                    "time": "12/11/2021-04:55:09",
                    "content": "    gcolvin:  it\u2019s not clear to me how the shimming would work   Let\u2019s say there are upgrades X, Y, and Z. During X, clients support X. During Y, clients support Y and so on. After each fork the code to run the fork is removed (save for the transition period of the fork). So if you want to validate the entire chain, you would start the client version that supports X and import all the X blocks. Then it would shutdown and you would run the client version that supports Y with the same data directory and again import all the blocks. Proceed with this and eventually you\u2019ll be executing the tip of the chain with everything validated. The two caveats are i) after the merge, you\u2019ll also need to get the beacon history and drive clients with that and ii) you may need to run glue code between client versions if there are breaking changes to the state / history storage.     gcolvin:  And I don\u2019t think mitigating these risks is \u201cout-of-scope for this proposal\u201d.   I think there are pretty straightforward solutions to replicated storage of static data that we can tap into for this (e.g. IPFS, torrent, data mirrors, etc). \u2013 Copying from the PR thread:  @djrtwo: Note, that discussions with @karalabe have pointed to making this a MUST because otherwise users will still rely on it while quality of this feature degrades until it is unusable. A MUST, instead, forces users and dapps to actually \u201cupgrade\u201d how they utilize this at the point of this shipping, forcing everyone\u2019s hand so it doesn\u2019t silently get worse and worse.   @djrtwo: If we go that path, then this spec should specify that devp2p does return errors on requests outside of the specified range of epochs/time  I feel like SHOULD is adequate level of force for clients to do this? Geth continue to make up a large portion of the network and if they stop serving the data, it\u2019s going to mostly be unavailable (and users will mostly be forced to adapt). That said, I don\u2019t have a strong argument for one or the other. The one caveat is that most devp2p messages wouldn\u2019t know how to distinguish requests for \u201cnon-existent data\u201d and \u201cexpired data\u201d because they are by hash. Only GetBlockHeaders is done via block number and could therefore return an error. If we were to go with SHOULD and GetBlockHeaders returns an empty response instead of error, I think the main fail case will be clients that try to header sync the old way will be confused since they don\u2019t get any headers back. This seems acceptable and avoids an new wire protocol version. \u2013  @axic:  Actually back in April 2020 or so, within the Ipsilon team we thought about making a proposal to just hardcode given hashes for given blocks in an EIP. The idea would be to hardcode the hashes for past hard forks.   However then regenesis was proposed, which in practice does the same, but programatically.   Since regenesis as a concept is delayed ever so often, and given this EIP, would such a proposal make sense now?  In my mind, regenesis tackles a slightly different problem (and when I say regenesis, I generally mean this version). This EIP is about removing the need to store historical data whereas regenesis is a mechanism primarily aimed at reducing the state data. Regenesis, as far as I understand it, does not prescribe that clients should discontinue holding historical headers / bodies / etc. I am curious to understand better what you mean about hard coding past fork blocks. I think you\u2019re referring to a type of weak subjectivity checkpoint? \u2013 Generally a question that keeps coming up is how to deal with the difference between non-existent and expired data. Geth has set the precedence in v1.10.0 by turning the txlookup index to prune indexes older that 1 year by default. This means that if a user calls eth_getTransactionByHash with a valid tx hash from Byzantium, it will return an empty response. Is the going to be acceptable behavior other data like blocks? And is acceptable over the wire? Seems like we\u2019re leaning towards \u201cyes\u201d. ",
                    "links": [
                        "https://notes.ethereum.org/@vbuterin/verkle_and_state_expiry_proposal"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.004501703969789
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "8",
                    "likes": "0",
                    "time": "12/11/2021-12:07:48",
                    "content": "    djrtwo:  I disagree here. If we don\u2019t make this prescriptive then we will have a gradual degradation in historic block and receipt sync while users/clients still rely on it, until it just becomes broken and a bunch of users are confused and upset. Making this a MUST NOT serve makes this a harder EIP to implement because it will require preparing dapps (especially receipts) and users for this breaking change, but then it will be completed in a clean way. If instead it\u2019s a SHOULD NOT or MAY NOT, we create a path for dapps to slowly become broken on some indefinite timeline (because many will just continue to rely on the functionality for as long as it seems to work).   My understanding of RFC 2119 is that what you are describing is exactly what SHOULD is used for.  SHOULD is a way for a specification to say \u201cthis behavior is what is best for the ecosystem/user, but it isn\u2019t something that is strictly enforced and if you don\u2019t follow it nothing is going to outright break\u201d. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.194444444444445
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "9",
                    "likes": "0",
                    "time": "12/11/2021-22:25:43",
                    "content": "    matt:  So if you want to validate the entire chain, you would start the client version that supports X and import all the X blocks. Then it would shutdown and you would run the client version that supports Y with the same data directory and again import all the blocks. Proceed with this and eventually you\u2019ll be executing the tip of the chain with everything validated   That\u2019s what I feared  It means all of these clients have to be maintained indefinitely. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "10",
                    "likes": "1",
                    "time": "12/11/2021-22:45:50",
                    "content": "    matt:      gcolvin:  And I don\u2019t think mitigating these risks is \u201cout-of-scope for this proposal\u201d.   I think there are pretty straightforward solutions to replicated storage of static data that we can tap into for this (e.g. IPFS, torrent, data mirrors, etc).   Then I\u2019d like a bit more discussion of what the solutions and who is responsible for fixing what is going to get broken. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 6.1875
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "11",
                    "likes": "0",
                    "time": "13/11/2021-11:37:29",
                    "content": "A. This is only necessary if one wants to validate the entire blockchain from genesis, which I argue is an uncommon operation at best, and I suspect eventually will simply be something that no one does. B. The old clients don\u2019t have to be maintained, they only need to continue to exist.  No updates need to be applied to them. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 6.166666666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/jpitts",
                    "index": "12",
                    "likes": "2",
                    "time": "15/11/2021-03:08:46",
                    "content": " Preserving the history of Ethereum is fundamental and we believe there are various out-of-band ways to achieve this.  It should be stated what preserving the history of Ethereum is fundamental to, and how important state history preservation is relative to other properties of the protocol. And if it is as fundamental as stated, why do the authors not propose an alternative, sustainable mechanism for it? Adding this: Not that the current situation is indefinitely sustainable, but the current requirement sufficiently preserves and provides state history. The burden put on network users is heavy and growing, but there needs a realistic plan for how to maintain this widely-used aspect of the Ethereum network. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.151041666666667
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "13",
                    "likes": "2",
                    "time": "15/11/2021-05:49:47",
                    "content": "Yes.  The Ethereum blockchain is, fundamentally, an immutable record of transactions \u2013 value transfers and  valid computations. It seems to me that there should be some standard protocol for that, however the history is stored. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "None",
                        "None"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/mkalinin",
                    "index": "14",
                    "likes": "0",
                    "time": "15/11/2021-09:37:33",
                    "content": "    djrtwo:  Making this a MUST NOT serve makes this a harder EIP to implement because it will require preparing dapps (especially receipts) and users for this breaking change   Additionally, if we use MUST NOT then the peer that breaks this requirement SHOULD be disconnected and penalised. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.75
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/tjayrush",
                    "index": "15",
                    "likes": "0",
                    "time": "24/11/2021-17:27:03",
                    "content": "I agree completely that \u201cpreserving this widely-used aspect of the Ethereum network\u201d is of utmost importance. But why does it have to be a capability that the node maintains? It seems to me that maintaining this ability in the node is exactly the problem. If the data is immutable, the entire state and the entire history of the chain can be written once to some content-addressable store (such as IPFS) and as long as someone preserves that data, anyone can get it. AN Ethereum node would not even have to be involved. All one would need is the hash of where the immutable data is stored. Fresh data can be written as an \u2018addendum\u2019, so there would have to be some sort of published manifest of the original hash and the periodic ongoing hashes. I would argue that the hash of the manifest should be part of the chain, but, short of that, the community would have to maintain it (perhaps by publishing the hash to a smart contract). My point is that because the data is immutable, and because we have content-address storage to store it in, there\u2019s literally no need to continue to provide the ability to regenerate this data from genesis. The only outcome of regenerating from genesis would be to arrive at the same IPFS hash as you already have. On top of that, there\u2019s no reason the clients have to maintain this capability, and the entire purpose of this EIP is to remove that requirement. This might possibly open a whole new area of innovation related to providing access to this historical data \u2013 which I think would allow for amazingly more interesting dApps than we currently have (because of the need for a node to get to it). Furthermore, if the historical data is chunked into manageable pieces, and it was properly indexed by chunk (with a bloom filter in front of each chunk) each individual user could easily download and pin only that portion of the database that they are interested in. Thereby distributing this historical data throughout the community as a natural by-product of using it. (See TrueBlocks). ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.634830447330447
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/anett",
                    "index": "16",
                    "likes": "2",
                    "time": "01/12/2021-12:33:50",
                    "content": "I agree that people are uploading lot of stuff on blockchain especially with the rise of NFTs but also not optimized token contracts which are causing state bloat. But did anyone think about other examples and use cases of blockchain? What if people uploaded their important documents like birth certificates on blockchain as the mission of blockchain is ledger which stores information on-chain forever. Suddenly those people won\u2019t be able to access their documents because some devs thought it\u2019s a good idea to delete blockchain state after some time\u2026 Another great example is NFTs especially NFTs that were made before ERC-721 ie 2017 and older like CryptoPunks. Those will be gone for ever. From developer perspective, I\u2019m sure that there are many data that are not important and doesn\u2019t need to be stored. Probably better idea would be to store data on full nodes and have light nodes or think about different ideas how to make infrastructure the most efficient without having to delete and loose data. Don\u2019t get me wrong, I\u2019m just trying to think realistically from non-core-dev perspective and I\u2019m against this EIP. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 6.1617746288798925
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/MicahZoltu",
                    "index": "17",
                    "likes": "3",
                    "time": "02/12/2021-04:34:16",
                    "content": "Ethereum was never designed to be a permanent data storage system.  Something like FileCoin is much better suited for long term data storage, and they have incentives built into the protocol to ensure that the cost of long term storage is paid for by those seeking it. Also, this EIP removes history but not state.  State expiry is also an active area of research, but out of scope for this thread. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.333333333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/niceblue",
                    "index": "18",
                    "likes": "3",
                    "time": "07/12/2021-18:37:42",
                    "content": "I have been an Ethereum watcher and dapp developer for years, and have admired all the EIPs that have come through. However, this EIP is deeply troubling. I think this would be an extremely negative EIP to implement. Here\u2019s why: Ethereum was touted as the system to build \u201cunstoppable\u201d apps over the years, I loved it. With this EIP, these \u201cunstoppable\u201d apps will simply, well, stop (at least, their UXs/UIs will). It forces substantial and necessary adoption of some a.n.other (unknown and uncertain) protocol entirely outside of the Ethereum system. Pulling the \u201cpromise\u201d of data persistence on old apps will be disastrous for the long term reputation of dapp development on Ethereum. For those who say Ethereum was never meant to store data permanently, that is simply not true. It was! It\u2019s specced that way and therefore used that way. And with this EIP, it will no longer. This is a truly fundamental change to (and destruction of) the Ethereum value proposition. Providing canonical transaction history tightly coupled with canonical transaction generation is CRITICAL to Ethereum\u2019s value proposition. Offloading this entirely outside of Etheruem\u2019s control gives away (and destroys) the future utility of Ether. Why? Because the whole point of canonical transaction generation is that you also have canonical transaction history. With EIP 4444, it is possible to lose entire chunks of past transaction history. Forever. As in gone. No one knows who sent (or did) what to who or when. There\u2019s a reason why JP Morgan, HSBC, etc, any of these long storied banks are still around. It\u2019s because you can rely on them having, somewhere inside their big walled offices, a transaction history going back over 100 years. This builds TRUST (yes, centralized trust). But that\u2019s why (other) people come back to them (even if blockchain ppl don\u2019t). The old history may be hand written in log books, sure not convenient, but it\u2019s there. Now imagine Ethereum was just such an organisation. You go to Ethereum in 12 years time and you ask (in code), what transaction happened on this account 10 years ago? The reply: oh go to Graph/Bittorrent/IPFS/a.n. other, we don\u2019t keep that. You try numerous of these organisations, by some stroke of bad luck, they messed up on your particular EOA/contract (or their tech dies), and it\u2019s gone. Would you trust the Ethereum system, simply because it moved to cool stateless consensus, and therefore decided they didn\u2019t need to include anything as boring as past history anymore? I wouldn\u2019t. What this EIP fails to realise is that the value of canonical decentralised transactions in almost all real world use cases isn\u2019t canonical decentralised  transactions, but canonical history of decentralised transactions. The blockchain that does this will win. Ethereum does both today, but with this EIP, it won\u2019t any more. That\u2019s what I would call the broken promise of Ethereum if this EIP happens, and without a simultaneous EIP that ensures canonical transaction history at the Ethereum protocol level. Perhaps some compromise can be made on time horizon. \u201cForever\u201d is not enforceable, but say 20 years (for example) is, and good enough for most real world use cases (but even then not good enough for academic records, for example). Remember, the well known banks most of us also use can keep (centralised) canonical records for over a century, and universities can keep records for multiple centuries. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving entirely negative feedback on proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 4.991934523809523
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Lilaaffe",
                    "index": "19",
                    "likes": "0",
                    "time": "07/12/2021-21:15:14",
                    "content": "Arweave fixes your problem. Period. ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/niceblue",
                    "index": "20",
                    "likes": "0",
                    "time": "07/12/2021-21:22:03",
                    "content": "It is a.n.other protocol. canonical transaction history is absolutely critical to Ethereum. Why entrust that outside? If Arweave figures out Smartweave properly, let\u2019s see where Ether ends up. More power to Arweave  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.625
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/aronchick",
                    "index": "21",
                    "likes": "1",
                    "time": "10/02/2022-20:24:16",
                    "content": "Hi! I co-lead ResDev for Protocol Labs, we\u2019re more than happy to help make available prove-ably immutable history of ETH forever on Filecoin & IPFS, and would not require any protocol changes. Please don\u2019t hesitate to let me know if anyone would like help doing this! Thanks! ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party offering collaboration on proposal",
                        "3rd party advertising proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 7.5625
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/poojaranjan",
                    "index": "22",
                    "likes": "0",
                    "time": "21/03/2022-14:12:58",
                    "content": "PEEPanEIP-4444: Bound Historical Data in Execution Clients with @ralexstokes \ufe0f     ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/Neurone",
                    "index": "23",
                    "likes": "2",
                    "time": "08/06/2022-02:33:20",
                    "content": "My two cents. I disagree with this EIP, both in general and about the contents. It doesn\u2019t add any real advantage to clients, I can already prune txs if I\u2019d like to. It removes a feature without offering any solution; everything is \u201cout-of-scope\u201d or \u201cjust use other p2p network for that\u201d. We can already use centralized servers or CANs (IPFS, Swarm, etc.) to store data, even Ethereum related (i.e., TrueBlocks indexes), but having that mandatory is something that makes no sense for me. Ethereum is a p2p network, using the network to share data and reach consensus should be what it\u2019s used for. And all the Ethereum ecosystem should be consistent without relying on others p2p networks. Furthermore, storage is a commodity by now, and it will be even more in the future. Four hundred GB (and counting) of old data is really nothing to fear of, I can buy an HDD to store 4 TB of data for $30, and it will last for another 20 years of historical txs. Storage technologies grow and improve really faster than Ethereum\u2019s txs. In my opinion, the right path is continuing to improve light client, spreading more nodes among constrained devices and hardware. The wrong path is transforming forcibly all full nodes into light clients. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.87124060150376
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ytrezq",
                    "index": "24",
                    "likes": "0",
                    "time": "03/09/2022-23:29:40",
                    "content": "Hi, your are saying this, but TheGraph just censored Tornado Cash. The underlying problem is things like databased shared over ipfs or Google Bigquery wouldn\u2019t allows us to have a realtime service as it would take a day to download updates. We also need very old data in order to let a user withdraw funds deposited several years ago. This isn\u2019t that much a problem currently as this just mean we will need our own home (not cloud) hosted node with parity_tracing, along OpenEthereum Fatdb like for getting smart contract storage range at past blocks. Such thing is possible because full anciant data is broadcasted and updated in real time but show why it is important to have node/rpc be able to broadcast full chain history over the p2p network. If third party service are made required, then well, Ethereum, will be decentralized at the currency level like Bitcoin while Dapps like casinos or yield farming, will be fully permissionned have to register with authorities by paying an army of lawayers in order to be allowed to run which means, DeFi won\u2019t be that much different than Fintechs on the tradionnal banking system which rent their computing hardware. I think keeping history on the p2p definitely worth the reduced transactions per seconds outputs or if we decide to behave like swift or MasterCard or Visa for being able to run as fast as them. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.845446428571428
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ytrezq",
                    "index": "25",
                    "likes": "0",
                    "time": "03/09/2022-23:34:33",
                    "content": "No, their point is large database reduce the transaction per second speed. But I think this is far fetched as Visa and Mastercard are in my country required to record data of all transactions of the past 10 years for law enforcement, and this doesn\u2019t prevent them to run. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.080357142857142
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ytrezq",
                    "index": "26",
                    "likes": "0",
                    "time": "03/09/2022-23:41:51",
                    "content": "Would this be in real time for each blocks like the current p2p network, or would there be daily updates pushes ? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.333333333333333
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ytrezq",
                    "index": "27",
                    "likes": "0",
                    "time": "03/09/2022-23:43:31",
                    "content": "Hi, your are saying this, but TheGraph just censored Tornado Cash. The underlying problem is things like databased shared over ipfs or Google Bigquery wouldn\u2019t allows us to have a realtime service as it would take a day to download updates. We also need very old data in order to let a user withdraw funds deposited several years ago. This isn\u2019t that much a problem currently as this just mean we will need our own home (not cloud) hosted node with parity_tracing, along OpenEthereum Fatdb like for getting smart contract storage range at past blocks. Such thing is possible because full anciant data is broadcasted and updated in real time but show why it is important to have node/rpc be able to broadcast full chain history over the p2p network. If third party service are made required, then well, Ethereum, will be decentralized at the currency level like Bitcoin while Dapps like casinos or yield farming, will be fully permissionned have to register with authorities by paying an army of lawayers in order to be allowed to run which means, DeFi won\u2019t be that much different than Fintechs on the tradionnal banking system which rent their computing hardware. I think keeping history on the p2p definitely worth the reduced transactions per seconds outputs or if we decide to behave like swift or MasterCard or Visa for being able to run as fast as them. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.845446428571428
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/ytrezq",
                    "index": "28",
                    "likes": "1",
                    "time": "03/09/2022-23:46:19",
                    "content": "I propose something which is done by the Cloud industry which bill money to keep data. Deposit Ethers on smart contracts : at each blocks, a very tiny fee is removed. When the smart contract values drops to 0, it\u2019s code/storage is SUICIDED and it\u2019s relevant transactions deleted from history. That way : what is needed is kept while what is forgotten is destroyed. This is also means more efficient than the proposal since stuff can be destroyed before 1 year. Please also notice that destroying what is unused is also how the human memory works and things always fit in the size of a human skull. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.84920634920635
                },
                {
                    "author_link": "https://ethereum-magicians.org/u/gcolvin",
                    "index": "29",
                    "likes": "0",
                    "time": "04/09/2022-03:54:28",
                    "content": " ytrezq Deposit Ethers on smart contracts : at each blocks, a very tiny fee is removed  Or maybe, stake the ether and use the proceeds to pay for the data storage in perpetuity. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal"
                    ],
                    "Sentiment": 5.5357142857142865
                }
            ]
        }
    ],
    "group_index": "1504"
}