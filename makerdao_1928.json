{
    "poll_list": [],
    "discourse_list": [
        {
            "thread_link": "https://forum.makerdao.com/t/mip55c3-sp11-pioneer-defi-focused-language-dataset-for-the-benefit-of-risk-modelling-governance-communication/18802",
            "title": "MIP55c3-SP11: Pioneer DeFi-focused Language Dataset for the Benefit of Risk Modelling & Governance Communication ",
            "index": 18802,
            "category": [
                "Maker Improvement Proposals",
                "Archive"
            ],
            "tags": [
                "spf",
                "impact-:-medium"
            ],
            "content": [
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#1",
                    "likes": "1",
                    "time": "15/11/2022-14:16:10",
                    "content": " MIP55c3-SP11: Pioneer DeFi-focused Language Dataset for the Benefit of Risk Modelling & Governance Communication  Preamble MIP55c3-SP#: 11 Author(s): @pvl Contributors: Tags: spf Status: Rejected Date Proposed: 2022-11-15 Date Ratified: N/A Amount Requested: 50,000 DAI Recipient Address: 0x6ebB1A9031177208A4CA50164206BF2Fa5ff7416 Forum URL: https://forum.makerdao.com/t/mip55c3-sp11-pioneer-defi-focused-language-dataset-for-the-benefit-of-risk-modelling-govcomms/18802 Ratification Poll URL: https://vote.makerdao.com/polling/QmYsosW6   Sentence Summary Develop a DeFi-focused dataset for the purpose of fine-tuning language models, which, if integrated, will result in millions of dollars of additional value from more accurate Risk estimates as well as improving governance communications.  Paragraph Summary Build the industry\u2019s first DeFi-focused dataset. The data will be used to tune a Large Language Model (BLOOM). A fine-tuned model integrated into Maker\u2019s pipelines will help gradually relax current Risk CU modeling assumptions, thereby delivering millions of dollars of missed value for the DAO & Maker users. A fine-tuned model may also provide opportunities to expand and improve on governance communications. Furthermore, a DeFi-focused dataset offers extensibility across other DAOs.  Motivation This SPF will fund the creation of industry first DeFi-focused dataset, which will be used to fine-tune general purpose Large Language Models. Such model will be used for the benefit of Maker\u2019s Risk Modelling and governance communications. I. The new model will augment and extend current Risk CU\u2019s Model. The goal is to turn Risk CU Model\u2019s conservative CONSTs into variables. (see appendix for details) Risk premium estimates could be reduced by relaxing the current conservative CONSTs when replacing input values with values learnt from real life data =>  Lower stability fees => better terms for protocol users => growth opportunities => more revenue from fees; Higher debt ceiling => higher capital efficiency => more revenue; Lower surplus buffer => higher capital efficiency => less DAI frozen, more DAI for DAO purposes.  For instance, if ETH-A Jump Severity goes from 50% to 45% and Jump Frequency from 2 to 1 per year:   Risk Premium at current Debt Exposure goes 0.5% \u2192 0.2%;  Maximum Debt Ceiling (Risk Premium = 10%) goes $410M \u2192 $570M;  Capital at Risk goes $958,217 \u2192 $383,287.  II. A Large Language Model fine-tuned on this dataset for DeFi sentiment analysis could also be integrated into the governance communication pipeline for the purposes of improved topic identification and research (more on the need covered here) as well as social sentiment analysis. III. As the industry\u2019s first DeFi-focused language model, it opens up a number of collaboration opportunities with other DAOs and DeFi protocols, who can benefit from better identifying risk, improving marketing techniques, and preserving brand reputation.  This project is a continuation of the work on web3 native intelligence. In the previous proof-of-concept step I showed that the UST stablecoin crash in May could have been predicted by significant downward movement in UST sentiment beginning mid April, which resulted in the gradual loss of confidence => panic => bank run => death spiral supported by the mechanics of the protocol. However, DeFi conversation on Twitter/Discord/Discourse is full of domain-specific slang, nuances, deep context. A general purpose model must natively understand these details to output the most adequate results. Moreover, the SemEval-2017-4A dataset, used for fine-tuning on the previous step, contains only 50k tweets. A bigger and more native fine-tuning dataset will improve accuracy of our model for sentiment detection and summarisation tasks.  Special Purpose Fund Name Pioneer DeFi-focused language dataset for the benefit of Risk modelling & governance communication  Special Purpose Fund Scope & Work Credentials This SPF will fund the next step of the web3 native intelligence program as follows:  Assemble the first DeFi-focused dataset for language model pretraining (3M tweets). Use BERTweet_large (355M parameters) vs BERTweet_base (135M parameters) and further pretrain it on the DeFi corpus from step 1. Build integration between LabelStudio and Dynabench platform via Mephisto library. Run around 20 small dynamic adversarial data collection rounds.   Funding Breakdown Fund the steps from the previous paragraph as follows:  6,000 DAI 6,000 DAI 6,000 DAI 8,000 DAI per each of 5 round slots out of 20 DADC rounds: 4.1 8000 DAI after round 5; 4.2 8000 DAI after round 10; 4.3 8000 DAI after round 15; 4.4 8000 DAI after round 20.   Special Purpose Fund Details Official Contact or Group Name: @pvl Contact Email/Handle: @pvl Date Added: (yyyy-mm-dd) Total Amount: 50,000 DAI Wallet Address: Public wallet address of Multi-Sig. Comptroller signers: @Patrick_J, @Retro ",
                    "links": [
                        "https://forum.makerdao.com/t/towards-web3-native-intelligence-tools-for-protocol-comprehension-and-stewardship/15642",
                        "https://labelstud.io",
                        "https://dynabench.org/",
                        "https://mephisto.ai",
                        "https://forum.makerdao.com/t/llama-delegate-platform/15191/34",
                        "https://forum.makerdao.com/t/weekly-mips-update-114/18875",
                        "https://forum.makerdao.com/t/weekly-mips-update-115/18956",
                        "https://forum.makerdao.com/t/weekly-mips-update-116/19059",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123/8",
                        "https://forum.makerdao.com/t/frontier-research-delegate-platform/17298/34",
                        "https://forum.makerdao.com/t/mhonkasalo-teemulau-delegate-platform/15473/46",
                        "https://forum.makerdao.com/t/feedblack-loops-llc-delegate-platform-fbl/9366/116",
                        "https://forum.makerdao.com/t/consensys-delegate-platform/18246/11",
                        "https://forum.makerdao.com/t/codeknight-delegate-platform/16806/39",
                        "https://forum.makerdao.com/t/london-business-school-delegate-platform/15203/34",
                        "https://forum.makerdao.com/t/schuppi-delegate-platform/11193/144"
                    ],
                    "GPT-summary": "The post proposes the creation of a DeFi-focused dataset to fine-tune language models for the purpose of improving risk modeling and governance communications. The proposal aims to augment and extend current Risk CU's Model by turning conservative CONSTs into variables, which could result in lower stability fees, higher debt ceiling, and higher capital efficiency. The post also highlights the potential collaboration opportunities with other DAOs and DeFi protocols. The proposal includes a funding breakdown for the next steps of the web3 native intelligence program.",
                    "GPT-proposal-categories": [
                        "Grants, Funding and resource allocation",
                        "Privacy, Security and risk management",
                        "Token economics",
                        "Smart contract updates",
                        "None"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party or author is advertising proposal"
                    ],
                    "Sentiment": 5.599631300902487
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#2",
                    "likes": "1",
                    "time": "15/11/2022-14:17:32",
                    "content": "Technical appendix Risk Premium estimate is at the center of Maker Risk CU\u2019s Collateral Risk Model. Currently Model\u2019s inputs \u201cJump Frequency\u201d and \u201cJump Severity\u201d estimates for a given period are conservative CONSTs \u2014 twice per year 45% for WBTC, 50% for ETH, and 60% for all other volatile assets. Model\u2019s output, in turn, Risk Premium (+ some other estimates) is used to propose key Maker\u2019s parameters: vault\u2019s Stability Fee, Debt Ceiling, System Surplus Buffer (via protocol-wide Capital at Risk metric). The goal of the project: offer a tool to learn model\u2019s inputs \u201cJump Severity\u201d and \u201cJump Frequency\u201d estimates for a given period IRL from crypto community sentiment towards collateral types as variables. Since current CONSTs are by design conservative, input values learnt from real life data will generally allow to lower Risk Premium estimates =>  Lower stability fees => better terms for protocol users => growth opportunities => more revenue from fees; Higher debt ceiling => higher capital efficiency => more revenue; Lower surplus buffer => higher capital efficiency => less DAI frozen, more DAI for DAO purposes.  For instance, if ETH-A Jump Severity goes from 50% to 45% and Jump Frequency from 2 to 1 per year:  Risk Premium at current Debt Exposure goes 0.5% \u2192 0.2%; Maximum Debt Ceiling (Risk Premium = 10%) goes from $410M to $570M; Capital at Risk goes $958,217 => $383,287.  At the first stage Sentiment dynamics will be wrapped into the dashboard as a tool for Computer-Aided Governance: a qualitative factor to be considered, allowing to relax assumptions when proposing protocol parameters for the MKR vote. Then it will gradually be integrated as an input into the quantitive Collateral Risk Model. One approach: there are two components: EMH and non-EMH. EMH is modelled as a GBM. It\u2019s augmented by a non-EMH (tail volatility) component modelled with a Sentiment Model. Once enough time-series data is accrued a regression model could be learnt with sentiment + vault activity as input and (price volatility => liquidation) Risk Premium prediction as output. Finally, a sentiment + vault activity model could be augmented with a Maker Vault Liquidation ML Model into a singe cause => effect model. The eventual goal could be for protocol parameters becoming fully autonomously derived from onchain and crypto community/offchain data. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.778985507246377
                },
                {
                    "author_link": "https://forum.makerdao.com/u/CodeKnight",
                    "index": "#3",
                    "likes": "1",
                    "time": "15/11/2022-14:31:37",
                    "content": "  So if I understand, you want to create a program that combs social media and adjusts risk parameters based on sentiment?   What do @Risk-Core-Unit and @gov-comms-core-unit think about using these tools?   ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party asking questions about proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.166666666666667
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Davidutro",
                    "index": "#4",
                    "likes": "0",
                    "time": "15/11/2022-15:29:27",
                    "content": "I\u2019m not an expert in this stuff by any means. When @pvl shared his idea, I encouraged him to bring it to the DAO for discussion. Here are a couple videos I found useful for understanding large language models(LLMs) and their applications.          LLMs seem to have some potential. For what it\u2019s worth, I think it\u2019s an interesting proposal that might be foundational for useful stuff down the road. Though, I am curious about the potential applications. Is it worth a 50k exploratory investment? That\u2019s not for me to decide. I don\u2019t know the relative costs of this sort of thing, so I am curious about what others have to say.  @pvl  Would the dataset and integration work be done under an open-source software license? What happens after this SPF?  ",
                    "links": [
                        "https://www.youtube.com/watch?v=xSGX8gBQDO8"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.6494708994708995
                },
                {
                    "author_link": "https://forum.makerdao.com/u/fig",
                    "index": "#5",
                    "likes": "3",
                    "time": "15/11/2022-16:17:07",
                    "content": "I will mirror @CodeKnight\u2019s inquiry above - I\u2019d love to understand the demand and need from the Risk CU and GovComms before implementation. While it may be interesting, the data set requires experience and confidence from these teams for it to be valuable - or a super streamlined tool. @pvl what qualifications make you a good fit for the Work Credentials? It seems to be a sentiment analysis that other teams are already building: https://www.moonpass.ai/ - this one goes beyond tweets https://twitter.com/oz_dao - this one is for Twitter only What makes your proposed solution better than these solutions above, one of which is free? ",
                    "links": [
                        "https://twitter.com/oz_dao",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.458333333333332
                },
                {
                    "author_link": "https://forum.makerdao.com/u/josolnik",
                    "index": "#6",
                    "likes": "7",
                    "time": "15/11/2022-17:52:52",
                    "content": "The key question that needs to be answered before considering this proposal from the risk side is whether (and if so, to what extent) the community wants to move in the direction of optimizing capital efficiency. Maker has managed to build its reputation around robustness which ultimately boils down to its smart contracts combined with risk monitoring and mitigation. This is especially clear in times like these when people are much less likely to withdraw their funds from Maker compared to the whole ecosystem. There will always be a protocol that offers higher capital efficiency at the expense of higher risk. As we\u2019ve seen multiple times, that usually doesn\u2019t play out in their favor during times of major downfalls. On the other hand, we are leaving on the table both additional DAI supply and income if we\u2019re overly cautious with setting risk parameters. OP provided the latter argument well. Regarding this specific proposal. Our view is that recent volatility doesn\u2019t change the current potential severity of a market shock. This is why we\u2019re not using GBM in our models which OP mentioned again that we do and/or that we aim to move into that direction. Additionally, as I mentioned to you personally @pvl, we are skeptical about sentiment analysis\u2019s capacity to provide substantial information edge on the tail risk volatility. If you find some empirical research where that\u2019s been validated in a meaningful way, please do share. Besides that, risk of bad debt at Maker mostly comes from the (in-)ability of large vault owners to protect themselves, not the current sentiment in the market. Assuming that this kind of modeling approach helps in providing an information edge (which would need to be rigorously validated after the model is built) and we decide to optimize capital efficiency, it adds an additional strong assumption to the question that we\u2019re trying to answer when modeling financial tail risk. Our current approach answers the question of \u201cwhat is the potential protocol loss given the kind of market shock we want to be protected against?\u201d. This proposal adds an assumption that our capturing of market sentiment is a strong enough signal to increase Maker\u2019s financial risk. That being said, my personal stance is that there is strong potential for using language models on different tasks across DAOs and the risk domain could be one of those applications. But I would argue that for NLP tasks, working towards further increasing governance engagement across different channels could have even more potential than the risk domain. Bringing together domain experts from @gov-comms-core-unit with @pvl\u2019s technical expertise can be a powerful combo. Finally, the OP has studied our methodology well and managed to provide a proposal that can be integrated into our existing risk model. Additionally, it doesn\u2019t need much of our active involvement (can be built in parallel and potentially integrated later). ",
                    "links": [
                        "https://forum.makerdao.com/t/risk-core-unit-month-in-review-november-2022/19046",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party extending to proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.394492210464432
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Gala",
                    "index": "#7",
                    "likes": "0",
                    "time": "15/11/2022-19:31:18",
                    "content": "This is a summary of the contents of this subproposal and its impact, from GovAlpha\u2019s perspective:  This is a valid subproposal to add a SPF, as defined in MIP55: Special Purpose Fund. This SPF aims to fund the creation of a DeFi-focused dataset, which will be used to fine-tune general purpose Large Language Models. The author suggests that such model could be eventually used for the benefit of Maker\u2019s Risk Modelling and Governance Communications. The specific improvement the author hopes for is around \u201csocial sentiment analysis\u201d and \u201cimproved topic identification\u201d. This proposal asks for 50,000 DAI. As per MIP55c3, it is eligible to enter the Weekly Governance Cycle after having fulfilled its two-week Feedback Period (from November 15 onwards) and its one-week Frozen Period. If this subproposal is ratified, 50,000 DAI will be transferred to the specified MIP47-compliant multisig wallet through the earliest executive vote. This proposal has been assigned Medium-Impact based on the methodology listed here.  Disclaimer: These summaries are intended to be a guide and not a source of truth. You remain responsible for any actions you take on the grounds of this information. Please note the date when the summary was last updated to judge accuracy. Updated 2022-11-29 ",
                    "links": [
                        "https://mips.makerdao.com/mips/details/MIP55#MIP55c3",
                        "https://mips.makerdao.com/mips/details/MIP47",
                        "https://forum.makerdao.com/t/introducing-impact-estimations/14267"
                    ],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party auditing and reviewing proposal",
                        "3rd party asking questions about proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.586734693877551
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Gala",
                    "index": "#8",
                    "likes": "0",
                    "time": "15/11/2022-19:34:00",
                    "content": "@Patrick_J please confirm you agree on being a comptroller signer @pvl a second comptroller signer is pending. Please let us know when you appoint one. Thank you ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "None of the topics listed match."
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#9",
                    "likes": "0",
                    "time": "16/11/2022-08:17:46",
                    "content": "@josolnik, with all due respect, this proposal is not increasing risk. Risk-wise this SPF works on increasing risk estimate accuracy. Specifically, it works on gradually replacing rather rough Model input constants with input variables learned from real-time data. Current risk model assumptions are somewhat dubious. Model\u2019s stress-test scenarios assume market risk on the level of the biggest price drop recorded for the respective collateral type (e.g. ETH). On the one hand, past performance is not indicative of future results. If we\u2019re building the most robust lending platform possible, we can\u2019t assume that in the future there won\u2019t be scenarios worse than in the past, that there won\u2019t be price drops bigger than in the past. That\u2019s just not justified by history. But on the other hand, getting prepared for the worst past price shock we significantly tighten parameter nuts limiting growth opportunities, DAO revenue and value for the users. Paradoxically, we\u2019re significantly limiting ourselves without gaining any guaranteed stability. It\u2019s not about capturing tail risks. It\u2019s not about whether currently assumed risk level is good or bad. It\u2019s about accuracy of risk estimates. Adequacy of adopted risk modelling methodology. In other words, while we can agree that the worst price shock recorded CONST is a good v1 rough estimate, the key question: Why does Maker assume that getting prepared for the past collateral price drop is a justifiably good risk management strategy for the future? Any risk modelling at Maker results in parameter proposals for MKR holders to decide on parameter values. Additional information on both rational and irrational (sentiment) factors contributing to real, not hypothetical, market risks learned from real-time data, might actually give MKR holders food for thought on the question: in the present environment how adequate is it for Maker to be constantly prepared exactly for the collateral price drop on the worst level recorded? The reality might well diverge from this historic precedent in both directions: real risks might be larger or smaller. This proposal brings flexibility to the key risk model assumption, provides more data to bring risk estimates to real life. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 4.87594696969697
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#10",
                    "likes": "0",
                    "time": "21/11/2022-07:19:47",
                    "content": "ELI5: General purpose large language models, LLM, are the biggest breakthrough of recent years  in natural language processing. They vary in size and architecture, hence accuracy, from BERT_base (110M parameters) to GPT-3 (175B) and BLOOM (176B). Usually an LLM is trained on a large corpus of texts from all sorts of available digitised sources. It is assumed that a trained LLM is an approximate representation of the (English) language. Then an LLM can be fine-tuned to perform a specific task. E.g.: you can fine-tune your LLM to recognise sentiment of a tweet by showing it a bunch (50-150k) of tweets with human-labelled sentiment. There are various ways to improve LLM accuracy in a particular task.  Normally (but not always) the more advanced (bigger) the model is the better. You better use BLOOM or GPT-3 than BERT_base. Training a state-of-the-art LLM may cost several mln $ and only big tech (GPT-3) or a special consortium (BLOOM) can do this for now. (maybe DAOs will join the ranks?) You can tune your LLM on a domain specific corpus, e.g. tweets. Twitter threads, not part of the original training corpus, significantly differ from a Wikipedia article or a novel and you want your language model to be able to work with this special case of (English) language usage. The better your dataset for LLM fine-tuning is, the higher LLM\u2019s accuracy will be. You could use publicly available datasets, like the SemEval-2017 Task 4 for Twitter sentiment analysis. However, DeFi chat contains a lot of nuances: slang (degen, rug pull etc), specific context, details. If you use a labelled dataset containing DeFi community chats/tweets to fine-tune your LLM, it will further improve its accuracy. Finally, you can fine-tune your model dynamically using adversarial data:   instruct human data labellers to find (or devise) such examples (e.g. for the sentiment recognition task), which would fool the model \u2014 it would recognise a tweet as positive with human consensus being that it\u2019s negative; assemble a (relatively small) dataset of such hard-for-the-model examples, label them correctly and fine-tune the model on it; repeat for, say, 20 rounds. (*note that each round the model will be better in recognising sentiment => it will be harder for human labellers to fool it, hence they\u2019ll need to play with it to find its weak points. But if they manage to fool the model again, such \u2018fooling\u2019 examples will be used to further improve sentiment-recognition abilities of the model).  The goal of this SPF is  to build a DeFi-community-chat-specific text corpus  => tune an LLM, with it; run up to 20 rounds of LLM dynamic fine-tuning for sentiment recognition collecting adversarial data from from DeFi community chats and using human labellers in the loop. Hence the cost.  All data gathered, models trained and code written will be published. The resulting LLM tuned for working with DeFi-related context and fine-tuned for recognising sentiment in the DeFi community chats will be used to improve MakerDAO\u2019s risk estimate accuracy and governance communication among other things as described above. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.484307359307358
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#11",
                    "likes": "0",
                    "time": "21/11/2022-15:43:49",
                    "content": "    CodeKnight:  So if I understand, you want to create a program that combs social media and adjusts risk parameters based on sentiment?   DeFi community sentiment and narratives exploration is a method to increase risk estimates accuracy and give more information for MKR governance to consider while making decision about protocol parameters values. For now risk modelling is based on the premise that at any given moment Maker should be ready to withstand price drop = 45% for WBTC, 50% for ETH, and 60% for all other volatile assets twice per year. No more, no less. DeFi community sentiment and narratives gauge provides more access to the real-life picture. Hence it will provide more context for the MKR governance. MKR holders and delegates will be able to understand better the necessity and the required degree of protocol parameters adjustment. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 6.220833333333333
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#12",
                    "likes": "2",
                    "time": "21/11/2022-15:49:54",
                    "content": "    Davidutro:  Would the dataset and integration work be done under an open-source software license?   Sure.     Davidutro:  What happens after this SPF?   Once we have an LLM fine-tuned for DeFi community sentiment analysis, this LLM will be wrapped into a dashboard in the maner similar to the maker.blockanalitica.com. It could be used as a tool for Computer-Aided Governance on its own: a factor to be considered, when estimating market risks => proposing protocol parameters for the DAO vote. Further on it will gradually be integrated as an input into the quantitive Market Risk Model. The same fine-tuned LLM will be available for GovComm purposes to gauge sentiment of Maker\u2019s own community. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 6.5
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#13",
                    "likes": "1",
                    "time": "21/11/2022-16:07:07",
                    "content": "    fig:  While it may be interesting, the data set requires experience and confidence from these teams for it to be valuable - or a super streamlined tool. @pvl what qualifications make you a good fit for the Work Credentials?   Most importantly, this proposal is a direct build-up on the UST crash case forensics I built. I published there a fine-tuned model + code + data scraped and processed and even visualisation script. Now the general approach is the same, just custom DeFi-specific vs general purpose (publicly available) dataset for model fine-tuning + domain-tuned bigger/better language model => better model accuracy. And while I\u2019m not prepared to talk about my outside-of-DeFi activities, I can say that I studied machine learning under J\u00fcrgen Schmidhuber and Michael Bronstein\u202c. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.638888888888888
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#14",
                    "likes": "1",
                    "time": "21/11/2022-22:58:33",
                    "content": "    fig:  It seems to be a sentiment analysis that other teams are already building: https://www.moonpass.ai/  - this one goes beyond tweets https://twitter.com/oz_dao  - this one is for Twitter only What makes your proposed solution better than these solutions above, one of which is free?   Ox_DAO: unfortunately couldn\u2019t find anything specific. Just a promise of \u201can unparalleled monitoring system and encyclopedia for Crypto-native social activity\u201d. They didn\u2019t publish any data/code/docs or anything else. Can\u2019t comment. Moonpass. First, a general approach comment. To quote:  We are building powerful AI models that source data from all relevant off-chain sources like Twitter, Discord, Reddit, Telegram, YouTube, Podcasts, News Articles, Medium Posts, GitHub, and much more.  Video, text, audio, code are different data, they require different model architectures. Different models, different datasets to train and fine-tune. Parallel threads of work. Natural Language Processing alone is a big research area. From my experience building a meaningful intelligence tool even with text input only is not a simple task. Let alone building all at once such Jack of all trades. E.g. OpenAI, a key ML research lab with funding from Microsoft and responsible for GPT-3 and DALL\u00b7E 2, focuses on LLMs and image generation from text prompts. Imo, Moonpass\u2019 approach is na\u00efve and doomed. Moreover, not all data is equally important here. As for some specifics. They\u2019re using general purpose BERT and RoBERTa models, which are fine, but far from state-of-the-art. Models were not tuned on domain-specific corpus. They\u2019re using dictionary-based sentiment model VADER to label the sentiment score of data. Sentiment analysis classifiers built on a manually labeled corpus have been shown to provide better accuracy. Moreover, if you fine-tune your model on a domain-specific dataset this improves model accuracy even more. It all boils down to model accuracy. That\u2019s the purpose of this SPF.  I\u2019d like to highlight one more time: training data quality is crucial for model accuracy. This SPF aims at building the first DeFi-focused datasets for Large Language Model tuning and fine-tuning, which will give edge to Maker and if outsourced to the whole DeFi community. It will deliver millions of dollars of additional value for the DAO & Maker users from more accurate Risk estimates as well as improving Governance Communications. ",
                    "links": [
                        "https://twitter.com/oz_dao"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.81046626984127
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Patrick_J",
                    "index": "#15",
                    "likes": "1",
                    "time": "23/11/2022-15:03:14",
                    "content": "Confirming I am willing to act as a comptroller for this SPF. Please note this should not be considered as support for the proposal but instead reflects my willingness to facilitate the funding of this work if approved by MKR voters. If this is approved I will use my usual signing address of 0x06ADa798f9323392cA30C755383Af879bd853168. Verification of ownership. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party wants to collaborate on proposal",
                        "3rd party giving entirely positive feedback on proposal"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Davidutro",
                    "index": "#16",
                    "likes": "1",
                    "time": "23/11/2022-15:50:00",
                    "content": "Confirming I am willing to act as a comptroller for this SPF. Please note this should not be considered as support for the proposal but instead reflects my willingness to facilitate the funding of this work if approved by MKR voters. If this is approved I will use my signing address of 0xE91f4F5834bB9312A45B2bd75804b32CC3DA8fd3. Verification of ownership ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://forum.makerdao.com/u/fumoffuXx",
                    "index": "#17",
                    "likes": "0",
                    "time": "24/11/2022-08:39:47",
                    "content": "Can i check if the risk model will be made public? ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#18",
                    "likes": "1",
                    "time": "24/11/2022-08:40:36",
                    "content": "sure, it will be public ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.25
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Gala",
                    "index": "#19",
                    "likes": "0",
                    "time": "24/11/2022-13:11:18",
                    "content": "Two comptrollers have been confirmed for this SPF: @Patrick_J and @Davidutro ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 7.0
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#20",
                    "likes": "0",
                    "time": "24/11/2022-13:52:53",
                    "content": "On the link between Social Media sentiment and financial market movements. Already in 2010 it was shown    arXiv.org    Twitter mood predicts the stock market Behavioral economics tells us that emotions can profoundly affect individual behavior and decision-making. Does this also apply to societies at large, i.e., can societies experience mood states that affect their collective decision making? By...      that the accuracy of a model predicting daily closing values of Dow Jones Industrial Average is significantly improved by the inclusion of specific public mood dimensions as measured from Twitter feed. Most recently it was confirmed    CBS Research Portal    Your Sentiment Matters: A Machine Learning Approach for Predicting Regime...      that similar impact is present in the case of Twitter sentiment and crypto market. Speaking more generally, Nobel-winning research by Robert Shiller shows that irrational factors like sentiment and narratives play an important role in asset pricing. In fact, the Nobel Prize in Economics \u201913 was shared by Shiller and Eugene Fama, the creator of the Efficient-market hypothesis. This underscores that EHM- and sentiment/narrative-driven asset pricing are two complimentary mechanisms and market risk modelling should be handled accordingly. The most obvious example of irrational exuberance on financial markets are bubbles like the one happened around UST \u2014 all information about its model was open, yet the bubble emerged and then collapsed. Such price movements also happen gradually over significant periods of time. Price volatility not explainable by objective economic factors happens in case of mature assets like ETH as well, see e.g. crypto winters. It\u2019s also an example of risk beyond the scope of risk models based on the Efficient-market hypothesis. Nobel Prize in economics \u201922 was awarded for showing mechanisms of bank vulnerability to rumours becoming a self-fulfilling prophecy and leading to a bank collapse (bank runs). This once again underscored the importance of financial markets sentiment analysis. Estimates of market irrationality impact It was shown that only about 7% of the variance of annual stock market returns can be justified in terms of efficient market factors for the aggregate U.S. stock market 1871\u20131987. For individual-firm stock price variations it was estimated that the standard deviation of the \u2018\u2018atypical discount\u2019\u2019 was about 25%. It was concluded that the inefficient component of stock price variation that generates predictable movements in future returns \u2018\u2018still has an economically significant impact on firm-level stock prices\u2019. Importantly, given high correlation between DeFi assets and due to the impact of sentiment/narrative on the DeFi market via social media/social networks like Twitter and Discord, it\u2019s very likely that the impact of inefficient market factors on DeFi asset price movements is far higher than that for individual TradFi asset prices and closer to 50%, maybe more. See crypto bubbles/winters. TradFi is already in Moreover, The Federal Reserve Bank of New York has been gauging public sentiment since 2013 and integrating this data into its models. Baker and Wurgler reviewed the investor sentiment literature and showed that a \u201csentiment index\u201d is highly correlated with aggregate stock returns. See also Handbook of Economic Expectations and The overview of literature on expectations data in asset pricing in particular. See also R. Shiller\u2019s book on Narrative economics. See also Nobel Prizes: \u201917 \u201cfor exploring the consequences of limited rationality, social preferences, and lack of self-control \u2026 how these human traits systematically affect individual decisions as well as market outcomes.\u201d \u201902 \u201cfor having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty\u201d ",
                    "links": [
                        "https://arxiv.org/abs/1010.3003",
                        "https://research.cbs.dk/en/publications/your-sentiment-matters-a-machine-learning-approach-for-predicting",
                        "https://research.cbs.dk/en/publications/your-sentiment-matters-a-machine-learning-approach-for-predicting",
                        "https://www.nobelprize.org/uploads/2018/06/shiller-lecture.pdf",
                        "https://www.nobelprize.org/prizes/economic-sciences/2013/press-release/",
                        "https://www.nobelprize.org/prizes/economic-sciences/2022/press-release/",
                        "https://www.nber.org/system/files/working_papers/w2100/w2100.pdf",
                        "https://www.nber.org/system/files/working_papers/w2511/w2511.pdf",
                        "https://www.nber.org/system/files/working_papers/w8240/w8240.pdf",
                        "https://www.newyorkfed.org/medialibrary/media/research/epr/2017/epr_2017_survey-consumer-expectations_armantier.pdf",
                        "https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr1018.pdf",
                        "https://pubs.aeaweb.org/doi/pdf/10.1257/jep.21.2.129",
                        "https://www.elsevier.com/books/handbook-of-economic-expectations/bachmann/978-0-12-822927-9",
                        "https://www.nber.org/system/files/working_papers/w29977/w29977.pdf",
                        "https://www.amazon.com/Narrative-Economics-Stories-Economic-Events/dp/0691182299",
                        "https://www.nobelprize.org/prizes/economic-sciences/2017/summary/",
                        "https://www.nobelprize.org/prizes/economic-sciences/2002/summary/",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.60769144648455
                },
                {
                    "author_link": "https://forum.makerdao.com/u/CodeKnight",
                    "index": "#21",
                    "likes": "1",
                    "time": "24/11/2022-13:59:38",
                    "content": "If you can make a model that predicts future price movements(even crudely), then you have developed a product worth tens of billions of dollars at least. That would be quite impressive on a 50K budget. I would suggest that rather than open-sourcing it, you sell/license it and become very rich. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party extending to proposal",
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.633928571428571
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#22",
                    "likes": "1",
                    "time": "24/11/2022-14:01:33",
                    "content": "It\u2019s not about predicting price. It\u2019s about increasing risk estimate accuracy and tools for GovComm for community sentiment analysis. 50k goes for assembling DeFi-focused datasets, hiring human labellers to label data in the dynamic adversarial data collection style, which will increase model accuracy when working with DeFi community chats data; tuning and fine-tuning language models detecting sentiment, which is a factor in risk estimates, this will require buying cloud computing resources from Google Cloud Platform. an example of Twitter scraping query for Uniswap tweets: for i, tweet in enumerate(sntwitter.TwitterSearchScraper('$UNI OR @Uniswap OR #Uniswap since:2018-04-01 lang:en -from:defisniper -from:DYORCryptoBot -from:FuturesTracker -from:Crypto3OT -from:BoxerXrp').get_items()):     tweets_list_uni.append([tweet.date, tweet.content])  ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 5.0
                },
                {
                    "author_link": "https://forum.makerdao.com/u/lyt",
                    "index": "#23",
                    "likes": "4",
                    "time": "25/11/2022-16:14:47",
                    "content": "Firstly I\u2019d like to mention that this iteration is much more extensively thought through than the last, and I really appreciate that.     josolnik:  we are skeptical about sentiment analysis\u2019s capacity to provide substantial information edge on the tail risk volatility. If you find some empirical research where that\u2019s been validated in a meaningful way, please do share.   I echo this and feel as though OP\u2019s responses do not address it sufficiently. On top of this, I have a more specific related concern: Training such models with pretty limited data available would likely lead to immense overfitting and render the models useless. There aren\u2019t many \u201cjumps\u201d that have happened & the aim of this here is to take imperfect tweet sentiment data (that we can\u2019t really be confident is related to jump severity & frequency to begin with) to distinguish those few jumps? Please tell me why this isn\u2019t a massive stretch. I still feel as though this is a solution looking for a problem. The nerd in me would be personally curious to see the output but don\u2019t think it could realistically provide value to Maker via assisting in risk parameter-setting. As always, my thoughts. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal"
                    ],
                    "Sentiment": 5.565714285714286
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#24",
                    "likes": "0",
                    "time": "27/11/2022-21:57:11",
                    "content": "    lyt:  Firstly I\u2019d like to mention that this iteration is much more extensively thought through than the last, and I really appreciate that.   Thank you.     lyt:  Training such models with pretty limited data available would likely lead to immense overfitting and render the models useless. There aren\u2019t many \u201cjumps\u201d that have happened & the aim of this here is to take imperfect tweet sentiment data (that we can\u2019t really be confident is related to jump severity & frequency to begin with) to distinguish those few jumps? Please tell me why this isn\u2019t a massive stretch.   The goal of the current proposal is to tune and fine-tune Large Language Models to increase accuracy of sentiment probes in DeFi-related communication. Not to predict asset price jumps from sentiment.     lyt:      josolnik:  we are skeptical about sentiment analysis\u2019s capacity to provide substantial information edge on the tail risk volatility. If you find some empirical research where that\u2019s been validated in a meaningful way, please do share.   I echo this and feel as though OP\u2019s responses do not address it sufficiently.   I encourage everyone to look into the bibliography, where the question was actually addressed MIP55c3-SP11: Pioneer DeFi-focused language dataset for the benefit of Risk modelling & GovComms - #20 by pvl For example, Baker and Wurgler showed in   aeaweb.org    Investor Sentiment in the Stock Market - American Economic Association (Spring 2007) - Investor sentiment, defined broadly, is a belief about future cash flows and investment risks that is not justified by the facts at hand. The question is no longer whether investor sentiment affects stock prices, but how to measure...      that a one-standard-deviation change in the sentiment changes index results in the price changes by over 2 percentage points (Panel B) for volatile assets :  Screenshot 2022-11-26 at 01.24.521424\u00d71782 347 KB  Basically it was shown that the more speculative, volatile an asset is, the more it\u2019s sensitive to investor sentiment. Bond-like stocks are not. Highly speculative assets like difficult-to-value technology stocks \u2014 very sensitive. Are crypto assets, like BTC or ETH, let alone the rest, speculative, highly volatile? Hell yeah. ",
                    "links": [
                        "https://www.aeaweb.org/articles?id=10.1257%2Fjep.21.2.129",
                        "https://www.aeaweb.org/articles?id=10.1257%2Fjep.21.2.129",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.516964285714286
                },
                {
                    "author_link": "https://forum.makerdao.com/u/pvl",
                    "index": "#25",
                    "likes": "0",
                    "time": "28/11/2022-09:55:49",
                    "content": "    lyt:  I still feel as though this is a solution looking for a problem.   The problem is very real: core risk model parameter inputs currently are just round figure guesstimates with no data or any other justification behind. It\u2019s just nobody asks the question you echoed, but about Maker\u2019s current risk model design. I. When Jan says     josolnik:  we are skeptical about sentiment analysis\u2019s capacity to provide substantial information edge on the tail risk volatility.   it is implied that current market risk modelling is good enough. Why would we need anything else? Sentiment data won\u2019t add any new substantial knowledge. However, the whole logic here is upside down. It\u2019s not that sentiment data should add anything to the current model. It\u2019s that current model inputs are v0.1 rough guesstimates => low model accuracy. Maker certainly could do risk modelling much better and this will translate into millions of $ of added value + dodged risks currently under the radar.  What are the current basic assumptions?  Screenshot 2022-11-26 at 15.44.111428\u00d7320 32.5 KB   Screenshot 2022-11-26 at 15.44.411428\u00d7470 41.4 KB  Current risk model optimises around the assumption that Maker must withstand two price drops per year at 45% for WBTC, 50% for ETH, and 60% for all other volatile assets.  But why exactly these (round!) model parameter CONSTs? Where do they come from? Why not 58.71% for ETH or 37.94% for WBTC?  And this matters, because risk model parameter values translate to the Risk Premium value, which is a key factor when proposing protocol parameters: vault\u2019s Stability Fee, Debt Ceiling, System Surplus Buffer (via protocol-wide Capital at Risk metric). Each percentage point of risk input values + or - results in $ of value lost or found both for the DAO and for the users. You can try the impact of different risk model parameter values on the protocol parameter values here:    maker.blockanalitica.com   Maker Risk | Block Analitica Maker Risk from Block Analitica      So, I\u2019m joining you in echoing Jan\u2019s question: what empirical research are these 45%;50%;60% twice per year CONSTs based on, where it\u2019s been validated in a meaningful way that these levels are optimal?  II.     lyt:  I still feel as though this is a solution looking for a problem.   The problem is that these round 45/50/60 model input CONSTs are not risk estimates; they are plugs in the absence of ones. Instead of researching constantly changing DeFi environment and understanding what level of risk Maker must withstand in the current climate => adapt accordingly, it\u2019s been fixed forever that Maker must always  prepare for these 45%;50%;60% market shocks. No more, no less.  I\u2019m proposing to turn \u201cJump Severity\u201d and \u201cJump Frequency\u201d risk inputs form rough round CONST guesstimates into variables learned from real-time data on DeFi activity, community sentiment and vault txns.  For now everybody got used to the business as usual in terms of risk modelling. However, Maker\u2019s balance is in the red. My proposal could well contribute significantly to bringing Maker to the break-even point by bringing risk estimates => protocol parameters like Stability Fee, Debt Ceiling, System Surplus Buffer closer to the reality. It\u2019s incorrect to say that now Maker has a good enough risk model and I am suggesting to increase capital efficiency at the expense of extra risk as @josolnik claimed. On the contrary, I\u2019m proposing  to better estimate real risks by gradually transitioning risk modelling from fixed round guesstimates to data-driven model inputs. And to learn to gauge DeFi sentiment efficiently, a key market risk factor for the highly volatile DeFi market, is the first step in this direction. Currently Maker is basing its risk estimates on the assumptions not grounded in the changing DeFi environment and I\u2019m proposing to fix this. ",
                    "links": [
                        "https://maker.blockanalitica.com/simulations/risk-model/",
                        "https://maker.blockanalitica.com/simulations/risk-model/",
                        "https://forum.makerdao.com/t/mip55c3-sp11-pioneer-defi-focused-language-dataset-for-the-benefit-of-risk-modelling-govcomms/18802/24",
                        "https://forum.makerdao.com/t/should-block-analitica-have-a-2-76m-year-2400-mkr-risk-management-monopoly-and-be-its-gatekeeper/19123"
                    ],
                    "GPT-discussion-categories": [
                        "Author of proposal is explaining proposal"
                    ],
                    "Sentiment": 5.2132950121157675
                },
                {
                    "author_link": "https://forum.makerdao.com/u/CodeKnight",
                    "index": "#26",
                    "likes": "0",
                    "time": "28/11/2022-14:19:20",
                    "content": "It seems like this should be evaluated and funded by Risk rather than directly by governance. A CU is better qualified to evaluate such a proposal than governance. A CU would also be able to track progress, set milestones and analyze outcomes far better than governance. ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "None of the topics listed match"
                    ],
                    "Sentiment": 6.416666666666667
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Doo_StableLab",
                    "index": "#27",
                    "likes": "4",
                    "time": "29/11/2022-09:55:17",
                    "content": "I think ultimately it\u2019s up to Maker Governance\u2019s voting to see if the community is interested in pursuing such. It\u2019s clear that the proposer of this initiative has a strong opinion (which is fine) and others pointing out why we don\u2019t need such is just going to be an endless debate till the voting ends ",
                    "links": [],
                    "GPT-discussion-categories": [
                        "3rd party giving constructive criticism of proposal",
                        "3rd party asking questions about proposal",
                        "3rd party or author wants to collaborate on proposal",
                        "3rd party auditing and reviewing proposal"
                    ],
                    "Sentiment": 5.671875
                },
                {
                    "author_link": "https://forum.makerdao.com/u/Retro",
                    "index": "#28",
                    "likes": "0",
                    "time": "02/12/2022-17:21:32",
                    "content": "Confirming I am willing to act as comptroller for this SPF. My signing address is: 0xa648640060d5d00914c05C10bDe3e0CBa5c88CD2 ",
                    "links": [],
                    "GPT-discussion-categories": null,
                    "Sentiment": 6.25
                }
            ]
        }
    ]
}